{"source_id":"core","type":"context_engineering","title":"Context Engineering Fundamentals","signals":["context_engineering","provider_adapter"],"tags":["ai","context","llm","production"],"summary":"Context Engineering principles - the evolution from prompt engineering to dynamic, context-aware AI systems for production-grade deployment.","chunks":[{"chunk_id":"core:context-eng-fundamentals:1","text":"## Context Engineering Fundamentals\n\n### Definition\nContext Engineering is the discipline of designing and optimizing how information is assembled and provided to LLMs at inference time.\n\n### Context Components\n```\ncontext = Assemble(instructions, knowledge, tools, memory, state, query)\n```\n\n- **Instructions**: System prompts and rules\n- **Knowledge**: Retrieved relevant information (RAG)\n- **Tools**: Available function definitions\n- **Memory**: Conversation history and learned facts\n- **State**: Current world/user state\n- **Query**: User's immediate request\n\n### Key Principles\n1. **Dynamic Adaptation**: Context assembly adapts to each query and state\n2. **Information Optimization**: Maximize relevant information within token limits\n3. **Structural Sensitivity**: Format aligns with LLM processing capabilities\n4. **System-Level Design**: Multi-component optimization, not simple prompting\n\n### Context Failures (Common Issues)\n- Insufficient context leading to hallucination\n- Irrelevant context diluting important information\n- Missing state causing inconsistent responses\n- Token overflow dropping critical information","start_line":1,"end_line":28}]}
{"source_id":"core","type":"context_engineering","title":"Context Window Optimization","signals":["context_engineering"],"tags":["ai","optimization","tokens","production"],"summary":"Strategies for optimizing context window usage including compression, prioritization, and dynamic context selection.","chunks":[{"chunk_id":"core:context-window-opt:1","text":"## Context Window Optimization\n\n### Token Budget Strategy\n- **Reserved**: System prompt, tools (fixed overhead)\n- **Dynamic**: Knowledge, memory, conversation\n- **Buffer**: 10-20% for response generation\n\n### Prioritization Order\n1. Active task instructions (highest)\n2. Immediate context (current conversation)\n3. Retrieved knowledge (RAG results)\n4. Historical memory (summarized)\n5. Background context (lowest)\n\n### Compression Techniques\n- **Summarization**: Compress long histories\n- **Deduplication**: Remove redundant information\n- **Selective Retrieval**: Only most relevant chunks\n- **Progressive Disclosure**: Expand on demand\n\n### Dynamic Context Selection\n```python\ndef select_context(query, available_contexts, max_tokens):\n    ranked = rank_by_relevance(query, available_contexts)\n    selected = []\n    tokens_used = 0\n    for ctx in ranked:\n        if tokens_used + ctx.tokens <= max_tokens:\n            selected.append(ctx)\n            tokens_used += ctx.tokens\n    return selected\n```","start_line":1,"end_line":30}]}
{"source_id":"core","type":"contract","title":"AI Agent Contract Template","signals":["contract_lock","agent_pattern"],"tags":["ai","agent","llm","template"],"summary":"Contract template for AI agents covering capabilities, tools, memory, guardrails, and evaluation criteria.","chunks":[{"chunk_id":"core:ai-agent-contract:1","text":"## AI Agent Contract\n\n### Agent Overview\n- **Name**: [Agent Name]\n- **Role**: [System prompt role definition]\n- **Model**: GPT-4 / Claude 3.5 / Llama 3\n\n### Capabilities\n- **Tools**: List of available functions\n- **Memory**: Short-term (window) + Long-term (vector DB)\n- **Planning**: ReAct / Chain-of-Thought / Tree of Thoughts\n\n### Guardrails\n- **Input**: PII redaction, injection detection\n- **Output**: Hallucination check, tone enforcement\n- **Budget**: Token limits per request/session\n- **Safety**: Content filtering, refusal patterns\n\n### Evaluation Criteria\n- **Accuracy**: % correct on golden Q&A set\n- **Latency**: P95 response time < 3s\n- **Cost**: $ per 1000 interactions\n- **Safety**: 0 harmful outputs\n\n### Escalation\n- Uncertain responses → human review\n- Error rate > 5% → alert + fallback\n- Token budget exceeded → graceful degradation","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"Web App Contract Template","signals":["contract_lock"],"tags":["web","template"],"summary":"A structured template for defining web application project contracts with clear goals, requirements, and acceptance criteria.","chunks":[{"chunk_id":"core:web-contract:1","text":"## Web Application Contract\n\n### Project Overview\n- **Name**: [Project Name]\n- **Type**: Web Application\n- **Target Users**: [Primary user personas]\n\n### Goals\n1. [Primary business goal]\n2. [User experience goal]\n3. [Technical goal]\n\n### Requirements\n- **MUST**: [Critical requirements that block ship]\n- **SHOULD**: [Important but not blocking]\n- **COULD**: [Nice-to-have features]\n\n### Acceptance Criteria\n- [ ] All MUST requirements implemented\n- [ ] Core user flows working end-to-end\n- [ ] Performance within acceptable thresholds\n- [ ] No critical security vulnerabilities","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"API Contract Template","signals":["contract_lock"],"tags":["api","template","backend"],"summary":"Template for API development contracts covering endpoints, authentication, rate limits, and versioning requirements.","chunks":[{"chunk_id":"core:api-contract:1","text":"## API Contract\n\n### Service Overview\n- **API Name**: [Service Name]\n- **Base URL**: /api/v1\n- **Authentication**: [JWT/API Key/OAuth2]\n\n### Endpoints\n| Method | Path | Description | Auth Required |\n|--------|------|-------------|---------------|\n| GET | /resources | List all | Yes |\n| POST | /resources | Create new | Yes |\n| GET | /resources/:id | Get by ID | Yes |\n\n### Rate Limits\n- Standard: 100 req/min\n- Authenticated: 1000 req/min\n\n### Versioning\n- URL path versioning (/v1, /v2)\n- Deprecation notice: 90 days minimum","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"CLI Tool Contract Template","signals":["contract_lock","state_machine"],"tags":["cli","template","rust","node"],"summary":"Contract template for command-line tools covering commands, flags, exit codes, and cross-platform requirements.","chunks":[{"chunk_id":"core:cli-contract:1","text":"## CLI Tool Contract\n\n### Tool Overview\n- **Name**: [tool-name]\n- **Purpose**: [One-line description]\n- **Platforms**: Windows, macOS, Linux\n\n### Commands\n| Command | Description | Example |\n|---------|-------------|--------|\n| init | Initialize workspace | tool init |\n| run | Execute main action | tool run --config x.json |\n| status | Show current state | tool status -v |\n\n### Exit Codes\n- 0: Success\n- 1: General error\n- 2: Invalid arguments\n- 3: Configuration error\n\n### Distribution\n- Single binary, no runtime dependencies\n- Install via curl/powershell one-liner","start_line":1,"end_line":22}]}
{"source_id":"core","type":"contract","title":"Acceptance Criteria: Performance","signals":["contract_lock"],"tags":["performance","criteria","web","api"],"summary":"Common performance acceptance criteria patterns for web apps and APIs including response times, throughput, and resource limits.","chunks":[{"chunk_id":"core:perf-criteria:1","text":"## Performance Acceptance Criteria\n\n### Response Time\n- P50 latency < 100ms for API calls\n- P95 latency < 500ms\n- P99 latency < 1000ms\n- Page load (LCP) < 2.5 seconds\n\n### Throughput\n- Handle 1000 concurrent users\n- Process 100 requests/second sustained\n\n### Resource Usage\n- Memory < 512MB under normal load\n- CPU < 70% average utilization\n- No memory leaks over 24h run\n\n### Failure Tolerance\n- Graceful degradation under 2x expected load\n- Auto-recovery within 30 seconds","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Acceptance Criteria: UX Patterns","signals":["contract_lock"],"tags":["ux","criteria","web","accessibility"],"summary":"User experience acceptance criteria covering accessibility, responsiveness, error handling, and loading states.","chunks":[{"chunk_id":"core:ux-criteria:1","text":"## UX Acceptance Criteria\n\n### Accessibility\n- WCAG 2.1 AA compliant\n- Keyboard navigation for all actions\n- Screen reader compatible\n- Color contrast ratio >= 4.5:1\n\n### Responsiveness\n- Mobile-first design (320px minimum)\n- Tablet breakpoint at 768px\n- Desktop breakpoint at 1024px\n\n### Loading States\n- Skeleton screens for async content\n- Loading indicator within 100ms\n- Progress for operations > 3 seconds\n\n### Error Handling\n- Clear error messages in plain language\n- Actionable recovery suggestions\n- No raw exception traces to users","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Acceptance Criteria: Error States","signals":["contract_lock"],"tags":["errors","criteria","reliability"],"summary":"Error handling acceptance criteria for graceful failures, user-friendly messages, and recovery options.","chunks":[{"chunk_id":"core:error-criteria:1","text":"## Error State Acceptance Criteria\n\n### User-Facing Errors\n- Friendly message explaining what happened\n- Suggested next steps or workarounds\n- Support contact or documentation link\n- Unique error code for support lookup\n\n### System Errors\n- Logged with full stack trace\n- Correlated with request ID\n- Alerting for error rate spikes\n- No sensitive data in error messages\n\n### Recovery\n- Retry button for transient failures\n- Auto-retry with exponential backoff\n- Preserve user input on form errors\n- Graceful fallback for non-critical features","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Acceptance Criteria: Security","signals":["contract_lock","security_pattern"],"tags":["security","criteria","authentication"],"summary":"Security acceptance criteria covering authentication, authorization, data protection, and vulnerability scanning.","chunks":[{"chunk_id":"core:security-criteria:1","text":"## Security Acceptance Criteria\n\n### Authentication\n- Strong password policy enforced\n- MFA option available\n- Session timeout after inactivity\n- Secure token storage (httpOnly cookies)\n\n### Authorization\n- Role-based access control (RBAC)\n- Principle of least privilege\n- Permission checks on every request\n\n### Data Protection\n- TLS 1.2+ for all connections\n- Sensitive data encrypted at rest\n- PII not logged or exposed\n- GDPR/CCPA compliance if applicable\n\n### Vulnerability Management\n- No critical CVEs in dependencies\n- Regular security scanning\n- Responsible disclosure process","start_line":1,"end_line":21}]}
{"source_id":"core","type":"contract","title":"Out-of-Scope Patterns","signals":["contract_lock"],"tags":["scope","planning"],"summary":"Common patterns for defining what is explicitly out of scope to prevent scope creep during development.","chunks":[{"chunk_id":"core:out-of-scope:1","text":"## Out-of-Scope Definition Patterns\n\n### Explicit Exclusions\n- Mobile native apps (web responsive only)\n- Offline mode (online-first for v1)\n- Third-party integrations (future phase)\n- Legacy browser support (IE11, old Safari)\n- Multi-language i18n (English only for v1)\n\n### Scope Creep Prevention\n- Document all scope changes in writing\n- Require stakeholder sign-off for additions\n- Assess impact on timeline before accepting\n- Create separate tickets for future features\n\n### Boundary Markers\n- \"Not in this release\" tracking list\n- Future roadmap for deferred items\n- Clear versioning for scope (v1 vs v2)","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Scope Creep Prevention Checklist","signals":["contract_lock"],"tags":["scope","planning","checklist"],"summary":"Checklist to identify and prevent scope creep during project development lifecycle.","chunks":[{"chunk_id":"core:scope-creep:1","text":"## Scope Creep Prevention Checklist\n\n### Before Accepting New Work\n- [ ] Is this in the original contract?\n- [ ] What is the timeline impact?\n- [ ] What existing work gets deprioritized?\n- [ ] Who approved this change?\n- [ ] Is there budget for this addition?\n\n### Red Flags\n- \"While you're at it, can you also...\"\n- \"This should be quick...\"\n- \"We forgot to mention...\"\n- Vague requirements that keep expanding\n\n### Response Strategy\n- Log the request formally\n- Provide effort estimate\n- Present trade-offs clearly\n- Defer to next phase if possible","start_line":1,"end_line":19}]}
{"source_id":"core","type":"plan","title":"Web App Implementation Plan Template","signals":["state_machine"],"tags":["web","plan","template"],"summary":"Structured implementation plan template for web application development with phases, milestones, and dependencies.","chunks":[{"chunk_id":"core:web-plan:1","text":"## Web App Implementation Plan\n\n### Phase 1: Foundation (Week 1-2)\n- [ ] Project scaffolding and tooling\n- [ ] Design system setup (colors, typography)\n- [ ] Core layout components\n- [ ] Authentication flow\n- [ ] CI/CD pipeline\n\n### Phase 2: Core Features (Week 3-4)\n- [ ] Primary user flows\n- [ ] Data models and API integration\n- [ ] Form handling and validation\n- [ ] Error boundaries and loading states\n\n### Phase 3: Polish (Week 5)\n- [ ] Responsive design refinements\n- [ ] Performance optimization\n- [ ] Accessibility audit\n- [ ] Cross-browser testing\n\n### Phase 4: Ship (Week 6)\n- [ ] Final QA pass\n- [ ] Documentation\n- [ ] Deployment and monitoring","start_line":1,"end_line":24}]}
{"source_id":"core","type":"plan","title":"API Implementation Plan Template","signals":["state_machine"],"tags":["api","plan","template","backend"],"summary":"Implementation plan template for API development covering design, implementation, testing, and deployment phases.","chunks":[{"chunk_id":"core:api-plan:1","text":"## API Implementation Plan\n\n### Phase 1: Design (Week 1)\n- [ ] OpenAPI/Swagger spec draft\n- [ ] Data model design\n- [ ] Authentication strategy\n- [ ] Rate limiting approach\n- [ ] Error response format\n\n### Phase 2: Core Implementation (Week 2-3)\n- [ ] Database schema and migrations\n- [ ] CRUD endpoints\n- [ ] Input validation\n- [ ] Business logic layer\n- [ ] Unit tests (80% coverage target)\n\n### Phase 3: Integration (Week 4)\n- [ ] External service integrations\n- [ ] Webhook handlers\n- [ ] Background job processing\n- [ ] Integration tests\n\n### Phase 4: Production Ready (Week 5)\n- [ ] Load testing\n- [ ] Security audit\n- [ ] Documentation\n- [ ] Deployment runbook","start_line":1,"end_line":26}]}
{"source_id":"core","type":"plan","title":"CLI Tool Implementation Plan Template","signals":["state_machine"],"tags":["cli","plan","template"],"summary":"Implementation plan for CLI tool development covering argument parsing, commands, cross-compilation, and distribution.","chunks":[{"chunk_id":"core:cli-plan:1","text":"## CLI Tool Implementation Plan\n\n### Phase 1: Foundation (Week 1)\n- [ ] Argument parsing setup\n- [ ] Configuration file handling\n- [ ] Logging infrastructure\n- [ ] Error handling patterns\n- [ ] Basic help and version commands\n\n### Phase 2: Core Commands (Week 2-3)\n- [ ] Primary command implementation\n- [ ] Subcommand structure\n- [ ] Interactive prompts\n- [ ] Progress indicators\n- [ ] Output formatting (JSON, table, plain)\n\n### Phase 3: Polish (Week 4)\n- [ ] Shell completions\n- [ ] Man page generation\n- [ ] Cross-platform testing\n- [ ] Performance profiling\n\n### Phase 4: Distribution (Week 5)\n- [ ] Cross-compilation setup\n- [ ] Install scripts (curl, PowerShell)\n- [ ] Release automation\n- [ ] Checksum generation","start_line":1,"end_line":26}]}
{"source_id":"core","type":"plan","title":"Test Strategy Template","signals":["iterate_loop"],"tags":["testing","plan","strategy"],"summary":"Comprehensive test strategy template covering unit, integration, e2e testing, and quality gates.","chunks":[{"chunk_id":"core:test-strategy:1","text":"## Test Strategy\n\n### Test Pyramid\n- Unit Tests (70%): Fast, isolated, mock dependencies\n- Integration Tests (20%): Real database, API contracts\n- E2E Tests (10%): Critical user journeys only\n\n### Coverage Targets\n- Overall: 80% line coverage minimum\n- Critical paths: 95% coverage\n- New code: Must not decrease coverage\n\n### Quality Gates\n- All tests pass before merge\n- No flaky tests allowed\n- Performance regression detection\n- Security scan clean\n\n### Test Environment\n- Local: SQLite, mocked externals\n- CI: Docker-based, ephemeral\n- Staging: Production-like, seeded data","start_line":1,"end_line":20}]}
{"source_id":"core","type":"plan","title":"Rollback and Safety Plan","signals":["release_install"],"tags":["rollback","safety","deployment"],"summary":"Rollback strategy template for safe deployments including triggers, procedures, and communication plans.","chunks":[{"chunk_id":"core:rollback-plan:1","text":"## Rollback and Safety Plan\n\n### Rollback Triggers\n- Error rate > 5% for 5 minutes\n- P95 latency > 2x baseline\n- Critical functionality broken\n- Security vulnerability discovered\n\n### Rollback Procedure\n1. Alert team in incident channel\n2. Revert to previous deployment\n3. Verify rollback successful\n4. Investigate root cause\n5. Document in post-mortem\n\n### Safety Measures\n- Blue-green deployments\n- Canary releases (5% → 25% → 100%)\n- Feature flags for risky changes\n- Database migration rollback scripts\n\n### Communication\n- Status page update within 5 minutes\n- Customer notification for major issues","start_line":1,"end_line":22}]}
{"source_id":"core","type":"evidence","title":"Evidence Collection Checklist","signals":["evidence_audit"],"tags":["evidence","checklist","audit"],"summary":"Comprehensive checklist for collecting evidence during development including diffs, logs, test results, and lint reports.","chunks":[{"chunk_id":"core:evidence-checklist:1","text":"## Evidence Collection Checklist\n\n### Code Changes\n- [ ] Git diff captured\n- [ ] Commit message follows convention\n- [ ] PR description complete\n- [ ] Linked to issue/ticket\n\n### Test Evidence\n- [ ] Test output captured\n- [ ] Coverage report generated\n- [ ] All tests passing\n- [ ] No skipped tests without reason\n\n### Quality Checks\n- [ ] Lint output clean (or warnings documented)\n- [ ] Format check passing\n- [ ] Type check passing\n- [ ] Security scan results\n\n### Build Artifacts\n- [ ] Build log captured\n- [ ] Binary checksums generated\n- [ ] Artifact sizes documented","start_line":1,"end_line":22}]}
{"source_id":"core","type":"evidence","title":"Git Diff Capture Best Practices","signals":["evidence_audit"],"tags":["git","diff","evidence"],"summary":"Best practices for capturing meaningful git diffs as evidence of work completed.","chunks":[{"chunk_id":"core:git-diff:1","text":"## Git Diff Capture Best Practices\n\n### What to Capture\n- Staged changes: `git diff --cached`\n- All changes: `git diff HEAD`\n- Specific files: `git diff -- path/to/file`\n- Summary stats: `git diff --stat`\n\n### Diff Formatting\n- Include file names and line numbers\n- Context of 3-5 lines around changes\n- Binary files noted but not diffed\n- Large diffs split by component\n\n### Storage\n- Save as .patch files for replay\n- Timestamp in filename\n- Compress if > 1MB\n- Link to commit SHA","start_line":1,"end_line":18}]}
{"source_id":"core","type":"audit","title":"Audit Event Schema Guidance","signals":["evidence_audit"],"tags":["audit","schema","logging"],"summary":"Guidance for designing audit event schemas with required fields, optional metadata, and privacy considerations.","chunks":[{"chunk_id":"core:audit-schema:1","text":"## Audit Event Schema\n\n### Required Fields\n- timestamp: ISO 8601 format with timezone\n- event_type: Enum of known event types\n- actor: User or system identifier\n- action: What was done (create, update, delete)\n- resource: What was affected\n- session_id: Correlation identifier\n\n### Optional Metadata\n- ip_address: For security audits\n- user_agent: Client identification\n- duration_ms: For performance tracking\n- previous_value: For change tracking\n- new_value: For change tracking (redacted if sensitive)\n\n### Privacy\n- No PII in audit logs by default\n- Redact sensitive fields\n- Retention policy defined","start_line":1,"end_line":20}]}
{"source_id":"core","type":"audit","title":"Session Naming Conventions","signals":["evidence_audit"],"tags":["session","naming","convention"],"summary":"Naming conventions for development sessions to enable clear tracking and correlation of work.","chunks":[{"chunk_id":"core:session-naming:1","text":"## Session Naming Conventions\n\n### Format\n- Pattern: {date}_{type}_{short-desc}_{id}\n- Example: 2024-01-15_feat_auth-flow_a1b2c3\n\n### Types\n- feat: New feature development\n- fix: Bug fix session\n- refactor: Code improvement\n- docs: Documentation work\n- test: Test writing session\n- build: Build/CI changes\n\n### Best Practices\n- Keep description under 20 chars\n- Use lowercase with hyphens\n- Include random suffix for uniqueness\n- Never reuse session IDs","start_line":1,"end_line":18}]}
{"source_id":"core","type":"audit","title":"Audit Log Retention Policies","signals":["evidence_audit"],"tags":["audit","retention","compliance"],"summary":"Guidelines for audit log retention periods based on compliance requirements and business needs.","chunks":[{"chunk_id":"core:audit-retention:1","text":"## Audit Log Retention Policies\n\n### Standard Retention\n- Development logs: 30 days\n- Production logs: 90 days\n- Security events: 1 year\n- Financial transactions: 7 years\n\n### Compliance Requirements\n- GDPR: Right to be forgotten considerations\n- SOC2: Minimum 1 year for security events\n- HIPAA: 6 years for healthcare data\n- PCI-DSS: 1 year minimum\n\n### Storage Strategy\n- Hot storage: Last 7 days (fast query)\n- Warm storage: 8-90 days (compressed)\n- Cold storage: 90+ days (archived)\n- Deletion: Automated after retention period","start_line":1,"end_line":18}]}
{"source_id":"core","type":"iterate","title":"Diagnose Failing Tests Playbook","signals":["iterate_loop"],"tags":["testing","debug","playbook"],"summary":"Step-by-step playbook for diagnosing and understanding failing tests before attempting fixes.","chunks":[{"chunk_id":"core:diagnose-tests:1","text":"## Diagnose Failing Tests Playbook\n\n### Step 1: Read the Error\n- Full error message and stack trace\n- Expected vs actual values\n- Which assertion failed\n\n### Step 2: Reproduce Locally\n- Run the specific test in isolation\n- Check if it's flaky (run 3x)\n- Verify test data/fixtures are correct\n\n### Step 3: Identify Root Cause\n- Is it a test bug or code bug?\n- Did recent changes break it?\n- Is it an environment issue?\n\n### Step 4: Categorize\n- Assertion failure: Logic bug\n- Timeout: Performance or async issue\n- Setup failure: Environment or fixture\n- Random failure: Flaky test or race condition","start_line":1,"end_line":20}]}
{"source_id":"core","type":"iterate","title":"Fix Strategy Ladder","signals":["iterate_loop"],"tags":["fixing","strategy","debugging"],"summary":"Graduated approach to fixing issues from quick fixes to larger refactors, with clear escalation criteria.","chunks":[{"chunk_id":"core:fix-ladder:1","text":"## Fix Strategy Ladder\n\n### Level 1: Quick Fix (< 15 min)\n- Typo or minor syntax error\n- Missing import or dependency\n- Configuration value wrong\n- Simple logic inversion\n\n### Level 2: Standard Fix (15-60 min)\n- Logic bug in single function\n- Missing edge case handling\n- Incorrect API usage\n- Test fixture needs update\n\n### Level 3: Medium Refactor (1-4 hours)\n- Multiple related bugs\n- Interface change needed\n- Better abstraction required\n- Significant test updates\n\n### Level 4: Large Refactor (> 4 hours)\n- Architectural issue\n- Create separate ticket\n- Discuss with team first\n- May need design review","start_line":1,"end_line":24}]}
{"source_id":"core","type":"iterate","title":"Max Iterations Policy","signals":["iterate_loop"],"tags":["iterate","policy","limits"],"summary":"Policy for setting maximum iteration limits in automated test-fix loops to prevent infinite cycles.","chunks":[{"chunk_id":"core:max-iterations:1","text":"## Max Iterations Policy\n\n### Recommended Limits\n- Simple lint fixes: 3 iterations\n- Unit test fixes: 5 iterations\n- Integration tests: 10 iterations\n- Full build cycle: 10 iterations\n\n### Escalation Triggers\n- Same error 3 times in a row\n- No progress after 50% of max\n- Error count increasing\n- Timeout exceeded\n\n### On Limit Reached\n1. Stop automated fixing\n2. Save current state\n3. Generate diagnostic report\n4. Flag for human review\n5. Log all attempted fixes","start_line":1,"end_line":19}]}
{"source_id":"core","type":"iterate","title":"Strict Mode Guidelines","signals":["iterate_loop"],"tags":["strict","quality","gates"],"summary":"Guidelines for strict mode in iterate loops where any failure stops the process immediately.","chunks":[{"chunk_id":"core:strict-mode:1","text":"## Strict Mode Guidelines\n\n### When to Use Strict Mode\n- Critical production fixes\n- Security-sensitive code\n- Financial calculations\n- Data migration scripts\n- Release candidates\n\n### Strict Mode Behavior\n- Fail on first error (no retry)\n- Warnings treated as errors\n- All lints must pass\n- No skipped tests allowed\n- Coverage must not decrease\n\n### When to Relax\n- Early prototyping\n- Exploratory development\n- Non-critical documentation\n- Known flaky test exclusions","start_line":1,"end_line":19}]}
{"source_id":"core","type":"iterate","title":"Test-Lint-Fix Loop Pattern","signals":["iterate_loop"],"tags":["testing","lint","automation"],"summary":"Standard pattern for automated test-lint-fix loops with ordering and failure handling.","chunks":[{"chunk_id":"core:test-lint-fix:1","text":"## Test-Lint-Fix Loop Pattern\n\n### Execution Order\n1. Format check (fastest)\n2. Lint check\n3. Type check\n4. Unit tests\n5. Integration tests (if unit passes)\n\n### On Failure\n- Capture full output\n- Parse error locations\n- Attempt targeted fix\n- Re-run failed check only\n- Success: continue to next check\n- Fail again: increment attempt counter\n\n### Exit Conditions\n- All checks pass: Success\n- Max iterations reached: Stop with report\n- Unrecoverable error: Stop immediately\n- User interrupt: Save state and exit","start_line":1,"end_line":20}]}
{"source_id":"core","type":"security","title":"Secret Redaction Rules","signals":["security_pattern"],"tags":["security","secrets","redaction"],"summary":"Rules for automatically detecting and redacting secrets from logs, diffs, and outputs.","chunks":[{"chunk_id":"core:secret-redaction:1","text":"## Secret Redaction Rules\n\n### Detection Patterns\n- API keys: sk-*, pk-*, api_key=, apikey:\n- AWS: AKIA*, aws_secret_access_key\n- GitHub: ghp_*, gho_*, github_token\n- Bearer tokens: Bearer *, Authorization: *\n- Passwords: password=, passwd:, pwd=\n- Private keys: -----BEGIN * PRIVATE KEY-----\n- Connection strings: ://user:pass@\n\n### Redaction Format\n- Replace with: [REDACTED]\n- Or: ***HIDDEN***\n- Keep type hint: [REDACTED:API_KEY]\n\n### What NOT to Redact\n- Public keys\n- Non-secret environment variables\n- Documentation examples\n- Test fixture placeholders","start_line":1,"end_line":20}]}
{"source_id":"core","type":"security","title":"Ignore and Allow Glob Patterns","signals":["security_pattern"],"tags":["security","glob","configuration"],"summary":"Guidance for configuring ignore and allow glob patterns for file operations and harvesting.","chunks":[{"chunk_id":"core:glob-patterns:1","text":"## Ignore and Allow Glob Patterns\n\n### Common Ignore Patterns\n- node_modules/**\n- .git/**\n- *.log\n- *.tmp\n- .env*\n- dist/**\n- build/**\n- coverage/**\n- *.min.js\n- vendor/**\n\n### Allow Patterns (override ignores)\n- !.env.example\n- !.github/**\n- !docs/**\n\n### Best Practices\n- Be specific, avoid ** alone\n- Test patterns before applying\n- Document unusual patterns\n- Review regularly for drift","start_line":1,"end_line":23}]}
{"source_id":"core","type":"security","title":"Safe Subprocess Execution Checklist","signals":["security_pattern"],"tags":["security","subprocess","safety"],"summary":"Checklist for safely executing subprocesses and external commands in applications.","chunks":[{"chunk_id":"core:subprocess-safety:1","text":"## Safe Subprocess Execution Checklist\n\n### Before Execution\n- [ ] Validate all input parameters\n- [ ] Avoid shell=true when possible\n- [ ] Use absolute paths for executables\n- [ ] Set explicit working directory\n- [ ] Define timeout limits\n\n### During Execution\n- [ ] Capture stdout and stderr\n- [ ] Monitor resource usage\n- [ ] Handle signals gracefully\n- [ ] Stream output for long processes\n\n### After Execution\n- [ ] Check exit code\n- [ ] Validate output format\n- [ ] Clean up temp files\n- [ ] Log execution details (redacted)\n\n### Never Do\n- Concatenate user input into commands\n- Trust subprocess output blindly\n- Ignore non-zero exit codes","start_line":1,"end_line":24}]}
{"source_id":"core","type":"security","title":"Environment Variable Security","signals":["security_pattern"],"tags":["security","environment","configuration"],"summary":"Best practices for handling environment variables securely in applications.","chunks":[{"chunk_id":"core:env-security:1","text":"## Environment Variable Security\n\n### Sensitive Variables\n- Never log or print\n- Never include in error messages\n- Never commit to source control\n- Rotate regularly\n\n### Loading Order\n1. Defaults (non-sensitive)\n2. Config files (checked in)\n3. Environment variables (override)\n4. Command-line args (highest priority)\n\n### Validation\n- Check required vars on startup\n- Fail fast if missing critical vars\n- Provide helpful error messages\n- Document all required variables\n\n### Examples\n- .env.example with placeholders\n- README with setup instructions\n- CI/CD secret management","start_line":1,"end_line":22}]}
{"source_id":"core","type":"release","title":"Release Artifact Naming Conventions","signals":["release_install"],"tags":["release","naming","distribution"],"summary":"Standard naming conventions for release artifacts to ensure clarity and prevent confusion.","chunks":[{"chunk_id":"core:artifact-naming:1","text":"## Release Artifact Naming Conventions\n\n### Binary Naming\n- Pattern: {name}-{os}-{arch}[.ext]\n- Examples:\n  - myapp-linux-x64\n  - myapp-macos-arm64\n  - myapp-windows-x64.exe\n\n### Archive Naming\n- Pattern: {name}-{version}-{os}-{arch}.{ext}\n- Examples:\n  - myapp-1.2.3-linux-x64.tar.gz\n  - myapp-1.2.3-windows-x64.zip\n\n### OS Values\n- linux, darwin/macos, windows\n\n### Arch Values\n- x64/x86_64, arm64/aarch64\n- Prefer shorter forms for filenames","start_line":1,"end_line":20}]}
{"source_id":"core","type":"release","title":"Checksum Verification Patterns","signals":["release_install"],"tags":["release","checksum","security"],"summary":"Patterns for generating and verifying checksums in release workflows.","chunks":[{"chunk_id":"core:checksum-patterns:1","text":"## Checksum Verification Patterns\n\n### Generation\n- SHA256 for all release binaries\n- One line per file: {hash}  {filename}\n- File: checksums.txt or SHA256SUMS\n\n### Verification (Unix)\n```\ncurl -LO release.tar.gz\ncurl -LO checksums.txt\nsha256sum -c checksums.txt\n```\n\n### Verification (Windows)\n```powershell\n$hash = (Get-FileHash release.zip).Hash\n$expected = (Get-Content checksums.txt | Select-String release.zip)\nif ($hash -ne $expected) { throw \"Mismatch\" }\n```\n\n### Best Practices\n- Sign checksums file with GPG\n- Include in release notes\n- Automate in CI/CD","start_line":1,"end_line":24}]}
{"source_id":"core","type":"release","title":"Self-Update Safety Patterns","signals":["release_install"],"tags":["release","update","safety"],"summary":"Safety patterns for implementing self-update functionality in CLI tools.","chunks":[{"chunk_id":"core:self-update:1","text":"## Self-Update Safety Patterns\n\n### Pre-Update Checks\n- Verify version is actually newer\n- Check disk space available\n- Validate download URL domain\n- Verify checksum before replacing\n\n### Update Process\n1. Download to temp location\n2. Verify checksum\n3. Backup current binary\n4. Atomic replace (rename)\n5. Verify new binary runs\n6. Clean up temp files\n\n### Rollback Strategy\n- Keep .bak of previous version\n- Auto-rollback if new version fails\n- Provide manual rollback command\n\n### Platform Notes\n- Windows: Can't replace running exe directly\n- Unix: Can replace, but restart required","start_line":1,"end_line":22}]}
{"source_id":"core","type":"release","title":"Install Script Best Practices","signals":["release_install"],"tags":["release","install","distribution"],"summary":"Best practices for creating reliable cross-platform install scripts.","chunks":[{"chunk_id":"core:install-scripts:1","text":"## Install Script Best Practices\n\n### Unix (curl | bash)\n- Detect OS and architecture\n- Verify curl/wget available\n- Download to temp first\n- Verify checksum\n- Install to ~/.local/bin or /usr/local/bin\n- Prompt for sudo if needed\n- Update PATH instructions\n\n### Windows (PowerShell)\n- Use Invoke-RestMethod for download\n- Detect architecture (x64/arm64)\n- Install to ~\\.local\\bin\n- Use native Write-Host for colors\n- Add to PATH guidance\n\n### Both Platforms\n- Fail loudly on errors\n- Provide uninstall instructions\n- Show version after install\n- Support --version flag immediately","start_line":1,"end_line":21}]}
{"source_id":"core","type":"template","title":"Changelog Entry Format","signals":["release_install"],"tags":["changelog","release","documentation"],"summary":"Standard format for changelog entries following Keep a Changelog conventions.","chunks":[{"chunk_id":"core:changelog-format:1","text":"## Changelog Entry Format\n\n### Structure\n- Group by version (newest first)\n- Date in ISO format (YYYY-MM-DD)\n- Categories: Added, Changed, Deprecated, Removed, Fixed, Security\n\n### Example\n```markdown\n## [1.2.0] - 2024-01-15\n\n### Added\n- New `brain search` command for semantic search\n- Core BrainPack shipped with CLI\n\n### Fixed\n- PowerShell color rendering on Windows\n- Memory leak in harvest command\n\n### Security\n- Updated dependencies to fix CVE-2024-XXXX\n```\n\n### Best Practices\n- Use imperative mood (Add, Fix, not Added, Fixed)\n- Link to issues/PRs\n- No internal jargon","start_line":1,"end_line":25}]}
{"source_id":"core","type":"template","title":"PR Description Template","signals":["evidence_audit"],"tags":["template","pr","documentation"],"summary":"Pull request description template for clear communication of changes.","chunks":[{"chunk_id":"core:pr-template:1","text":"## Pull Request Description Template\n\n### What\n[One paragraph describing the change]\n\n### Why\n[Business or technical reason for the change]\n\n### How\n[Brief technical approach taken]\n\n### Testing\n- [ ] Unit tests added/updated\n- [ ] Integration tests pass\n- [ ] Manual testing completed\n\n### Checklist\n- [ ] Code follows style guidelines\n- [ ] Documentation updated\n- [ ] No breaking changes (or documented)\n- [ ] Reviewed by: @teammate\n\n### Screenshots\n[If UI changes, include before/after]","start_line":1,"end_line":22}]}
{"source_id":"core","type":"template","title":"Bug Report Template","signals":["evidence_audit"],"tags":["template","bug","documentation"],"summary":"Template for filing clear and actionable bug reports.","chunks":[{"chunk_id":"core:bug-template:1","text":"## Bug Report Template\n\n### Description\n[Clear, concise description of the bug]\n\n### Steps to Reproduce\n1. [First step]\n2. [Second step]\n3. [Third step]\n\n### Expected Behavior\n[What should happen]\n\n### Actual Behavior\n[What actually happens]\n\n### Environment\n- OS: [e.g., Windows 11, macOS 14]\n- Version: [e.g., v1.2.3]\n- Shell: [e.g., PowerShell 7, bash 5]\n\n### Additional Context\n- Screenshots if applicable\n- Error logs (redacted)\n- Related issues","start_line":1,"end_line":24}]}
{"source_id":"core","type":"plan","title":"Code Review Checklist","signals":["evidence_audit"],"tags":["review","checklist","quality"],"summary":"Comprehensive code review checklist covering functionality, code quality, testing, and security.","chunks":[{"chunk_id":"core:review-checklist:1","text":"## Code Review Checklist\n\n### Functionality\n- [ ] Code does what it claims to do\n- [ ] Edge cases handled\n- [ ] Error handling appropriate\n- [ ] No obvious logic bugs\n\n### Code Quality\n- [ ] Clear naming for variables/functions\n- [ ] No unnecessary complexity\n- [ ] DRY principle followed\n- [ ] Comments explain \"why\" not \"what\"\n\n### Testing\n- [ ] Tests cover new functionality\n- [ ] Tests are readable and maintainable\n- [ ] No flaky tests introduced\n- [ ] Edge cases tested\n\n### Security\n- [ ] No secrets in code\n- [ ] Input validation present\n- [ ] No SQL injection vectors\n- [ ] Dependencies checked for CVEs","start_line":1,"end_line":24}]}
{"source_id":"core","type":"iterate","title":"Common Lint Errors and Fixes","signals":["iterate_loop"],"tags":["lint","errors","fixing"],"summary":"Common lint errors encountered during development and their typical fixes.","chunks":[{"chunk_id":"core:lint-fixes:1","text":"## Common Lint Errors and Fixes\n\n### Unused Variables\n- Error: Variable defined but never used\n- Fix: Remove or prefix with underscore (_unused)\n\n### Missing Return Type\n- Error: Function missing explicit return type\n- Fix: Add return type annotation\n\n### Unused Imports\n- Error: Import not used in file\n- Fix: Remove unused import statements\n\n### Line Too Long\n- Error: Line exceeds maximum length\n- Fix: Break into multiple lines, extract variable\n\n### Mixed Indentation\n- Error: Tabs and spaces mixed\n- Fix: Use consistent indentation (prefer spaces)","start_line":1,"end_line":20}]}
{"source_id":"core","type":"iterate","title":"Build Error Resolution Guide","signals":["iterate_loop"],"tags":["build","errors","debugging"],"summary":"Guide for resolving common build errors across different build systems.","chunks":[{"chunk_id":"core:build-errors:1","text":"## Build Error Resolution Guide\n\n### Missing Dependency\n- Error: Cannot find module/crate\n- Fix: Add to package.json/Cargo.toml, run install\n\n### Version Mismatch\n- Error: Incompatible versions\n- Fix: Check lock file, update or pin versions\n\n### Type Error\n- Error: Type mismatch or missing\n- Fix: Correct type annotations, add conversions\n\n### Linking Error\n- Error: Undefined symbol\n- Fix: Check feature flags, add missing native deps\n\n### Out of Memory\n- Error: Build killed/OOM\n- Fix: Increase memory, use incremental builds","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Definition of Done","signals":["contract_lock"],"tags":["criteria","done","checklist"],"summary":"Standard definition of done checklist for completing work items.","chunks":[{"chunk_id":"core:definition-done:1","text":"## Definition of Done\n\n### Code Complete\n- [ ] All acceptance criteria met\n- [ ] Code reviewed and approved\n- [ ] Tests written and passing\n- [ ] No known bugs introduced\n\n### Quality Complete\n- [ ] Lint and format checks pass\n- [ ] Coverage target maintained\n- [ ] No new security warnings\n- [ ] Performance within bounds\n\n### Documentation Complete\n- [ ] Code comments updated\n- [ ] README updated if needed\n- [ ] Changelog entry added\n- [ ] API docs regenerated\n\n### Deployment Ready\n- [ ] Merged to main branch\n- [ ] CI/CD pipeline green\n- [ ] Ready for release","start_line":1,"end_line":24}]}
{"source_id":"core","type":"contract","title":"MOSCOW Prioritization Guide","signals":["contract_lock"],"tags":["prioritization","planning","requirements"],"summary":"Guide for using MOSCOW method to prioritize requirements in contracts.","chunks":[{"chunk_id":"core:moscow-guide:1","text":"## MOSCOW Prioritization\n\n### MUST Have\n- Critical for delivery\n- System won't work without\n- Cannot workaround\n- Legal/compliance requirement\n\n### SHOULD Have\n- Important but not critical\n- Workaround exists\n- Expected by users\n- Significant value add\n\n### COULD Have\n- Nice to have\n- Low impact if missing\n- Can defer to later phase\n\n### WON'T Have (this time)\n- Explicitly excluded\n- Deferred to future scope\n- Document why excluded","start_line":1,"end_line":22}]}
{"source_id":"core","type":"evidence","title":"Test Output Capture Standards","signals":["evidence_audit"],"tags":["testing","output","evidence"],"summary":"Standards for capturing and preserving test output as evidence.","chunks":[{"chunk_id":"core:test-output:1","text":"## Test Output Capture Standards\n\n### What to Capture\n- Full stdout and stderr\n- Exit code\n- Duration of each test\n- Memory usage (if monitoring)\n- Timestamp of run\n\n### Output Format\n- Plain text for logs\n- JSON for machine processing\n- JUnit XML for CI integration\n\n### Storage\n- Session directory with timestamp\n- Compress after 24 hours\n- Retain for audit period\n- Include in evidence bundle","start_line":1,"end_line":18}]}
{"source_id":"core","type":"security","title":"CORS Configuration Patterns","signals":["security_pattern"],"tags":["security","cors","web"],"summary":"Cross-Origin Resource Sharing configuration patterns for web APIs.","chunks":[{"chunk_id":"core:cors-patterns:1","text":"## CORS Configuration Patterns\n\n### Restrictive (Recommended)\n- Allow-Origin: Specific domains only\n- Allow-Methods: Only needed methods\n- Allow-Headers: Specific required headers\n- Max-Age: Cache preflight for 24h\n\n### Permissive (Development Only)\n- Allow-Origin: *\n- Allow-Credentials: false (required with *)\n- Never use in production\n\n### Best Practices\n- Validate Origin header server-side\n- Don't trust only client validation\n- List allowed origins explicitly\n- Audit CORS config regularly","start_line":1,"end_line":17}]}
{"source_id":"core","type":"security","title":"Input Validation Patterns","signals":["security_pattern"],"tags":["security","validation","input"],"summary":"Common input validation patterns to prevent security vulnerabilities.","chunks":[{"chunk_id":"core:input-validation:1","text":"## Input Validation Patterns\n\n### String Inputs\n- Max length check\n- Allowed characters whitelist\n- Trim whitespace\n- Normalize unicode\n\n### Numeric Inputs\n- Range validation\n- Type coercion safety\n- Overflow prevention\n\n### File Inputs\n- Extension whitelist\n- MIME type validation\n- Max size limit\n- Virus scanning for uploads\n\n### URL Inputs\n- Protocol whitelist (https only)\n- Domain validation\n- Path traversal prevention\n- No localhost/internal IPs","start_line":1,"end_line":23}]}
{"source_id":"core","type":"template","title":"Post-Mortem Template","signals":["evidence_audit"],"tags":["template","incident","retrospective"],"summary":"Template for documenting and learning from production incidents.","chunks":[{"chunk_id":"core:postmortem-template:1","text":"## Post-Mortem Template\n\n### Incident Summary\n- Date/Time: [When it happened]\n- Duration: [How long]\n- Impact: [What was affected]\n- Severity: [P1/P2/P3/P4]\n\n### Timeline\n- [HH:MM] First alert triggered\n- [HH:MM] Team notified\n- [HH:MM] Root cause identified\n- [HH:MM] Mitigation applied\n- [HH:MM] Fully resolved\n\n### Root Cause\n[What actually caused the incident]\n\n### Action Items\n- [ ] Immediate fix\n- [ ] Preventive measures\n- [ ] Monitoring improvements\n- [ ] Documentation updates\n\n### Lessons Learned\n[What we learned, no blame]","start_line":1,"end_line":25}]}
{"source_id":"core","type":"plan","title":"Sprint Planning Template","signals":["state_machine"],"tags":["planning","sprint","agile"],"summary":"Template for sprint planning with capacity, goals, and commitment tracking.","chunks":[{"chunk_id":"core:sprint-template:1","text":"## Sprint Planning Template\n\n### Sprint Overview\n- Sprint: [Number]\n- Dates: [Start] to [End]\n- Capacity: [Story points available]\n\n### Sprint Goal\n[One clear, measurable goal]\n\n### Committed Items\n| ID | Title | Points | Owner |\n|----|-------|--------|-------|\n| T-1 | Feature X | 5 | @dev |\n| T-2 | Bug fix Y | 2 | @dev |\n\n### Stretch Goals\n[Items to pick up if time permits]\n\n### Dependencies\n- [External team/service dependencies]\n\n### Risks\n- [Known risks and mitigation]","start_line":1,"end_line":23}]}
{"source_id":"core","type":"iterate","title":"Performance Debugging Playbook","signals":["iterate_loop"],"tags":["performance","debugging","profiling"],"summary":"Playbook for diagnosing and fixing performance issues.","chunks":[{"chunk_id":"core:perf-debug:1","text":"## Performance Debugging Playbook\n\n### Step 1: Quantify\n- Measure current baseline\n- Define target improvement\n- Identify bottleneck metric (CPU, memory, I/O)\n\n### Step 2: Profile\n- Use profiler for CPU-bound\n- Memory analyzer for leaks\n- Trace for I/O-bound\n- Flame graphs for visualization\n\n### Step 3: Analyze\n- Find hotspots (top 20%)\n- Look for N+1 queries\n- Check cache hit rates\n- Review algorithm complexity\n\n### Step 4: Fix\n- Optimize hot paths first\n- Add caching where beneficial\n- Batch database queries\n- Lazy load where possible","start_line":1,"end_line":22}]}
{"source_id":"core","type":"evidence","title":"Coverage Report Interpretation","signals":["evidence_audit","iterate_loop"],"tags":["testing","coverage","metrics"],"summary":"How to interpret and act on code coverage reports.","chunks":[{"chunk_id":"core:coverage-interpret:1","text":"## Coverage Report Interpretation\n\n### Metrics Explained\n- Line coverage: % of lines executed\n- Branch coverage: % of decision paths\n- Function coverage: % of functions called\n\n### Healthy Targets\n- 80% overall is good baseline\n- 95% for critical paths\n- 100% for security-sensitive code\n\n### What to Ignore\n- Generated code\n- Vendor/third-party code\n- Unreachable error handlers\n- Deprecated code paths\n\n### Red Flags\n- Coverage drops on PR\n- Critical code uncovered\n- Tests exist but coverage low (assertions missing)","start_line":1,"end_line":20}]}
{"source_id":"core","type":"release","title":"Semantic Versioning Guide","signals":["release_install"],"tags":["versioning","release","semver"],"summary":"Guide to semantic versioning for release management.","chunks":[{"chunk_id":"core:semver-guide:1","text":"## Semantic Versioning Guide\n\n### Version Format: MAJOR.MINOR.PATCH\n\n### MAJOR (breaking)\n- Remove public API\n- Change behavior incompatibly\n- Major dependency update\n\n### MINOR (feature)\n- Add new capability\n- Deprecate (not remove) API\n- Backward-compatible changes\n\n### PATCH (fix)\n- Bug fixes\n- Security patches\n- Documentation updates\n- Performance improvements\n\n### Pre-release\n- Alpha: 1.0.0-alpha.1 (unstable)\n- Beta: 1.0.0-beta.1 (feature complete)\n- RC: 1.0.0-rc.1 (release candidate)","start_line":1,"end_line":22}]}
{"source_id":"core","type":"plan","title":"Technical Debt Tracking","signals":["state_machine"],"tags":["planning","debt","maintenance"],"summary":"Framework for identifying, prioritizing, and addressing technical debt.","chunks":[{"chunk_id":"core:tech-debt:1","text":"## Technical Debt Tracking\n\n### Categories\n- Code debt: Hacks, workarounds, copy-paste\n- Design debt: Poor abstractions, missing patterns\n- Test debt: Missing coverage, flaky tests\n- Dependency debt: Outdated packages, vulnerabilities\n- Documentation debt: Outdated docs, missing examples\n\n### Prioritization\n- Pain level (1-5): How much it hurts development\n- Effort (1-5): How much work to fix\n- Risk (1-5): Consequences if not addressed\n- Priority = Pain × Risk / Effort\n\n### Tracking\n- Tag tickets with tech-debt label\n- Allocate 20% sprint capacity\n- Track debt reduction over time\n- Celebrate debt paydown","start_line":1,"end_line":19}]}
{"source_id":"core","type":"security","title":"Dependency Scanning Patterns","signals":["security_pattern"],"tags":["security","dependencies","scanning"],"summary":"Patterns for scanning and managing dependency vulnerabilities.","chunks":[{"chunk_id":"core:dep-scanning:1","text":"## Dependency Scanning Patterns\n\n### Scan Frequency\n- On every PR (blocking)\n- Daily for main branch\n- Weekly full audit\n\n### Severity Handling\n- Critical: Block merge, fix immediately\n- High: Fix within 48 hours\n- Medium: Fix within 1 week\n- Low: Track, fix when convenient\n\n### Safe Update Strategy\n1. Review changelog\n2. Check for breaking changes\n3. Run test suite\n4. Deploy to staging first\n5. Monitor for issues\n\n### Tools\n- Native: npm audit, cargo audit\n- Dedicated: Dependabot, Snyk, Renovate","start_line":1,"end_line":21}]}
{"source_id":"core","type":"contract","title":"API Versioning Strategy","signals":["contract_lock"],"tags":["api","versioning","contract"],"summary":"Strategies for versioning APIs to maintain backward compatibility.","chunks":[{"chunk_id":"core:api-versioning:1","text":"## API Versioning Strategy\n\n### Methods\n- URL path: /api/v1/resource (recommended)\n- Header: Accept-Version: v1\n- Query param: ?version=1 (not recommended)\n\n### When to Version\n- Breaking response format change\n- Removing fields or endpoints\n- Changing authentication method\n- Incompatible behavior change\n\n### Deprecation Process\n1. Announce deprecation (90 days notice)\n2. Add Deprecation header\n3. Log usage of deprecated endpoints\n4. Notify active users directly\n5. Remove after notice period\n\n### Best Practices\n- Support N and N-1 versions\n- Clear migration guides\n- Sunset dates documented","start_line":1,"end_line":22}]}
{"source_id":"core","type":"template","title":"README Structure Template","signals":["evidence_audit"],"tags":["template","readme","documentation"],"summary":"Standard structure for project README files.","chunks":[{"chunk_id":"core:readme-template:1","text":"## README Structure Template\n\n### Essential Sections\n1. Title and badges\n2. One-line description\n3. Quick install command\n4. Minimal usage example\n5. Link to full documentation\n\n### Recommended Sections\n- Features list\n- Requirements/Prerequisites\n- Configuration options\n- Contributing guidelines\n- License information\n- Support channels\n\n### Best Practices\n- Keep main README concise\n- Link to detailed docs\n- Update with each release\n- Include screenshots for UI","start_line":1,"end_line":20}]}
{"source_id":"core","type":"provider","title":"AI Provider Adapter Pattern","signals":["provider_adapter"],"tags":["ai","provider","adapter","pattern"],"summary":"Pattern for creating abstracted AI provider adapters that can switch between different LLM backends.","chunks":[{"chunk_id":"core:provider-adapter:1","text":"## AI Provider Adapter Pattern\n\n### Interface Design\n- Define common interface/trait for all providers\n- Methods: send_prompt, stream_response, get_capabilities\n- Return unified response format\n\n### Implementation\n- Factory pattern for provider instantiation\n- Configuration-driven provider selection\n- Graceful fallback on provider failure\n\n### Example Providers\n- Claude (Anthropic)\n- GPT (OpenAI)\n- Local LLMs (Ollama)\n- Mock provider (testing)\n\n### Configuration\n- Environment variable for default\n- CLI flag override: --provider\n- Provider-specific settings in config","start_line":1,"end_line":20}]}
{"source_id":"core","type":"provider","title":"Provider Registry Pattern","signals":["provider_adapter"],"tags":["registry","plugin","extensibility"],"summary":"Registry pattern for dynamically registering and discovering providers.","chunks":[{"chunk_id":"core:provider-registry:1","text":"## Provider Registry Pattern\n\n### Purpose\n- Central catalog of available providers\n- Dynamic discovery and registration\n- Lazy initialization on first use\n\n### Implementation\n- Global HashMap/registry of provider factories\n- Register at startup or via plugins\n- Lookup by name: registry.get(\"claude\")\n\n### Best Practices\n- Default provider in registry\n- Clear error on unknown provider\n- List available providers in help\n- Log provider selection at startup","start_line":1,"end_line":16}]}
{"source_id":"core","type":"provider","title":"Provider Response Streaming","signals":["provider_adapter"],"tags":["streaming","async","response"],"summary":"Patterns for streaming responses from AI providers in real-time.","chunks":[{"chunk_id":"core:provider-streaming:1","text":"## Provider Response Streaming\n\n### Why Stream\n- Better perceived latency\n- User feedback during generation\n- Early termination option\n- Memory efficiency for long outputs\n\n### Implementation\n- Async iterator/stream pattern\n- Chunk-by-chunk processing\n- Progress indicator during stream\n- Buffer for partial tokens\n\n### Error Handling\n- Timeout per chunk, not total\n- Reconnect on stream break\n- Save partial output on error\n- Clear termination signals","start_line":1,"end_line":18}]}
{"source_id":"core","type":"provider","title":"Provider Rate Limiting","signals":["provider_adapter","security_pattern"],"tags":["rate-limit","throttling","api"],"summary":"Client-side rate limiting patterns for API providers.","chunks":[{"chunk_id":"core:provider-ratelimit:1","text":"## Provider Rate Limiting\n\n### Client-Side Throttling\n- Track requests per minute\n- Queue requests when near limit\n- Exponential backoff on 429\n\n### Implementation\n- Token bucket algorithm\n- Sliding window counter\n- Priority queue for requests\n\n### Best Practices\n- Respect Retry-After header\n- Log rate limit events\n- Fallback to secondary provider\n- User notification on wait","start_line":1,"end_line":16}]}
{"source_id":"core","type":"cli","title":"Rust CLI Argument Parsing (Clap)","signals":["command_surface"],"tags":["rust","cli","clap","arguments"],"summary":"Best practices for argument parsing in Rust CLIs using derive macros.","chunks":[{"chunk_id":"core:rust-clap:1","text":"## Rust CLI with Clap\n\n### Derive Pattern\n- #[derive(Parser)] for main struct\n- #[derive(Subcommand)] for commands\n- #[derive(ValueEnum)] for enums\n\n### Field Attributes\n- #[arg(short, long)] for flags\n- #[arg(default_value)] for defaults\n- #[arg(value_enum)] for choices\n- #[command(subcommand)] for nested\n\n### Best Practices\n- Short and long flags for common options\n- Sensible defaults reduce required args\n- Help strings with #[arg(help = ...)]\n- Examples in about text","start_line":1,"end_line":18}]}
{"source_id":"core","type":"cli","title":"Go CLI with Cobra","signals":["command_surface"],"tags":["go","cli","cobra","arguments"],"summary":"Patterns for building CLI tools in Go using Cobra framework.","chunks":[{"chunk_id":"core:go-cobra:1","text":"## Go CLI with Cobra\n\n### Structure\n- cmd/ package for command files\n- Root command with PersistentFlags\n- Subcommands added via AddCommand\n\n### Flags\n- StringVar for string flags\n- BoolP for boolean with short\n- Viper integration for config\n\n### Best Practices\n- PreRun for validation\n- RunE to return errors\n- SilenceUsage for clean errors\n- Completions for shell support","start_line":1,"end_line":16}]}
{"source_id":"core","type":"cli","title":"Node CLI with Commander","signals":["command_surface"],"tags":["node","cli","commander","javascript"],"summary":"Patterns for building CLI tools in Node.js using Commander.","chunks":[{"chunk_id":"core:node-commander:1","text":"## Node CLI with Commander\n\n### Setup\n- program.name().description().version()\n- program.command() for subcommands\n- .option() for flags\n- .argument() for positional\n\n### Patterns\n- Async action handlers\n- Error handling with try/catch\n- process.exit for status codes\n- chalk/colors for output\n\n### Distribution\n- bin field in package.json\n- Shebang: #!/usr/bin/env node\n- npm link for local testing","start_line":1,"end_line":17}]}
{"source_id":"core","type":"cli","title":"CLI Output Formatting","signals":["command_surface"],"tags":["cli","output","formatting","colors"],"summary":"Patterns for formatting CLI output including colors, tables, and progress.","chunks":[{"chunk_id":"core:cli-output:1","text":"## CLI Output Formatting\n\n### Color Usage\n- Green: success, completion\n- Yellow: warnings, caution\n- Red: errors, failures\n- Cyan: info, progress\n- Dim: secondary information\n\n### Structured Output\n- Tables for lists/comparisons\n- JSON with --json flag\n- Progress bars for long ops\n- Spinners for indeterminate\n\n### Best Practices\n- Respect NO_COLOR env var\n- TTY detection for colors\n- Quiet mode (--quiet)\n- Verbose mode (-v, -vv)","start_line":1,"end_line":19}]}
{"source_id":"core","type":"cli","title":"CLI Exit Codes","signals":["command_surface"],"tags":["cli","exit-codes","errors"],"summary":"Standard exit code conventions for CLI tools.","chunks":[{"chunk_id":"core:cli-exitcodes:1","text":"## CLI Exit Codes\n\n### Standard Codes\n- 0: Success\n- 1: General error\n- 2: Invalid usage/arguments\n- 3: Configuration error\n- 4: Resource not found\n- 126: Permission denied\n- 127: Command not found\n\n### Best Practices\n- Document exit codes in help\n- Consistent across commands\n- Non-zero = failed\n- Scripts rely on exit codes","start_line":1,"end_line":16}]}
{"source_id":"core","type":"cli","title":"CLI Configuration Hierarchy","signals":["command_surface"],"tags":["cli","config","hierarchy"],"summary":"Configuration precedence patterns for CLI tools.","chunks":[{"chunk_id":"core:cli-config:1","text":"## CLI Configuration Hierarchy\n\n### Precedence (high to low)\n1. Command-line flags\n2. Environment variables\n3. Local config (./.toolrc)\n4. User config (~/.config/tool)\n5. System config (/etc/tool)\n6. Built-in defaults\n\n### Implementation\n- Layer configs, override in order\n- Clear provenance logging\n- --config to specify file\n- --no-config to skip files\n\n### Validation\n- Validate combined config\n- Report which source won\n- Fail fast on conflicts","start_line":1,"end_line":19}]}
{"source_id":"core","type":"cli","title":"Interactive Prompts","signals":["command_surface"],"tags":["cli","interactive","prompts"],"summary":"Patterns for interactive CLI prompts and wizards.","chunks":[{"chunk_id":"core:cli-prompts:1","text":"## Interactive Prompts\n\n### Prompt Types\n- Text input: single line\n- Password: hidden input\n- Confirm: yes/no\n- Select: choose from list\n- Multi-select: checkboxes\n\n### Best Practices\n- Default values for quick accept\n- Validation with helpful errors\n- Non-interactive fallback (--yes)\n- CI detection: skip prompts\n\n### UX\n- Clear prompts explain what's needed\n- Allow going back in wizards\n- Summary before final action","start_line":1,"end_line":18}]}
{"source_id":"core","type":"security","title":"Token Storage Best Practices","signals":["security_pattern"],"tags":["security","tokens","storage"],"summary":"Secure patterns for storing authentication tokens in CLI tools.","chunks":[{"chunk_id":"core:token-storage:1","text":"## Token Storage Best Practices\n\n### Secure Options (Preferred)\n- System keychain (keyring crate)\n- OS credential store\n- Encrypted config file\n\n### Fallback Options\n- File with 600 permissions\n- Environment variable (session only)\n- Never in command history\n\n### Implementation\n- Prompt for token if missing\n- Validate token on storage\n- Clear token on logout\n- Rotate token periodically","start_line":1,"end_line":17}]}
{"source_id":"core","type":"security","title":"Command Injection Prevention","signals":["security_pattern"],"tags":["security","injection","commands"],"summary":"Patterns to prevent command injection vulnerabilities.","chunks":[{"chunk_id":"core:cmd-injection:1","text":"## Command Injection Prevention\n\n### Never Do\n- shell=true with user input\n- String concatenation for commands\n- eval() on external input\n\n### Safe Patterns\n- Array of arguments (no shell)\n- Allowlist valid inputs\n- Escape special characters\n- Validate before execution\n\n### Example (safe)\n- Command::new(\"git\").arg(\"clone\").arg(url)\n- Not: format!(\"git clone {}\", url)","start_line":1,"end_line":16}]}
{"source_id":"core","type":"security","title":"Path Traversal Prevention","signals":["security_pattern"],"tags":["security","path","traversal"],"summary":"Preventing path traversal attacks in file operations.","chunks":[{"chunk_id":"core:path-traversal:1","text":"## Path Traversal Prevention\n\n### Attack Patterns\n- ../../../etc/passwd\n- Encoded: %2e%2e%2f\n- Null bytes: file.txt\\0.jpg\n\n### Defenses\n- Canonicalize paths\n- Check starts_with(allowed_root)\n- Reject .. components\n- Normalize before comparison\n\n### Implementation\n- PathBuf::canonicalize()\n- path.starts_with(&base_dir)\n- Reject absolute paths when relative expected","start_line":1,"end_line":16}]}
{"source_id":"core","type":"release","title":"Binary Stripping Best Practices","signals":["release_install"],"tags":["release","binary","optimization"],"summary":"Reducing binary size for release distributions.","chunks":[{"chunk_id":"core:binary-strip:1","text":"## Binary Stripping\n\n### Rust Release Optimization\n- strip = true in Cargo.toml\n- lto = true for link-time opt\n- opt-level = \"z\" for size\n- panic = \"abort\" (no unwinding)\n\n### Post-Build\n- strip --strip-all binary\n- UPX compression (optional)\n\n### Size Targets\n- CLI tools: < 10MB ideal\n- Plugins: < 5MB\n- WASM: < 1MB","start_line":1,"end_line":15}]}
{"source_id":"core","type":"release","title":"GitHub Actions Release Workflow","signals":["release_install"],"tags":["release","github-actions","ci"],"summary":"Pattern for automated releases via GitHub Actions.","chunks":[{"chunk_id":"core:gh-release-workflow:1","text":"## GitHub Actions Release Workflow\n\n### Trigger\n- on: push: tags: [\"v*.*.*\"]\n- Creates release on tag push\n\n### Jobs\n1. Build matrix: [linux, macos, windows] x [x64, arm64]\n2. Create checksums\n3. Create GitHub release\n4. Upload assets\n\n### Best Practices\n- Matrix for cross-compile\n- Artifact upload between jobs\n- Release notes from changelog\n- Draft release first, publish after verify","start_line":1,"end_line":17}]}
{"source_id":"core","type":"release","title":"Release Changelog Generation","signals":["release_install"],"tags":["release","changelog","automation"],"summary":"Automated changelog generation from commit history.","chunks":[{"chunk_id":"core:changelog-gen:1","text":"## Changelog Generation\n\n### Conventional Commits\n- feat: → Added section\n- fix: → Fixed section\n- docs: → Documentation\n- BREAKING CHANGE: → highlighted\n\n### Tools\n- git-cliff (Rust)\n- conventional-changelog (Node)\n- github-changelog-generator\n\n### Best Practices\n- Generate from tags\n- Include PR links\n- Categorize by type\n- Highlight breaking changes","start_line":1,"end_line":17}]}
{"source_id":"core","type":"template","title":"GitHub Issue Templates","signals":["evidence_audit"],"tags":["template","github","issues"],"summary":"Templates for GitHub issue forms to gather structured information.","chunks":[{"chunk_id":"core:issue-templates:1","text":"## GitHub Issue Templates\n\n### Bug Report Template\n- Description: What happened?\n- Steps to reproduce: 1, 2, 3\n- Expected vs actual behavior\n- Environment: OS, version, shell\n- Screenshots/logs if applicable\n\n### Feature Request Template\n- Problem to solve\n- Proposed solution\n- Alternatives considered\n- Additional context\n\n### Location\n- .github/ISSUE_TEMPLATE/\n- bug_report.yml\n- feature_request.yml","start_line":1,"end_line":18}]}
{"source_id":"core","type":"template","title":"Pull Request Template","signals":["evidence_audit"],"tags":["template","github","pr"],"summary":"Template for consistent pull request descriptions.","chunks":[{"chunk_id":"core:pr-template-file:1","text":"## Pull Request Template\n\n### Location\n- .github/PULL_REQUEST_TEMPLATE.md\n\n### Sections\n- What: Brief description of changes\n- Why: Reason/motivation for change\n- How: Technical approach taken\n- Testing: How this was tested\n- Checklist: Standard checks\n\n### Checklist Items\n- [ ] Tests added/updated\n- [ ] Docs updated\n- [ ] Changelog entry\n- [ ] Breaking changes noted","start_line":1,"end_line":17}]}
{"source_id":"core","type":"iterate","title":"Flaky Test Detection","signals":["iterate_loop"],"tags":["testing","flaky","detection"],"summary":"Patterns for detecting and handling flaky tests.","chunks":[{"chunk_id":"core:flaky-tests:1","text":"## Flaky Test Detection\n\n### Indicators\n- Passes locally, fails in CI\n- Fails intermittently\n- Order-dependent results\n- Time-sensitive assertions\n\n### Detection Strategy\n- Run test N times (e.g., 10x)\n- Different orders each run\n- Track historical pass rate\n- Flag tests with < 100% rate\n\n### Remediation\n- Fix root cause if possible\n- Quarantine while fixing\n- Add retry with backoff\n- Mark with skip reason","start_line":1,"end_line":18}]}
{"source_id":"core","type":"iterate","title":"Test Fixture Management","signals":["iterate_loop"],"tags":["testing","fixtures","data"],"summary":"Patterns for managing test fixtures and data.","chunks":[{"chunk_id":"core:test-fixtures:1","text":"## Test Fixture Management\n\n### Types\n- Static: JSON/YAML files in tests/\n- Generated: Created per test\n- Shared: Setup once, use many\n- Database: Seeded schemas\n\n### Best Practices\n- Isolate tests with unique fixtures\n- Clean up after tests\n- Version fixtures with code\n- Document fixture schema\n\n### Naming\n- test_data/scenario_name.json\n- fixtures/valid_user.json\n- mocks/api_response.json","start_line":1,"end_line":17}]}
{"source_id":"core","type":"iterate","title":"Test Timeout Configuration","signals":["iterate_loop"],"tags":["testing","timeout","configuration"],"summary":"Configuring appropriate timeouts for different test types.","chunks":[{"chunk_id":"core:test-timeouts:1","text":"## Test Timeout Configuration\n\n### Guidelines by Type\n- Unit tests: 100ms - 1s\n- Integration tests: 5-30s\n- E2E tests: 30s - 2min\n- Performance tests: as needed\n\n### Configuration\n- Default timeout per suite\n- Override per test annotation\n- Environment variable for CI\n\n### On Timeout\n- Kill and report failure\n- Log what was running\n- Check for resource leaks\n- Investigate slow tests","start_line":1,"end_line":16}]}
{"source_id":"core","type":"evidence","title":"Snapshot Testing Patterns","signals":["evidence_audit"],"tags":["testing","snapshots","evidence"],"summary":"Using snapshot testing for regression detection.","chunks":[{"chunk_id":"core:snapshot-testing:1","text":"## Snapshot Testing\n\n### Purpose\n- Capture expected output\n- Detect unintended changes\n- Review diffs easily\n\n### Implementation\n- Store snapshots alongside tests\n- Update with command flag\n- Review in PR diffs\n\n### Best Practices\n- Normalize timestamps/IDs\n- Human-readable formats\n- Commit snapshots to git\n- Small, focused snapshots","start_line":1,"end_line":16}]}
{"source_id":"core","type":"evidence","title":"Build Artifact Metadata","signals":["evidence_audit","release_install"],"tags":["build","artifacts","metadata"],"summary":"Recording metadata about build artifacts.","chunks":[{"chunk_id":"core:build-metadata:1","text":"## Build Artifact Metadata\n\n### What to Record\n- Commit SHA\n- Build timestamp\n- Builder (CI job ID)\n- Compiler version\n- Dependencies locked versions\n\n### Format\n- buildinfo.json alongside binary\n- Embedded in binary (--version)\n- Release notes attachment\n\n### Usage\n- Reproducibility verification\n- Bug report debugging\n- Audit trail","start_line":1,"end_line":16}]}
{"source_id":"core","type":"plan","title":"Dependency Update Strategy","signals":["state_machine"],"tags":["dependencies","planning","maintenance"],"summary":"Strategy for keeping dependencies updated safely.","chunks":[{"chunk_id":"core:dep-update:1","text":"## Dependency Update Strategy\n\n### Frequency\n- Security: immediately\n- Major: quarterly review\n- Minor/Patch: monthly batch\n\n### Process\n1. Review changelog\n2. Check breaking changes\n3. Update in feature branch\n4. Full test suite\n5. Staged rollout\n\n### Automation\n- Dependabot/Renovate for PRs\n- Auto-merge for patch versions\n- Require approval for major","start_line":1,"end_line":17}]}
{"source_id":"core","type":"plan","title":"Feature Flag Strategy","signals":["state_machine"],"tags":["feature-flags","planning","rollout"],"summary":"Using feature flags for safe feature rollout.","chunks":[{"chunk_id":"core:feature-flags:1","text":"## Feature Flag Strategy\n\n### Types\n- Release flags: hide incomplete features\n- Experiment flags: A/B testing\n- Ops flags: kill switches\n\n### Implementation\n- Boolean flag at minimum\n- Percentage rollout if needed\n- User targeting for betas\n\n### Lifecycle\n1. Create flag (default off)\n2. Develop behind flag\n3. Test with flag on\n4. Gradual rollout\n5. Clean up flag when stable","start_line":1,"end_line":17}]}
{"source_id":"core","type":"contract","title":"Data Migration Contract","signals":["contract_lock"],"tags":["migration","data","contract"],"summary":"Contract template for database migrations.","chunks":[{"chunk_id":"core:migration-contract:1","text":"## Data Migration Contract\n\n### Pre-Migration\n- [ ] Backup completed\n- [ ] Rollback script ready\n- [ ] Downtime window communicated\n- [ ] Dry run successful\n\n### Migration Steps\n- [ ] Run migration script\n- [ ] Verify data integrity\n- [ ] Update application code\n- [ ] Monitor for issues\n\n### Rollback Criteria\n- Data loss detected\n- Error rate > threshold\n- Performance degradation\n- User reports of issues","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Integration Points Contract","signals":["contract_lock"],"tags":["integration","api","contract"],"summary":"Defining integration points and responsibilities.","chunks":[{"chunk_id":"core:integration-contract:1","text":"## Integration Points Contract\n\n### For Each Integration\n- Endpoint/interface specification\n- Authentication method\n- Rate limits and quotas\n- Error handling expectations\n- SLA requirements\n\n### Responsibilities\n- Provider: uptime, documentation\n- Consumer: rate limit respect, error handling\n- Both: communication on changes\n\n### Change Management\n- 30 day notice for breaking changes\n- Version new endpoints\n- Maintain backward compatibility","start_line":1,"end_line":16}]}
{"source_id":"core","type":"security","title":"Logging Sensitive Data Rules","signals":["security_pattern"],"tags":["logging","security","privacy"],"summary":"Rules for what should never appear in logs.","chunks":[{"chunk_id":"core:log-security:1","text":"## Logging Sensitive Data Rules\n\n### Never Log\n- Passwords or password hashes\n- API keys or tokens\n- Credit card numbers\n- Social security numbers\n- Session identifiers\n- Full request bodies with PII\n\n### Redaction Patterns\n- Authorization: [REDACTED]\n- \"password\": \"***\"\n- First 4 / Last 4 only\n\n### Safe Logging\n- User IDs (if not PII)\n- Request IDs\n- Timestamps\n- Status codes\n- General error categories","start_line":1,"end_line":19}]}
{"source_id":"core","type":"security","title":"HTTPS Configuration Checklist","signals":["security_pattern"],"tags":["security","https","tls"],"summary":"Checklist for secure HTTPS configuration.","chunks":[{"chunk_id":"core:https-config:1","text":"## HTTPS Configuration Checklist\n\n### Minimum Requirements\n- [ ] TLS 1.2 minimum (prefer 1.3)\n- [ ] Strong cipher suites only\n- [ ] Valid certificate chain\n- [ ] HSTS header enabled\n\n### Headers\n- Strict-Transport-Security\n- X-Content-Type-Options: nosniff\n- X-Frame-Options: DENY\n- Content-Security-Policy\n\n### Certificate Management\n- Auto-renewal (Let's Encrypt)\n- Expiry monitoring\n- Key rotation schedule","start_line":1,"end_line":17}]}
{"source_id":"core","type":"template","title":"Contributing Guidelines Template","signals":["evidence_audit"],"tags":["template","contributing","documentation"],"summary":"Template for project contributing guidelines.","chunks":[{"chunk_id":"core:contributing-template:1","text":"## Contributing Guidelines Template\n\n### Getting Started\n- Fork and clone repository\n- Install dependencies\n- Run tests locally\n- Read code of conduct\n\n### Making Changes\n- Create feature branch\n- Make focused commits\n- Write/update tests\n- Update documentation\n\n### Submitting\n- Open pull request\n- Fill out PR template\n- Respond to feedback\n- Squash if requested","start_line":1,"end_line":18}]}
{"source_id":"core","type":"template","title":"Code of Conduct Template","signals":["evidence_audit"],"tags":["template","conduct","community"],"summary":"Template for project code of conduct.","chunks":[{"chunk_id":"core:coc-template:1","text":"## Code of Conduct Summary\n\n### Expected Behavior\n- Be respectful and inclusive\n- Focus on constructive feedback\n- Accept differing viewpoints\n- Show empathy to others\n\n### Unacceptable Behavior\n- Harassment or discrimination\n- Personal attacks\n- Trolling or insulting comments\n- Publishing others' private info\n\n### Enforcement\n- Reports to maintainers\n- Investigation within 1 week\n- Actions: warning to ban\n- All decisions final","start_line":1,"end_line":18}]}
{"source_id":"core","type":"iterate","title":"Memory Leak Detection","signals":["iterate_loop"],"tags":["debugging","memory","performance"],"summary":"Detecting and fixing memory leaks in applications.","chunks":[{"chunk_id":"core:memory-leaks:1","text":"## Memory Leak Detection\n\n### Symptoms\n- Memory usage grows over time\n- Out of memory errors\n- Performance degradation\n- Process restart \"fixes\" it\n\n### Detection Tools\n- valgrind (C/C++)\n- heaptrack\n- Browser DevTools (JS)\n- Memory profilers\n\n### Common Causes\n- Event listeners not removed\n- Caches without limits\n- Circular references\n- Global state accumulation","start_line":1,"end_line":17}]}
{"source_id":"core","type":"iterate","title":"Deadlock Debugging","signals":["iterate_loop"],"tags":["debugging","concurrency","deadlock"],"summary":"Identifying and resolving deadlock issues.","chunks":[{"chunk_id":"core:deadlock-debug:1","text":"## Deadlock Debugging\n\n### Symptoms\n- Application hangs\n- No CPU usage but no progress\n- Multiple threads waiting\n\n### Detection\n- Thread dump analysis\n- Lock ordering analysis\n- Timeout-based detection\n\n### Prevention\n- Consistent lock ordering\n- Lock timeouts\n- Avoid nested locks\n- Use lock-free structures where possible\n- Document lock order requirements","start_line":1,"end_line":16}]}
{"source_id":"core","type":"plan","title":"API Design Checklist","signals":["contract_lock"],"tags":["api","design","checklist"],"summary":"Checklist for designing consistent, usable APIs.","chunks":[{"chunk_id":"core:api-design:1","text":"## API Design Checklist\n\n### Naming\n- [ ] Consistent resource naming\n- [ ] Plural nouns for collections\n- [ ] Descriptive action names\n\n### Structure\n- [ ] RESTful conventions\n- [ ] Consistent response format\n- [ ] Pagination for lists\n- [ ] Filtering and sorting\n\n### Usability\n- [ ] Clear error messages\n- [ ] Examples in documentation\n- [ ] Rate limit headers\n- [ ] Versioning strategy","start_line":1,"end_line":17}]}
{"source_id":"core","type":"plan","title":"Database Schema Guidelines","signals":["state_machine"],"tags":["database","schema","design"],"summary":"Guidelines for designing database schemas.","chunks":[{"chunk_id":"core:db-schema:1","text":"## Database Schema Guidelines\n\n### General\n- Use UUIDs or auto-increment for PKs\n- Timestamps: created_at, updated_at\n- Soft delete with deleted_at\n- Foreign keys with constraints\n\n### Naming\n- snake_case for columns\n- Plural table names\n- Prefix indexes: idx_table_column\n\n### Performance\n- Index frequently queried columns\n- Avoid SELECT *\n- Normalize, then denormalize if needed\n- Partition large tables","start_line":1,"end_line":17}]}
{"source_id":"core","type":"evidence","title":"Metrics Collection Patterns","signals":["evidence_audit"],"tags":["metrics","monitoring","observability"],"summary":"Patterns for collecting useful metrics.","chunks":[{"chunk_id":"core:metrics-patterns:1","text":"## Metrics Collection Patterns\n\n### Key Metrics\n- Latency (p50, p95, p99)\n- Throughput (req/sec)\n- Error rate (%)\n- Saturation (queue depth)\n\n### Implementation\n- Counters for totals\n- Gauges for current values\n- Histograms for distributions\n\n### Best Practices\n- Meaningful metric names\n- Consistent labels/tags\n- Avoid high cardinality\n- Alert on thresholds","start_line":1,"end_line":17}]}
{"source_id":"core","type":"evidence","title":"Distributed Tracing Basics","signals":["evidence_audit"],"tags":["tracing","observability","debugging"],"summary":"Basic patterns for implementing distributed tracing.","chunks":[{"chunk_id":"core:dist-tracing:1","text":"## Distributed Tracing Basics\n\n### Concepts\n- Trace: full request journey\n- Span: single operation\n- Context: passed between services\n\n### Implementation\n- Generate trace ID at entry\n- Propagate via headers\n- Create spans for key operations\n- Log trace ID for correlation\n\n### What to Trace\n- HTTP requests\n- Database queries\n- External service calls\n- Async job processing","start_line":1,"end_line":17}]}
{"source_id":"core","type":"release","title":"Staged Rollout Patterns","signals":["release_install"],"tags":["release","rollout","deployment"],"summary":"Patterns for gradually rolling out releases.","chunks":[{"chunk_id":"core:staged-rollout:1","text":"## Staged Rollout Patterns\n\n### Stages\n1. Internal dogfooding (1%)\n2. Canary (5%)\n3. Early adopters (25%)\n4. General availability (100%)\n\n### Monitoring Between Stages\n- Error rates\n- Performance metrics\n- User feedback\n- Rollback if issues\n\n### Timing\n- Internal: 1-2 days\n- Canary: 1 day minimum\n- Full rollout: 1-2 weeks total","start_line":1,"end_line":17}]}
{"source_id":"core","type":"release","title":"Hotfix Process","signals":["release_install"],"tags":["release","hotfix","emergency"],"summary":"Process for emergency production fixes.","chunks":[{"chunk_id":"core:hotfix-process:1","text":"## Hotfix Process\n\n### When to Hotfix\n- Security vulnerability\n- Data corruption risk\n- Critical feature broken\n- Major user impact\n\n### Process\n1. Assess severity and impact\n2. Create hotfix branch from main\n3. Minimal fix only\n4. Fast-track review\n5. Deploy immediately\n6. Backport to develop\n\n### Post-Hotfix\n- Post-mortem within 48h\n- Prevent recurrence\n- Update monitoring","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Performance SLA Template","signals":["contract_lock"],"tags":["sla","performance","contract"],"summary":"Template for defining performance service level agreements.","chunks":[{"chunk_id":"core:perf-sla:1","text":"## Performance SLA Template\n\n### Metrics\n- Availability: 99.9% (8.7h downtime/year)\n- Response time: p95 < 200ms\n- Throughput: 1000 req/sec sustained\n\n### Measurement\n- External uptime monitoring\n- Response time from edge\n- 5-minute rolling windows\n\n### Exclusions\n- Planned maintenance windows\n- Force majeure events\n- Customer-caused issues\n\n### Remedies\n- Credits for SLA breaches\n- Root cause reports","start_line":1,"end_line":17}]}
{"source_id":"core","type":"contract","title":"Data Retention Policy Template","signals":["contract_lock"],"tags":["data","retention","policy"],"summary":"Template for defining data retention policies.","chunks":[{"chunk_id":"core:data-retention:1","text":"## Data Retention Policy\n\n### By Data Type\n- User data: Until account deletion + 30 days\n- Audit logs: 1 year\n- Analytics: 90 days aggregated\n- Backups: 30 day rolling\n\n### Deletion Process\n- Soft delete first (30 days)\n- Hard delete after grace period\n- Backup purging follows retention\n\n### Compliance\n- GDPR: right to erasure\n- Data subject requests: 30 days\n- Retention schedule documented","start_line":1,"end_line":16}]}
{"source_id":"core","type":"security","title":"API Key Best Practices","signals":["security_pattern"],"tags":["security","api-keys","authentication"],"summary":"Best practices for API key management.","chunks":[{"chunk_id":"core:api-keys:1","text":"## API Key Best Practices\n\n### Generation\n- Cryptographically random\n- Sufficient entropy (256 bits)\n- Prefix for type: sk_, pk_\n\n### Storage\n- Hash in database (for lookup)\n- Never log full key\n- Display only last 4 chars in UI\n\n### Management\n- Rotation capability\n- Expiration dates\n- Scope limitations\n- Revocation mechanism\n- Creation audit log","start_line":1,"end_line":17}]}
{"source_id":"core","type":"security","title":"Authentication Token Types","signals":["security_pattern"],"tags":["security","authentication","tokens"],"summary":"Overview of different authentication token types and use cases.","chunks":[{"chunk_id":"core:auth-tokens:1","text":"## Authentication Token Types\n\n### JWT (JSON Web Token)\n- Use: Stateless authentication\n- Pros: No server lookup needed\n- Cons: Can't revoke easily\n- Best for: APIs, microservices\n\n### Session Tokens\n- Use: Web application sessions\n- Stored: Database or cache\n- Can revoke: Yes, delete from store\n- Best for: Traditional web apps\n\n### API Keys\n- Use: Machine-to-machine auth\n- Long-lived, per-application\n- Best for: Service integrations","start_line":1,"end_line":17}]}
{"source_id":"core","type":"template","title":"Architecture Decision Record","signals":["evidence_audit"],"tags":["template","adr","documentation"],"summary":"Template for documenting architecture decisions.","chunks":[{"chunk_id":"core:adr-template:1","text":"## Architecture Decision Record\n\n### Title\n[Short descriptive title]\n\n### Status\n[Proposed, Accepted, Deprecated, Superseded]\n\n### Context\n[What is the issue we're addressing?]\n\n### Decision\n[What is our decision?]\n\n### Consequences\n[What are the results of this decision?]\n\n### Alternatives Considered\n[What else did we consider?]","start_line":1,"end_line":18}]}
{"source_id":"core","type":"template","title":"Runbook Template","signals":["evidence_audit"],"tags":["template","runbook","operations"],"summary":"Template for operational runbooks.","chunks":[{"chunk_id":"core:runbook-template:1","text":"## Runbook Template\n\n### Overview\n[What this runbook covers]\n\n### When to Use\n[Trigger conditions]\n\n### Prerequisites\n[Access, tools, permissions needed]\n\n### Steps\n1. [First action]\n2. [Second action]\n3. [Continue...]\n\n### Rollback\n[How to undo if something goes wrong]\n\n### Escalation\n[Who to contact if stuck]","start_line":1,"end_line":19}]}
{"source_id":"core","type":"iterate","title":"Root Cause Analysis Template","signals":["iterate_loop"],"tags":["debugging","rca","analysis"],"summary":"Template for conducting root cause analysis.","chunks":[{"chunk_id":"core:rca-template:1","text":"## Root Cause Analysis\n\n### 5 Whys Method\n1. Why did X happen? Because Y\n2. Why did Y happen? Because Z\n3. Continue until root cause\n\n### Categories\n- Process: Missing steps/checks\n- People: Training/knowledge gap\n- Technology: Bug/limitation\n- External: Third-party/environment\n\n### Output\n- Root cause statement\n- Contributing factors\n- Corrective actions\n- Prevention measures","start_line":1,"end_line":17}]}
{"source_id":"core","type":"iterate","title":"Debug Logging Strategy","signals":["iterate_loop"],"tags":["debugging","logging","strategy"],"summary":"Strategy for effective debug logging.","chunks":[{"chunk_id":"core:debug-logging:1","text":"## Debug Logging Strategy\n\n### Log Levels\n- ERROR: Something failed\n- WARN: Something unexpected\n- INFO: Normal operations\n- DEBUG: Detailed for debugging\n- TRACE: Very verbose\n\n### What to Log\n- Entry/exit of key functions\n- Decision points with values\n- External calls and responses\n- State transitions\n\n### Best Practices\n- Structured logging (JSON)\n- Include context IDs\n- Timestamps with timezone\n- Easy to grep/filter","start_line":1,"end_line":19}]}
{"source_id":"core","type":"plan","title":"Load Testing Strategy","signals":["iterate_loop"],"tags":["testing","performance","load"],"summary":"Strategy for conducting load tests.","chunks":[{"chunk_id":"core:load-testing:1","text":"## Load Testing Strategy\n\n### Test Types\n- Smoke: Basic functionality under load\n- Load: Normal expected load\n- Stress: Beyond normal capacity\n- Spike: Sudden traffic bursts\n- Soak: Extended duration\n\n### Metrics to Capture\n- Response times (p50, p95, p99)\n- Error rates\n- Throughput\n- Resource utilization\n\n### Process\n1. Baseline performance\n2. Increase load gradually\n3. Find breaking point\n4. Document limits","start_line":1,"end_line":18}]}
{"source_id":"core","type":"plan","title":"Capacity Planning Guide","signals":["state_machine"],"tags":["planning","capacity","infrastructure"],"summary":"Guide for infrastructure capacity planning.","chunks":[{"chunk_id":"core:capacity-planning:1","text":"## Capacity Planning Guide\n\n### Current State\n- Measure current usage\n- Peak vs average load\n- Growth trend analysis\n\n### Projections\n- 3-6 month forecast\n- Account for seasonal peaks\n- Include planned features\n\n### Thresholds\n- Alert at 70% capacity\n- Scale at 80% capacity\n- Emergency at 90% capacity\n\n### Review Cadence\n- Monthly metrics review\n- Quarterly planning update","start_line":1,"end_line":18}]}

{"source_id":"core","type":"contract","title":"Rust Web Service Contract","signals":["contract_lock"],"tags":["rust","web","axum","actix"],"summary":"Contract template for high-performance Rust web services including middleware, error handling, and async runtime settings.","chunks":[{"chunk_id":"core:rust-web-contract:1","text":"## Rust Web Service Contract\n\n### Service Overview\n- **Name**: [Service Name]\n- **Framework**: Axum / Actix-web\n- **Runtime**: Tokio (Multi-thread)\n\n### Core Components\n- **Router**: Centralized route definitions\n- **Handlers**: Async functions returning Result<impl IntoResponse, AppError>\n- **State**: Arc<AppState> for shared resources (DB pool, config)\n- **Middleware**: Tracing, CORS, Timeout, Compression\n\n### Error Handling\n- Custom AppError enum implementing IntoResponse\n- Centralized error logging\n- User-friendly error messages (no internal details)\n\n### Performance Targets\n- P99 Latency < 50ms\n- Throughput > 10k req/sec\n- Memory < 100MB idle","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Go Microservice Contract","signals":["contract_lock"],"tags":["go","microservice","grpc"],"summary":"Contract template for Go microservices using gRPC/Protobuf and Clean Architecture.","chunks":[{"chunk_id":"core:go-microservice-contract:1","text":"## Go Microservice Contract\n\n### Service Overview\n- **Name**: [Service Name]\n- **Protocol**: gRPC + HTTP/JSON Gateway\n- **Architecture**: Clean Architecture (Transport -> Endpoint -> Service -> Repository)\n\n### API Definition\n- **Protobuf**: Defined in /api/proto/v1\n- **Versioning**: Package versioning (v1, v2)\n- **Validation**: Protoc-gen-validate rules\n\n### Core Layers\n1. **Transport**: gRPC/HTTP handlers, decoding\n2. **Endpoint**: Request/Response structs, middleware\n3. **Service**: Business logic interface\n4. **Repository**: Data access interface\n\n### Observability\n- Structured logging (slog/zap)\n- Prometheus metrics (latency, errors, saturation)\n- Distributed tracing (OpenTelemetry)","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"TUI Application Contract","signals":["contract_lock"],"tags":["tui","rust","go","cli"],"summary":"Contract template for Terminal User Interface applications covering layout, input handling, and rendering loop.","chunks":[{"chunk_id":"core:tui-contract:1","text":"## TUI Application Contract\n\n### App Overview\n- **Name**: [App Name]\n- **Library**: Ratatui (Rust) / Bubbletea (Go)\n- **Theme**: Adaptive (Dark/Light support)\n\n### Architecture\n- **Model**: Application state\n- **View**: Rendering logic (Widgets, Layouts)\n- **Update**: Message handling loop\n\n### Key Components\n- **Input Handler**: Keyboard/Mouse events -> Actions\n- **Layout Engine**: Flexbox/Grid constraints\n- **Widgets**: List, Table, Chart, Paragraph\n\n### UX Requirements\n- **Responsive**: Adapts to terminal resize\n- **Shortcuts**: Vim-like navigation (j/k/h/l)\n- **Help**: Contextual help overlay (?)","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Modern React App Contract","signals":["contract_lock"],"tags":["react","web","frontend"],"summary":"Contract template for modern React applications using Next.js/Remix, Server Components, and Tailwind CSS.","chunks":[{"chunk_id":"core:react-contract:1","text":"## Modern React App Contract\n\n### App Overview\n- **Framework**: Next.js (App Router) / Remix\n- **Styling**: Tailwind CSS + Shadcn/UI\n- **State**: Server State (TanStack Query) + Client State (Zustand)\n\n### Architecture\n- **Server Components**: Data fetching, heavy rendering\n- **Client Components**: Interactivity, hooks\n- **Layouts**: Nested layouts for persistence\n\n### Data Strategy\n- **Fetching**: Server-side fetching (RSC)\n- **Mutations**: Server Actions / API Routes\n- **Caching**: Revalidation strategy (Time-based / On-demand)\n\n### Performance\n- **Core Web Vitals**: All green (LCP, CLS, INP)\n- **Bundle Size**: < 100KB initial load JS\n- **Images**: Next/Image optimization","start_line":1,"end_line":20}]}
{"source_id":"core","type":"plan","title":"Database Migration Plan","signals":["state_machine"],"tags":["database","migration","plan"],"summary":"Plan template for safe database migrations including backup, dry-run, and rollback procedures.","chunks":[{"chunk_id":"core:db-migration-plan:1","text":"## Database Migration Plan\n\n### Pre-Migration Checklist\n- [ ] Backup verified (restore tested)\n- [ ] Migration script reviewed by DBA\n- [ ] Dry-run on staging successful\n- [ ] Application downtime scheduled (if needed)\n\n### Execution Steps\n1. **Lock**: Prevent new writes (if offline migration)\n2. **Backup**: Take point-in-time snapshot\n3. **Migrate**: Run `sqlx migrate run` / `prisma migrate deploy`\n4. **Verify**: Check schema changes and row counts\n5. **Unlock**: Enable application writes\n\n### Rollback Strategy\n- **Down Script**: Tested SQL to revert changes\n- **Restore**: Restore from snapshot if data corruption occurs\n- **Verification**: Ensure app works with old schema","start_line":1,"end_line":18}]}
{"source_id":"core","type":"plan","title":"Distributed System Implementation Plan","signals":["state_machine"],"tags":["distributed","plan","system"],"summary":"Implementation plan for distributed systems covering consensus, replication, and partition tolerance.","chunks":[{"chunk_id":"core:dist-sys-plan:1","text":"## Distributed System Implementation Plan\n\n### Phase 1: Node Architecture\n- [ ] Node lifecycle (join, leave, crash)\n- [ ] RPC interface definition\n- [ ] Local storage engine\n- [ ] Configuration propagation\n\n### Phase 2: Consensus & Replication\n- [ ] Leader election (Raft/Paxos)\n- [ ] Log replication\n- [ ] State machine application\n- [ ] Snapshotting for log compaction\n\n### Phase 3: Resilience\n- [ ] Network partition handling\n- [ ] Byzantine fault tolerance (if needed)\n- [ ] Jepsen testing suite\n- [ ] Chaos engineering drills\n\n### Phase 4: Operations\n- [ ] Monitoring dashboard (Grafana)\n- [ ] Alerting rules\n- [ ] Rolling upgrade procedure","start_line":1,"end_line":20}]}
{"source_id":"core","type":"evidence","title":"Performance Audit Checklist","signals":["evidence_audit"],"tags":["performance","audit","checklist"],"summary":"Checklist for performance auditing covering CPU, memory, I/O, and network metrics.","chunks":[{"chunk_id":"core:perf-audit:1","text":"## Performance Audit Checklist\n\n### CPU Profiling\n- [ ] Hot paths identified (Flamegraph)\n- [ ] Lock contention analysis\n- [ ] Context switch rate checked\n- [ ] SIMD opportunities identified\n\n### Memory Analysis\n- [ ] Leak detection (Valgrind/Heaptrack)\n- [ ] Allocation rate (gc pressure)\n- [ ] Cache hit rates (L1/L2/L3)\n- [ ] Struct padding optimization\n\n### I/O & Network\n- [ ] Disk IOPS saturation check\n- [ ] Network throughput vs bandwidth\n- [ ] Serialization overhead measured\n- [ ] Database query performance (N+1 check)","start_line":1,"end_line":18}]}
{"source_id":"core","type":"evidence","title":"Security Audit Checklist","signals":["security_pattern"],"tags":["security","audit","checklist"],"summary":"Comprehensive security audit checklist covering auth, data, network, and dependencies.","chunks":[{"chunk_id":"core:sec-audit:1","text":"## Security Audit Checklist\n\n### Authentication & AuthZ\n- [ ] No hardcoded credentials\n- [ ] JWT signature verification\n- [ ] Password hashing (Argon2/Bcrypt)\n- [ ] RBAC enforcement on all endpoints\n\n### Data Security\n- [ ] SQL injection prevention (Prepared statements)\n- [ ] XSS prevention (Content Security Policy)\n- [ ] CSRF tokens implemented\n- [ ] Sensitive data redacted in logs\n\n### Infrastructure\n- [ ] TLS configuration (HSTS, Cipher suites)\n- [ ] Firewalls and Security Groups\n- [ ] Container security (Rootless, Minimal base)\n- [ ] Dependency vulnerability scan (Audit)","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"REST API Design Contract","signals":["contract_lock"],"tags":["api","rest","design"],"summary":"Contract template for REST API design covering endpoints, HTTP methods, status codes, and pagination.","chunks":[{"chunk_id":"core:rest-api-design:1","text":"## REST API Design Contract\n\n### Endpoint Naming\n- Use nouns, not verbs: `/users`, `/orders`\n- Use plural nouns: `/users` not `/user`\n- Use lowercase with hyphens: `/user-profiles`\n\n### HTTP Methods\n| Method | Use Case | Idempotent |\n|--------|----------|------------|\n| GET | Read | Yes |\n| POST | Create | No |\n| PUT | Full Update | Yes |\n| PATCH | Partial Update | No |\n| DELETE | Remove | Yes |\n\n### Status Codes\n- 200: Success\n- 201: Created\n- 204: No Content\n- 400: Bad Request\n- 401: Unauthorized\n- 403: Forbidden\n- 404: Not Found\n- 500: Internal Error","start_line":1,"end_line":24}]}
{"source_id":"core","type":"contract","title":"GraphQL API Contract","signals":["contract_lock"],"tags":["api","graphql","design"],"summary":"Contract template for GraphQL API design covering schema, queries, mutations, and subscriptions.","chunks":[{"chunk_id":"core:graphql-api:1","text":"## GraphQL API Contract\n\n### Schema Design\n- **Types**: Use PascalCase (User, Order)\n- **Fields**: Use camelCase (firstName, createdAt)\n- **Enums**: Use SCREAMING_SNAKE_CASE\n\n### Query Patterns\n- Single item: `user(id: ID!): User`\n- List: `users(first: Int, after: String): UserConnection`\n- Use connections for pagination\n\n### Mutations\n- Input types: `CreateUserInput`, `UpdateUserInput`\n- Return payload: `CreateUserPayload { user, errors }`\n\n### Best Practices\n- N+1 prevention with DataLoader\n- Depth limiting (max 5 levels)\n- Query complexity analysis\n- Rate limiting per operation","start_line":1,"end_line":20}]}
{"source_id":"core","type":"iterate","title":"Code Review Checklist","signals":["iterate_loop"],"tags":["review","checklist","quality"],"summary":"Checklist for conducting effective code reviews focusing on correctness, maintainability, and security.","chunks":[{"chunk_id":"core:code-review:1","text":"## Code Review Checklist\n\n### Correctness\n- [ ] Logic handles edge cases\n- [ ] Error handling is appropriate\n- [ ] No off-by-one errors\n- [ ] Concurrent access is safe\n\n### Maintainability\n- [ ] Code is self-documenting\n- [ ] Functions are small and focused\n- [ ] No magic numbers or strings\n- [ ] Consistent naming conventions\n\n### Performance\n- [ ] No N+1 queries\n- [ ] Appropriate data structures\n- [ ] No unnecessary allocations\n\n### Security\n- [ ] Input validation present\n- [ ] No SQL injection risk\n- [ ] Sensitive data protected","start_line":1,"end_line":22}]}
{"source_id":"core","type":"iterate","title":"PR Description Template","signals":["iterate_loop"],"tags":["pr","template","git"],"summary":"Template for writing effective pull request descriptions with context, changes, and testing.","chunks":[{"chunk_id":"core:pr-checklist-template:1","text":"## PR Description Template\n\n### Context\n<!-- Why is this change needed? -->\n\n### Changes\n- [ ] Added/Modified feature X\n- [ ] Updated tests\n- [ ] Updated documentation\n\n### Type\n- [ ] Feature\n- [ ] Bug Fix\n- [ ] Refactor\n- [ ] Documentation\n\n### Testing\n- [ ] Unit tests added/updated\n- [ ] Integration tests pass\n- [ ] Manual testing performed\n\n### Screenshots\n<!-- If UI changes, add screenshots -->","start_line":1,"end_line":22}]}
{"source_id":"core","type":"plan","title":"CI/CD Pipeline Plan","signals":["state_machine"],"tags":["cicd","devops","plan"],"summary":"Plan template for CI/CD pipeline covering build, test, security, and deployment stages.","chunks":[{"chunk_id":"core:cicd-plan:1","text":"## CI/CD Pipeline Plan\n\n### Stage 1: Build\n- [ ] Compile/transpile source\n- [ ] Run linters (eslint, clippy)\n- [ ] Check formatting\n- [ ] Generate artifacts\n\n### Stage 2: Test\n- [ ] Unit tests\n- [ ] Integration tests\n- [ ] Coverage report (>80%)\n\n### Stage 3: Security\n- [ ] Dependency scan (Dependabot/Snyk)\n- [ ] Secret detection\n- [ ] Container scan\n\n### Stage 4: Deploy\n- [ ] Staging deployment\n- [ ] Smoke tests\n- [ ] Production deployment\n- [ ] Health checks","start_line":1,"end_line":22}]}
{"source_id":"core","type":"plan","title":"Incident Response Plan","signals":["state_machine"],"tags":["incident","sre","plan"],"summary":"Plan template for incident response covering detection, mitigation, resolution, and post-mortem.","chunks":[{"chunk_id":"core:incident-plan:1","text":"## Incident Response Plan\n\n### Detection (0-5 min)\n- Alert received via PagerDuty/Slack\n- Acknowledge incident\n- Assess severity (P1-P4)\n\n### Mitigation (5-30 min)\n- Rollback if recent deploy\n- Scale resources if load issue\n- Enable feature flags fallback\n- Communicate to stakeholders\n\n### Resolution\n- Root cause identified\n- Fix implemented\n- Tests verified\n- Deploy fix\n\n### Post-Mortem (within 48h)\n- Timeline of events\n- Root cause analysis\n- Action items with owners\n- Lessons learned","start_line":1,"end_line":22}]}
{"source_id":"core","type":"evidence","title":"API Documentation Checklist","signals":["evidence_audit"],"tags":["api","documentation","checklist"],"summary":"Checklist for comprehensive API documentation covering endpoints, examples, and error codes.","chunks":[{"chunk_id":"core:api-docs:1","text":"## API Documentation Checklist\n\n### Essentials\n- [ ] Base URL and versioning\n- [ ] Authentication method\n- [ ] Rate limits specified\n\n### For Each Endpoint\n- [ ] HTTP method and path\n- [ ] Request parameters (query, path, body)\n- [ ] Request body schema with example\n- [ ] Response schema with example\n- [ ] Error codes and messages\n\n### Examples\n- [ ] cURL examples\n- [ ] Language-specific SDKs\n- [ ] Postman collection\n\n### Changelog\n- [ ] Breaking changes highlighted\n- [ ] Deprecation notices","start_line":1,"end_line":20}]}
{"source_id":"core","type":"evidence","title":"Logging Standards","signals":["evidence_audit"],"tags":["logging","observability","standards"],"summary":"Standards for structured logging covering log levels, fields, and best practices.","chunks":[{"chunk_id":"core:logging-standards:1","text":"## Logging Standards\n\n### Log Levels\n- **ERROR**: System failures requiring action\n- **WARN**: Potential issues to monitor\n- **INFO**: Key business events\n- **DEBUG**: Detailed diagnostic info\n\n### Required Fields\n- timestamp: ISO 8601\n- level: log level\n- service: service name\n- trace_id: distributed tracing\n- user_id: (if authenticated)\n\n### Best Practices\n- Never log sensitive data (PII, secrets)\n- Use structured JSON format\n- Include context (request_id, user_id)\n- Rate limit high-volume logs","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Microservice Boundaries Contract","signals":["contract_lock"],"tags":["microservices","architecture","boundaries"],"summary":"Contract for defining microservice boundaries covering ownership, APIs, and data isolation.","chunks":[{"chunk_id":"core:microservice-boundaries:1","text":"## Microservice Boundaries Contract\n\n### Service Identity\n- **Name**: [service-name]\n- **Owner**: [team]\n- **Domain**: [bounded context]\n\n### API Contract\n- Sync: REST/gRPC endpoints\n- Async: Event topics produced/consumed\n- Schema registry for versioning\n\n### Data Ownership\n- Own database instance\n- No direct DB access from other services\n- Data shared via API/events\n\n### Dependencies\n- Upstream services called\n- Downstream consumers\n- Circuit breaker policies","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Feature Flag Contract","signals":["contract_lock"],"tags":["feature-flag","release","contract"],"summary":"Contract for feature flag implementation covering naming, rollout, and cleanup.","chunks":[{"chunk_id":"core:feature-flag:1","text":"## Feature Flag Contract\n\n### Naming Convention\n- Format: `ff_<feature>_<variant>`\n- Example: `ff_new_checkout_enabled`\n\n### Flag Types\n- **Release**: Gradual rollout (0-100%)\n- **Experiment**: A/B testing\n- **Ops**: Kill switch for features\n- **Permission**: User segment targeting\n\n### Lifecycle\n1. Create flag (disabled by default)\n2. Gradual rollout (10% -> 50% -> 100%)\n3. Monitor metrics\n4. Remove flag code (within 30 days of 100%)\n\n### Cleanup\n- Stale flags: >90 days at 100%\n- Quarterly flag audit\n- Remove dead code paths","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Docker Container Contract","signals":["contract_lock","release_install"],"tags":["docker","container","devops"],"summary":"Contract for Docker container best practices covering multi-stage builds, security, and health checks.","chunks":[{"chunk_id":"core:docker-contract:1","text":"## Docker Container Contract\n\n### Image Requirements\n- **Base Image**: Use official slim/alpine variants\n- **Multi-stage**: Build in one stage, run in minimal\n- **Non-root**: Run as non-root user (USER 1000)\n\n### Dockerfile Best Practices\n- Order layers by change frequency\n- Combine RUN commands to reduce layers\n- Use .dockerignore for build context\n- Pin dependency versions\n\n### Health & Signals\n- HEALTHCHECK instruction defined\n- Handle SIGTERM gracefully\n- Startup time < 30 seconds\n\n### Security\n- No secrets in image layers\n- Read-only root filesystem\n- Drop all capabilities, add needed","start_line":1,"end_line":20}]}
{"source_id":"core","type":"contract","title":"Kubernetes Deployment Contract","signals":["contract_lock","release_install"],"tags":["kubernetes","k8s","deployment"],"summary":"Contract for Kubernetes deployments covering resource limits, probes, and rolling updates.","chunks":[{"chunk_id":"core:k8s-contract:1","text":"## Kubernetes Deployment Contract\n\n### Resource Management\n- **Requests**: Minimum guaranteed resources\n- **Limits**: Maximum allowed resources\n- **QoS**: Target Guaranteed or Burstable class\n\n### Probes\n- **Liveness**: Restart if unhealthy (path: /healthz)\n- **Readiness**: Remove from LB if not ready (path: /ready)\n- **Startup**: Allow slow startup (initialDelaySeconds)\n\n### Rolling Update\n- maxSurge: 25%\n- maxUnavailable: 0 (for zero-downtime)\n- minReadySeconds: 10\n\n### Pod Disruption Budget\n- minAvailable: 2 (or percentage)\n- Allow graceful node drains","start_line":1,"end_line":18}]}
{"source_id":"core","type":"plan","title":"Unit Testing Strategy","signals":["iterate_loop"],"tags":["testing","unit","strategy"],"summary":"Comprehensive unit testing strategy covering test structure, mocking, and coverage.","chunks":[{"chunk_id":"core:unit-test-strategy:1","text":"## Unit Testing Strategy\n\n### Test Structure (AAA)\n- **Arrange**: Set up test data and mocks\n- **Act**: Execute the function under test\n- **Assert**: Verify the result\n\n### Naming Convention\n- test_<function>_<scenario>_<expected>\n- Example: test_calculate_total_with_discount_returns_reduced_price\n\n### Mocking Guidelines\n- Mock external dependencies only\n- Use dependency injection for testability\n- Avoid testing implementation details\n\n### Coverage Targets\n- 80% line coverage minimum\n- 100% for critical paths\n- Focus on branch coverage for conditionals","start_line":1,"end_line":18}]}
{"source_id":"core","type":"plan","title":"Integration Testing Strategy","signals":["iterate_loop"],"tags":["testing","integration","strategy"],"summary":"Integration testing patterns covering database setup, API testing, and test isolation.","chunks":[{"chunk_id":"core:integration-test-strategy:1","text":"## Integration Testing Strategy\n\n### Database Tests\n- Use transactions with rollback\n- Or: Fresh database per test suite\n- Seed with minimal required data\n\n### API Tests\n- Test through HTTP layer\n- Validate status codes and bodies\n- Test auth flows end-to-end\n\n### Isolation\n- Each test independent\n- No shared mutable state\n- Clean up after tests\n\n### Performance\n- Integration tests can be slower\n- Parallelize where safe\n- Use test containers for dependencies","start_line":1,"end_line":18}]}
{"source_id":"core","type":"iterate","title":"Git Commit Message Convention","signals":["evidence_audit"],"tags":["git","commit","convention"],"summary":"Conventional commit message format for clear and automated changelog generation.","chunks":[{"chunk_id":"core:git-commit-convention:1","text":"## Git Commit Message Convention\n\n### Format\n<type>(<scope>): <subject>\n\n### Types\n- **feat**: New feature\n- **fix**: Bug fix\n- **docs**: Documentation only\n- **style**: Formatting, no code change\n- **refactor**: Code change, no feature/fix\n- **test**: Adding tests\n- **chore**: Maintenance tasks\n\n### Rules\n- Subject: imperative mood, no period\n- Body: explain what and why\n- Footer: breaking changes, issue refs\n\n### Examples\n- feat(auth): add OAuth2 login\n- fix(api): handle null response","start_line":1,"end_line":20}]}
{"source_id":"core","type":"iterate","title":"Git Branch Strategy","signals":["state_machine"],"tags":["git","branching","workflow"],"summary":"Git branching strategies for team development including GitFlow and trunk-based.","chunks":[{"chunk_id":"core:git-branch-strategy:1","text":"## Git Branch Strategy\n\n### Trunk-Based (Recommended)\n- main: always deployable\n- Short-lived feature branches\n- Feature flags for incomplete work\n- Merge via PR with CI gates\n\n### GitFlow (Legacy)\n- main: production releases\n- develop: integration branch\n- feature/*: new features\n- release/*: release prep\n- hotfix/*: production fixes\n\n### Branch Naming\n- feature/TICKET-123-description\n- fix/TICKET-456-bug-name\n- chore/update-dependencies","start_line":1,"end_line":18}]}
{"source_id":"core","type":"security","title":"Container Security Checklist","signals":["security_pattern"],"tags":["container","docker","security"],"summary":"Security checklist for container images and runtime.","chunks":[{"chunk_id":"core:container-security:1","text":"## Container Security Checklist\n\n### Build Time\n- [ ] Use minimal base images\n- [ ] No secrets in build args\n- [ ] Pin base image digests\n- [ ] Scan for vulnerabilities (Trivy)\n\n### Runtime\n- [ ] Run as non-root user\n- [ ] Read-only root filesystem\n- [ ] Drop all capabilities\n- [ ] No privileged containers\n\n### Network\n- [ ] Network policies defined\n- [ ] Ingress/egress restricted\n- [ ] Service mesh encryption","start_line":1,"end_line":18}]}
{"source_id":"core","type":"evidence","title":"Alerting Best Practices","signals":["evidence_audit"],"tags":["alerting","monitoring","sre"],"summary":"Best practices for actionable alerts that reduce noise and improve response.","chunks":[{"chunk_id":"core:alerting-best-practices:1","text":"## Alerting Best Practices\n\n### Alert Quality\n- Every alert must be actionable\n- Include runbook link in alert\n- Clear severity levels (P1-P4)\n\n### Reduce Noise\n- Aggregate related alerts\n- Use appropriate thresholds\n- Avoid flapping (min duration)\n\n### On-Call\n- Clear escalation paths\n- Rotation schedules\n- Post-incident review\n\n### Metrics\n- Track alert frequency\n- Measure time-to-resolution\n- Review and tune regularly","start_line":1,"end_line":18}]}
{"source_id":"core","type":"plan","title":"Code Refactoring Playbook","signals":["iterate_loop"],"tags":["refactoring","code-quality","playbook"],"summary":"Systematic approach to code refactoring with safety and measurable improvements.","chunks":[{"chunk_id":"core:refactoring-playbook:1","text":"## Code Refactoring Playbook\n\n### Before Refactoring\n- [ ] Tests cover affected code\n- [ ] Benchmark if performance-related\n- [ ] Small, incremental changes\n\n### Safe Refactoring Steps\n1. Extract: Pull out functions/classes\n2. Rename: Improve naming clarity\n3. Move: Better file organization\n4. Simplify: Reduce complexity\n\n### Verification\n- All tests still pass\n- No behavior changes\n- Performance maintained\n\n### Anti-patterns\n- Refactoring and adding features together\n- Big-bang rewrites\n- Skipping tests","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"API Rate Limiting Contract","signals":["contract_lock","security_pattern"],"tags":["api","rate-limit","contract"],"summary":"Contract for API rate limiting covering quotas, headers, and client handling.","chunks":[{"chunk_id":"core:rate-limit-contract:1","text":"## API Rate Limiting Contract\n\n### Limits\n- **Anonymous**: 60 requests/hour\n- **Authenticated**: 1000 requests/hour\n- **Premium**: 10000 requests/hour\n\n### Response Headers\n- X-RateLimit-Limit: Total allowed\n- X-RateLimit-Remaining: Left in window\n- X-RateLimit-Reset: Unix timestamp\n\n### On Limit Exceeded\n- HTTP 429 Too Many Requests\n- Retry-After header\n- Clear error message\n\n### Client Behavior\n- Respect Retry-After\n- Exponential backoff\n- Cache responses where possible","start_line":1,"end_line":18}]}
{"source_id":"core","type":"template","title":"Dockerfile Multi-Stage Template","signals":["release_install"],"tags":["docker","dockerfile","template"],"summary":"Multi-stage Dockerfile template for minimal production images.","chunks":[{"chunk_id":"core:dockerfile-template:1","text":"## Dockerfile Multi-Stage Template\n\n### Rust Example\n```dockerfile\n# Build stage\nFROM rust:1.75-slim AS builder\nWORKDIR /app\nCOPY . .\nRUN cargo build --release\n\n# Runtime stage\nFROM debian:bookworm-slim\nRUN useradd -r -u 1000 app\nCOPY --from=builder /app/target/release/myapp /usr/local/bin/\nUSER app\nCMD [\"myapp\"]\n```\n\n### Benefits\n- Small final image (<100MB)\n- No build tools in production\n- Non-root user for security","start_line":1,"end_line":20}]}
{"source_id":"core","type":"template","title":"GitHub Actions Workflow Template","signals":["release_install","iterate_loop"],"tags":["github-actions","ci","template"],"summary":"Reusable GitHub Actions workflow template for Rust/Node/Go projects.","chunks":[{"chunk_id":"core:gh-actions-template:1","text":"## GitHub Actions Workflow Template\n\n### Structure\n```yaml\nname: CI\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Setup\n        uses: actions/setup-node@v4\n      - run: npm ci\n      - run: npm test\n```\n\n### Best Practices\n- Cache dependencies\n- Fail fast on errors\n- Matrix for multi-platform","start_line":1,"end_line":22}]}
{"source_id":"core","type":"iterate","title":"Code Review Response Guide","signals":["iterate_loop","evidence_audit"],"tags":["code-review","collaboration","guide"],"summary":"Guide for responding to code review feedback constructively and efficiently.","chunks":[{"chunk_id":"core:review-response-guide:1","text":"## Code Review Response Guide\n\n### Receiving Feedback\n- Read all comments before responding\n- Assume positive intent\n- Ask for clarification if unclear\n- Thank reviewers for their time\n\n### Responding\n- Address each comment explicitly\n- Explain reasoning for disagreements\n- Mark resolved when fixed\n- Request re-review when ready\n\n### Common Responses\n- \"Done\" - Fixed as suggested\n- \"Good catch!\" - Acknowledge mistakes\n- \"Let's discuss\" - Need conversation\n- \"Will address in follow-up\" - Defer to new PR","start_line":1,"end_line":18}]}
{"source_id":"core","type":"contract","title":"Monorepo Structure Contract","signals":["contract_lock"],"tags":["monorepo","architecture","structure"],"summary":"Contract for monorepo organization covering package structure, dependencies, and tooling.","chunks":[{"chunk_id":"core:monorepo-contract:1","text":"## Monorepo Structure Contract\n\n### Directory Layout\n```\n/\n├── apps/           # Deployable applications\n├── packages/       # Shared libraries\n├── tools/          # Build/dev tooling\n└── docs/           # Documentation\n```\n\n### Dependency Rules\n- packages/* can depend on other packages/*\n- apps/* can depend on packages/*\n- No circular dependencies\n- Version sync across packages\n\n### Tooling\n- Turborepo / Nx for build orchestration\n- Changesets for versioning\n- Shared ESLint/TSConfig","start_line":1,"end_line":18}]}
{"source_id":"core","type":"context_engineering","title":"Model Context Protocol (MCP) Overview","signals":["context_engineering","provider_adapter"],"tags":["mcp","ai","protocol","integration"],"summary":"Overview of Model Context Protocol (MCP) - the standardized protocol for connecting AI models to external tools and data sources.","chunks":[{"chunk_id":"core:mcp-overview:1","text":"## Model Context Protocol (MCP) Overview\n\n### What is MCP?\nAn open standard for connecting AI assistants to external data sources and tools, enabling:\n- Standardized tool discovery and invocation\n- Secure resource access\n- Dynamic context injection\n\n### Core Concepts\n- **Host**: AI application (Claude Desktop, IDE)\n- **Client**: MCP client in the host\n- **Server**: External service providing tools/resources\n- **Transport**: Communication layer (stdio, SSE, WebSocket)\n\n### Key Benefits\n- Universal tool interface\n- Secure sandboxed execution\n- Dynamic capability discovery\n- Consistent error handling\n\n### Use Cases\n- File system access\n- Database queries\n- API integrations\n- Code execution environments","start_line":1,"end_line":26}]}
{"source_id":"core","type":"contract","title":"MCP Server Contract","signals":["contract_lock","provider_adapter"],"tags":["mcp","server","contract","ai"],"summary":"Contract template for building MCP servers that expose tools and resources to AI assistants.","chunks":[{"chunk_id":"core:mcp-server-contract:1","text":"## MCP Server Contract\n\n### Server Overview\n- **Name**: [server-name]\n- **Transport**: stdio / SSE / WebSocket\n- **Purpose**: [What resources/tools it provides]\n\n### Tools Exposed\n| Name | Description | Parameters | Returns |\n|------|-------------|------------|--------|\n| read_file | Read file contents | path: string | content: string |\n| query_db | Execute SQL query | sql: string | rows: array |\n\n### Resources Provided\n- URI scheme: `resource://server-name/path`\n- MIME types supported\n- Caching policy\n\n### Security\n- Input validation on all parameters\n- Sandboxed execution environment\n- No arbitrary code execution\n- Rate limiting per client\n\n### Error Handling\n- Structured error codes\n- User-friendly messages\n- Recovery suggestions","start_line":1,"end_line":28}]}
{"source_id":"core","type":"plan","title":"MCP Server Implementation Plan","signals":["state_machine"],"tags":["mcp","implementation","plan"],"summary":"Implementation plan for building MCP servers with tool definitions, resource handling, and transport setup.","chunks":[{"chunk_id":"core:mcp-server-plan:1","text":"## MCP Server Implementation Plan\n\n### Phase 1: Foundation\n- [ ] Choose transport (stdio for CLI, SSE for web)\n- [ ] Set up MCP SDK (@modelcontextprotocol/sdk)\n- [ ] Define server metadata (name, version, capabilities)\n- [ ] Implement basic request/response loop\n\n### Phase 2: Tools\n- [ ] Define tool schemas (JSON Schema)\n- [ ] Implement tool handlers\n- [ ] Add input validation (Zod/JSON Schema)\n- [ ] Return structured results\n\n### Phase 3: Resources\n- [ ] Define resource URI schemes\n- [ ] Implement resource listing\n- [ ] Handle resource reads\n- [ ] Add content type detection\n\n### Phase 4: Production\n- [ ] Add logging and tracing\n- [ ] Implement rate limiting\n- [ ] Handle graceful shutdown\n- [ ] Write integration tests","start_line":1,"end_line":24}]}
{"source_id":"core","type":"context_engineering","title":"RAG (Retrieval-Augmented Generation) Patterns","signals":["context_engineering"],"tags":["rag","ai","retrieval","vector"],"summary":"Patterns for implementing effective Retrieval-Augmented Generation systems.","chunks":[{"chunk_id":"core:rag-patterns:1","text":"## RAG Patterns\n\n### Architecture\n```\nQuery → Embed → Search → Rerank → Augment → Generate\n```\n\n### Embedding Strategy\n- Chunk size: 256-512 tokens (experiment)\n- Overlap: 10-20% for context continuity\n- Model: text-embedding-3-small/large\n\n### Retrieval\n- Vector similarity (cosine/dot product)\n- Hybrid search (vector + keyword BM25)\n- MMR for diversity\n\n### Reranking\n- Cross-encoder for precision\n- Filter by metadata\n- Deduplicate similar chunks\n\n### Context Injection\n- Prepend to system prompt\n- Or insert in user message\n- Include source attribution\n\n### Quality Metrics\n- Recall@k for retrieval\n- Answer relevance score\n- Faithfulness to sources","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"RAG Pipeline Contract","signals":["contract_lock","context_engineering"],"tags":["rag","pipeline","contract"],"summary":"Contract template for RAG pipeline systems covering ingestion, indexing, retrieval, and generation.","chunks":[{"chunk_id":"core:rag-contract:1","text":"## RAG Pipeline Contract\n\n### Pipeline Overview\n- **Data Sources**: Documents, APIs, databases\n- **Vector Store**: Pinecone / Weaviate / Qdrant / Chroma\n- **Embedding Model**: OpenAI / Cohere / local\n- **LLM**: GPT-4 / Claude / Llama\n\n### Ingestion\n- File formats supported (PDF, MD, HTML)\n- Chunking strategy (recursive, semantic)\n- Metadata extraction (title, date, source)\n\n### Retrieval\n- Top-k results (default: 5)\n- Similarity threshold (> 0.7)\n- Hybrid search weighting\n\n### Generation\n- System prompt with instructions\n- Context window management\n- Citation format\n\n### Quality Gates\n- Chunk relevance > 0.7\n- Answer groundedness check\n- Hallucination detection","start_line":1,"end_line":26}]}
{"source_id":"core","type":"context_engineering","title":"Agentic Workflow State Machine","signals":["state_machine","agent_pattern"],"tags":["agent","workflow","state-machine"],"summary":"State machine patterns for building reliable agentic AI workflows with planning, execution, and reflection loops.","chunks":[{"chunk_id":"core:agentic-workflow:1","text":"## Agentic Workflow State Machine\n\n### States\n```\nIDLE → PLANNING → EXECUTING → REFLECTING → COMPLETE\n         ↑           ↓\n         ←── RETRY ←──\n```\n\n### PLANNING Phase\n- Decompose goal into subtasks\n- Select tools for each subtask\n- Estimate token/cost budget\n- Validate plan feasibility\n\n### EXECUTING Phase\n- Execute one action at a time\n- Capture tool outputs\n- Update working memory\n- Check safety guardrails\n\n### REFLECTING Phase\n- Evaluate progress toward goal\n- Detect errors or stalls\n- Decide: continue, retry, or abort\n- Update plan if needed\n\n### Transitions\n- Max iterations: 10 (configurable)\n- Timeout per action: 60s\n- Escalate to human if stuck","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"AI Agent Memory Contract","signals":["contract_lock","agent_pattern"],"tags":["agent","memory","contract"],"summary":"Contract for AI agent memory systems covering short-term, long-term, and episodic memory.","chunks":[{"chunk_id":"core:agent-memory-contract:1","text":"## AI Agent Memory Contract\n\n### Memory Types\n- **Working Memory**: Current task context (in prompt)\n- **Short-term**: Conversation history (sliding window)\n- **Long-term**: Persistent facts (vector DB)\n- **Episodic**: Past task summaries\n\n### Short-term Memory\n- Token limit: Last N messages\n- Summarize older messages\n- Keep user preferences\n\n### Long-term Memory\n- Store: user facts, learned preferences\n- Retrieve: semantic search on query\n- Update: periodically consolidate\n\n### Memory Operations\n- Save(key, value, metadata)\n- Retrieve(query, top_k)\n- Forget(key) - explicit deletion\n- Summarize(conversation) - compression","start_line":1,"end_line":22}]}
{"source_id":"core","type":"plan","title":"Multi-Agent System Plan","signals":["state_machine","agent_pattern"],"tags":["multi-agent","orchestration","plan"],"summary":"Implementation plan for multi-agent systems with role specialization, communication, and coordination.","chunks":[{"chunk_id":"core:multi-agent-plan:1","text":"## Multi-Agent System Plan\n\n### Phase 1: Agent Definitions\n- [ ] Define agent roles (Planner, Executor, Critic)\n- [ ] Set agent capabilities and constraints\n- [ ] Design system prompts per role\n- [ ] Implement agent interfaces\n\n### Phase 2: Communication\n- [ ] Message passing protocol\n- [ ] Shared blackboard/workspace\n- [ ] Turn-taking or parallel execution\n- [ ] Conflict resolution rules\n\n### Phase 3: Orchestration\n- [ ] Supervisor agent or fixed workflow\n- [ ] Task delegation logic\n- [ ] Result aggregation\n- [ ] Consensus mechanisms\n\n### Phase 4: Safety\n- [ ] Agent isolation (no direct tool access)\n- [ ] Supervisor approval for actions\n- [ ] Budget limits per agent\n- [ ] Logging all inter-agent messages","start_line":1,"end_line":24}]}
{"source_id":"core","type":"evidence","title":"LLM Evaluation Framework","signals":["evidence_audit","iterate_loop"],"tags":["llm","evaluation","testing"],"summary":"Framework for evaluating LLM outputs covering accuracy, safety, and performance metrics.","chunks":[{"chunk_id":"core:llm-eval-framework:1","text":"## LLM Evaluation Framework\n\n### Evaluation Types\n- **Automated**: Programmatic checks\n- **LLM-as-Judge**: Use another LLM to evaluate\n- **Human Eval**: Manual review for quality\n\n### Automated Metrics\n- Exact match / BLEU / ROUGE (NLP)\n- JSON validity (structured output)\n- Regex patterns (format compliance)\n- Latency and token usage\n\n### LLM-as-Judge Criteria\n- Relevance: Does it answer the question?\n- Correctness: Is the information accurate?\n- Helpfulness: Is it useful to the user?\n- Safety: No harmful content?\n\n### Golden Dataset\n- Curated Q&A pairs with expected answers\n- Edge cases and adversarial inputs\n- Diverse topics and difficulty levels\n- Version controlled with code","start_line":1,"end_line":24}]}
{"source_id":"core","type":"plan","title":"LLM Testing Strategy","signals":["iterate_loop"],"tags":["llm","testing","strategy"],"summary":"Strategy for testing LLM-powered applications covering prompt regression, A/B testing, and monitoring.","chunks":[{"chunk_id":"core:llm-testing-strategy:1","text":"## LLM Testing Strategy\n\n### Prompt Regression Testing\n- Golden test cases for each prompt\n- Assert output structure/format\n- Track quality scores over time\n- Alert on regression (score drop)\n\n### A/B Testing\n- Compare prompt variants\n- Measure: quality, latency, cost\n- Statistical significance required\n- Document winning variant\n\n### Continuous Monitoring\n- Sample production requests\n- Human review random samples\n- Track error categories\n- Alert on new failure modes\n\n### Tools\n- Promptfoo for evaluation\n- LangSmith for tracing\n- Braintrust for experiments","start_line":1,"end_line":22}]}
{"source_id":"core","type":"template","title":"Cursor Rules Template","signals":["context_engineering"],"tags":["cursor","rules","ai-coding","template"],"summary":"Template for .cursorrules files to configure AI coding assistant behavior per project.","chunks":[{"chunk_id":"core:cursor-rules:1","text":"## Cursor Rules Template\n\n### File: .cursorrules\n```markdown\n# Project Context\nThis is a [language] project using [framework].\n\n# Code Style\n- Use [tabs/spaces] for indentation\n- Prefer [functional/OOP] patterns\n- Error handling: [Result types/exceptions]\n\n# Conventions\n- File naming: [snake_case/kebab-case]\n- Test files: [*.test.ts/*_test.go]\n- Import order: [stdlib, external, internal]\n\n# Dos\n- Write comprehensive error messages\n- Add JSDoc/docstrings for public APIs\n- Use meaningful variable names\n\n# Don'ts\n- Don't use any/unknown types\n- Don't commit console.log statements\n- Don't ignore errors silently\n```","start_line":1,"end_line":26}]}
{"source_id":"core","type":"template","title":"GitHub Copilot Instructions Template","signals":["context_engineering"],"tags":["copilot","instructions","ai-coding","template"],"summary":"Template for .github/copilot-instructions.md to customize GitHub Copilot behavior.","chunks":[{"chunk_id":"core:copilot-instructions:1","text":"## GitHub Copilot Instructions Template\n\n### File: .github/copilot-instructions.md\n```markdown\n# Project Overview\nBrief description of what this project does.\n\n# Technology Stack\n- Language: TypeScript 5.x\n- Runtime: Node.js 20 LTS\n- Framework: Express / Next.js\n- Database: PostgreSQL\n\n# Architecture\n- src/api - API routes\n- src/services - Business logic\n- src/models - Data models\n- src/utils - Shared utilities\n\n# Coding Standards\n- Use async/await over callbacks\n- Prefer composition over inheritance\n- All functions must have return types\n\n# Testing\n- Unit tests with Vitest\n- Integration tests in tests/integration\n- Aim for 80% coverage\n```","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"AI Safety Guardrails Contract","signals":["contract_lock","security_pattern"],"tags":["ai","safety","guardrails","contract"],"summary":"Contract for implementing AI safety guardrails covering input filtering, output validation, and escalation.","chunks":[{"chunk_id":"core:ai-safety-contract:1","text":"## AI Safety Guardrails Contract\n\n### Input Guardrails\n- **PII Detection**: Mask SSN, credit cards, emails\n- **Injection Detection**: Prompt injection patterns\n- **Content Filtering**: Block harmful requests\n- **Length Limits**: Max input tokens\n\n### Output Guardrails\n- **Hallucination Check**: Verify against sources\n- **Tone Enforcement**: Professional, helpful\n- **Format Validation**: Expected structure\n- **Sensitive Info Leak**: No internal details\n\n### Runtime Controls\n- **Rate Limiting**: Per user/session\n- **Cost Caps**: Max spend per request\n- **Timeout**: Max processing time\n- **Circuit Breaker**: Disable on high error rate\n\n### Escalation\n- Uncertain → Return \"I don't know\"\n- Policy violation → Block and log\n- System error → Graceful fallback","start_line":1,"end_line":26}]}
{"source_id":"core","type":"iterate","title":"Prompt Engineering Playbook","signals":["iterate_loop","context_engineering"],"tags":["prompt","engineering","llm","playbook"],"summary":"Playbook for effective prompt engineering with techniques for clarity, structure, and reliability.","chunks":[{"chunk_id":"core:prompt-engineering:1","text":"## Prompt Engineering Playbook\n\n### Structure\n1. **Role**: Define the AI's persona\n2. **Context**: Provide relevant background\n3. **Task**: Clear instruction\n4. **Format**: Expected output structure\n5. **Examples**: Few-shot demonstrations\n\n### Techniques\n- **Chain of Thought**: \"Think step by step\"\n- **Self-Consistency**: Multiple samples, vote\n- **ReAct**: Reason + Act interleaved\n- **Structured Output**: JSON mode, function calling\n\n### Reliability Tips\n- Be specific, not vague\n- Include edge case handling\n- Test with adversarial inputs\n- Version control your prompts\n\n### Anti-patterns\n- Overly long system prompts (> 2000 tokens)\n- Conflicting instructions\n- Assuming unstated context","start_line":1,"end_line":24}]}
{"source_id":"core","type":"plan","title":"Vibe Coding Workflow","signals":["state_machine","context_engineering"],"tags":["vibe-coding","ai-assisted","workflow"],"summary":"Workflow for AI-assisted 'vibe coding' - rapid prototyping with AI pair programming.","chunks":[{"chunk_id":"core:vibe-coding-workflow:1","text":"## Vibe Coding Workflow\n\n### Phase 1: Spec\n- Write clear requirements in natural language\n- Define acceptance criteria\n- List constraints and non-goals\n- Create contract before coding\n\n### Phase 2: Generate\n- Use AI to scaffold initial code\n- Provide context (existing code, patterns)\n- Review generated code carefully\n- Iterate on feedback\n\n### Phase 3: Refine\n- Run tests, fix failures\n- Apply linting and formatting\n- Refactor for clarity\n- Add documentation\n\n### Phase 4: Verify\n- Full test suite passes\n- Security scan clean\n- Performance acceptable\n- Code review by human\n\n### Best Practices\n- Never ship AI code without review\n- Understand every line you commit\n- AI augments, doesn't replace judgment","start_line":1,"end_line":28}]}
{"source_id":"core","type":"evidence","title":"AI Code Review Checklist","signals":["evidence_audit","iterate_loop"],"tags":["ai","code-review","checklist"],"summary":"Checklist for reviewing AI-generated code before committing.","chunks":[{"chunk_id":"core:ai-code-review:1","text":"## AI Code Review Checklist\n\n### Understanding\n- [ ] I understand what this code does\n- [ ] I can explain it to a teammate\n- [ ] Logic matches my intent\n\n### Correctness\n- [ ] Edge cases handled\n- [ ] No obvious bugs\n- [ ] Error handling appropriate\n- [ ] Types are correct\n\n### Security\n- [ ] No hardcoded secrets\n- [ ] Input validation present\n- [ ] No injection vulnerabilities\n\n### Quality\n- [ ] Code is idiomatic for the language\n- [ ] No unnecessary complexity\n- [ ] Tests cover new code\n- [ ] Documentation updated","start_line":1,"end_line":22}]}
{"source_id":"core","type":"contract","title":"Tool Use Contract (Function Calling)","signals":["contract_lock","agent_pattern"],"tags":["tools","function-calling","ai"],"summary":"Contract for LLM tool use and function calling covering schema, validation, and error handling.","chunks":[{"chunk_id":"core:tool-use-contract:1","text":"## Tool Use Contract\n\n### Tool Definition\n- **Name**: Verb-noun format (get_weather, search_docs)\n- **Description**: When and why to use\n- **Parameters**: JSON Schema with descriptions\n- **Returns**: Expected output format\n\n### Parameter Design\n- Required vs optional clearly marked\n- Enums for constrained values\n- Examples in descriptions\n- Default values documented\n\n### Validation\n- Validate all inputs before execution\n- Return structured errors for invalid input\n- Type coercion where safe\n\n### Error Handling\n- Clear error codes and messages\n- Suggest corrections if possible\n- Never expose internal errors\n\n### Safety\n- Confirm destructive actions\n- Rate limit expensive operations\n- Log all tool invocations","start_line":1,"end_line":26}]}
{"source_id":"core","type":"context_engineering","title":"Context Assembly Strategies","signals":["context_engineering"],"tags":["context","assembly","optimization"],"summary":"Strategies for assembling optimal context for LLM prompts including prioritization and compression.","chunks":[{"chunk_id":"core:context-assembly:1","text":"## Context Assembly Strategies\n\n### Priority Ranking\n1. Current task instructions (highest)\n2. Immediate user query\n3. Relevant code/docs (RAG)\n4. Conversation history\n5. User profile/preferences\n6. General knowledge (lowest)\n\n### Compression Techniques\n- **Summarization**: Condense long histories\n- **Truncation**: Remove oldest messages\n- **Extraction**: Pull key facts only\n- **Deduplication**: Remove redundant info\n\n### Dynamic Selection\n- Score context by relevance to query\n- Fit within token budget\n- Leave room for response\n\n### Quality Signals\n- Recency (newer = more relevant)\n- Semantic similarity to query\n- User engagement (frequently referenced)\n- Source authority","start_line":1,"end_line":24}]}
{"source_id":"core","type":"plan","title":"Prompt Template Management","signals":["state_machine"],"tags":["prompt","template","management"],"summary":"Plan for managing prompt templates with versioning, testing, and deployment.","chunks":[{"chunk_id":"core:prompt-management:1","text":"## Prompt Template Management\n\n### Storage\n- Version controlled in repo\n- Separate folder: /prompts\n- Named by use case: chat.md, summarize.md\n\n### Templating\n- Use variables: {{user_name}}, {{context}}\n- Jinja2 / Handlebars syntax\n- Validate template before deploy\n\n### Testing\n- Golden test cases per template\n- A/B testing infrastructure\n- Track quality metrics over versions\n\n### Deployment\n- Promote from dev → staging → prod\n- Feature flags for rollback\n- Monitor quality post-deploy\n\n### Best Practices\n- Document template purpose\n- Include example inputs/outputs\n- Review changes like code","start_line":1,"end_line":22}]}
{"source_id":"core","type":"security","title":"LLM Security Patterns","signals":["security_pattern"],"tags":["llm","security","injection"],"summary":"Security patterns for LLM applications covering prompt injection, data exfiltration, and model abuse.","chunks":[{"chunk_id":"core:llm-security:1","text":"## LLM Security Patterns\n\n### Prompt Injection Prevention\n- Separate system and user prompts clearly\n- Validate user input before insertion\n- Use structured outputs (JSON mode)\n- Monitor for injection attempts\n\n### Data Exfiltration\n- Don't include secrets in prompts\n- Redact PII before sending\n- Validate outputs for leaked data\n- Log requests without sensitive content\n\n### Model Abuse Prevention\n- Rate limit by user/IP\n- Cost caps per request/session\n- Monitor for unusual patterns\n- CAPTCHA for public endpoints\n\n### Supply Chain\n- Validate third-party prompts\n- Audit MCP servers\n- Pin model versions\n- Test after model updates","start_line":1,"end_line":24}]}
{"source_id":"core","type":"template","title":"AI Project README Template","signals":["evidence_audit"],"tags":["readme","ai","template"],"summary":"README template for AI-powered projects covering model usage, prompts, and evaluation.","chunks":[{"chunk_id":"core:ai-readme-template:1","text":"## AI Project README Template\n\n### Model Usage\n- **Provider**: OpenAI / Anthropic / Local\n- **Model**: gpt-4-turbo / claude-3.5-sonnet\n- **Estimated Cost**: $X per 1000 requests\n\n### Setup\n```bash\nexport OPENAI_API_KEY=sk-...\nnpm install\nnpm start\n```\n\n### Prompts\n- Located in `/prompts` directory\n- Main system prompt: `prompts/system.md`\n- Tool definitions: `prompts/tools.json`\n\n### Evaluation\n- Golden test set: `eval/golden.jsonl`\n- Run evals: `npm run eval`\n- Baseline scores in `eval/baseline.json`\n\n### Known Limitations\n- May hallucinate on [topic]\n- Slow on [edge case]\n- Not suitable for [use case]","start_line":1,"end_line":28}]}
{"source_id":"core","type":"context_engineering","title":"AI Environment Pack System","signals":["context_engineering","provider_adapter"],"tags":["vibe-coding","pack","environment","ai"],"summary":"Pack system for composable, versionable AI environments - the evolution from vibe coding to vibe engineering.","chunks":[{"chunk_id":"core:ai-pack-system:1","text":"## AI Environment Pack System\n\n### Concept: Vibe Engineering\nElevate vibe coding to vibe engineering by treating AI context as composable, versionable environments.\n\n### Pack Components\n- **Rules**: AI operating instructions and workflows\n- **Context**: Persistent knowledge base (memory bank)\n- **Tools**: Helper scripts the AI can invoke\n\n### Pack Lifecycle\n1. **Add**: Install packs to project library\n2. **Compose**: Mix and match packs for your needs\n3. **Sync**: Generate assistant-specific rule files\n4. **Version**: Track changes in source control\n\n### Profile System\n- Named groups of packs for instant switching\n- `dev-profile`: Development packs\n- `review-profile`: Code review packs\n- `deploy-profile`: Deployment packs\n\n### Multi-Assistant Sync\n- Define once, deploy to: Cursor, Copilot, Claude, Gemini\n- Automatic format translation per assistant\n- Consistent behavior across all tools","start_line":1,"end_line":26}]}
{"source_id":"core","type":"contract","title":"AI Memory Bank Contract","signals":["contract_lock","context_engineering"],"tags":["memory","context","persistence","ai"],"summary":"Contract for persistent AI memory systems - project documentation, learned patterns, and context that survives across sessions.","chunks":[{"chunk_id":"core:memory-bank-contract:1","text":"## AI Memory Bank Contract\n\n### Memory Structure\n```\nmemory/\n├── project/          # Project-specific context\n│   ├── architecture.md\n│   ├── conventions.md\n│   └── decisions.md\n├── docs/             # Documentation context\n│   └── user_guide/\n└── patterns/         # Learned patterns\n    └── common_fixes.md\n```\n\n### Memory Types\n- **Static**: Architecture docs, coding standards\n- **Learned**: Patterns discovered during development\n- **Session**: Current task context (ephemeral)\n\n### Memory Operations\n- **Read**: Always include relevant memory in context\n- **Write**: Update after significant discoveries\n- **Prune**: Remove outdated information\n- **Version**: Track memory changes in git\n\n### Best Practices\n- Keep memory files focused and concise\n- Use markdown for human+AI readability\n- Reference memory in conversations","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"Project Analysis & Detection Plan","signals":["state_machine"],"tags":["analysis","detection","automation"],"summary":"Plan for automatic project analysis - detecting technologies, frameworks, and patterns to generate AI-aware configurations.","chunks":[{"chunk_id":"core:project-analysis-plan:1","text":"## Project Analysis & Detection Plan\n\n### Phase 1: Technology Detection\n- [ ] Parse package.json / Cargo.toml / pyproject.toml\n- [ ] Detect frameworks (React, Next.js, FastAPI, Axum)\n- [ ] Identify build tools (Vite, Webpack, esbuild)\n- [ ] Recognize testing frameworks (Jest, Vitest, pytest)\n\n### Phase 2: Pattern Recognition\n- [ ] Analyze file structure conventions\n- [ ] Detect naming patterns (camelCase, snake_case)\n- [ ] Identify architectural patterns (MVC, Clean, Hexagonal)\n- [ ] Extract common code patterns\n\n### Phase 3: Rule Generation\n- [ ] Create project-specific AI rules\n- [ ] Generate framework best practices\n- [ ] Add detected patterns as conventions\n- [ ] Configure tool integrations\n\n### Phase 4: Deployment\n- [ ] Generate .cursorrules / CLAUDE.md\n- [ ] Create .github/copilot-instructions.md\n- [ ] Set up IDE-specific configurations","start_line":1,"end_line":24}]}
{"source_id":"core","type":"contract","title":"AI Context Migration Contract","signals":["contract_lock","context_engineering"],"tags":["migration","context","conversion"],"summary":"Contract for migrating existing AI contexts between different assistant formats.","chunks":[{"chunk_id":"core:context-migration-contract:1","text":"## AI Context Migration Contract\n\n### Supported Sources\n- CLAUDE.md files and .claude/ directories\n- .cursorrules and .cursor/ configurations\n- .github/copilot-instructions.md\n- .windsurf/ rules and configurations\n\n### Migration Process\n1. **Detect**: Scan for existing AI contexts\n2. **Parse**: Extract rules, patterns, instructions\n3. **Normalize**: Convert to universal format\n4. **Enhance**: Add project-specific adaptations\n5. **Deploy**: Generate target assistant files\n\n### Confidence Levels\n- **High**: Direct format match, no changes\n- **Medium**: Minor adaptations needed\n- **Low**: Manual review recommended\n\n### Best Practices\n- Always backup before migration\n- Review converted rules manually\n- Test with simple prompts first\n- Iterate based on AI behavior","start_line":1,"end_line":24}]}
{"source_id":"core","type":"contract","title":"Decorator-Based Tool Pattern","signals":["contract_lock","agent_pattern"],"tags":["tools","decorator","python","pattern"],"summary":"Pattern for creating AI tools using decorators - converting Python functions into Claude-compatible tools.","chunks":[{"chunk_id":"core:decorator-tool-pattern:1","text":"## Decorator-Based Tool Pattern\n\n### Basic Pattern\n```python\nfrom toolkit import BaseTool, tool\n\nclass CalculatorTool(BaseTool):\n    @tool()\n    async def add(self, a: float, b: float) -> dict:\n        \"\"\"Adds two numbers together\"\"\"\n        return {\"result\": a + b}\n```\n\n### Decorator Options\n- `@tool()`: Basic async tool\n- `@tool(parallel=True)`: CPU-intensive, runs in process pool\n- `@tool(timeout_s=60)`: Custom timeout\n\n### Best Practices\n- Use type hints for auto-schema generation\n- Docstrings become tool descriptions\n- Return dicts for structured output\n- Use context managers for cleanup\n\n### Error Handling\n- Raise specific exceptions\n- Return error info in dict\n- Log failures for debugging","start_line":1,"end_line":26}]}
{"source_id":"core","type":"security","title":"Runtime Isolation Patterns","signals":["security_pattern","agent_pattern"],"tags":["isolation","docker","security","runtime"],"summary":"Patterns for isolating AI tool execution using containers for production safety.","chunks":[{"chunk_id":"core:runtime-isolation:1","text":"## Runtime Isolation Patterns\n\n### Why Isolate?\n- Prevent unintended system access\n- Control exactly which tools are available\n- Consistent behavior across environments\n- Production-ready safety guarantees\n\n### Docker Isolation (Production)\n- Container runs only defined tools\n- No access to system tools (ls, grep)\n- Clean, predictable execution\n- ~3 second startup overhead\n\n### Subprocess Isolation (Development)\n- Process isolation with limited scope\n- Fast startup (~0.5 seconds)\n- Good for local development\n- Still isolated from main process\n\n### Permission-Based File Access\n```python\npermissions = [\n    (\"*.txt\", \"read\"),\n    (\"data/**\", \"write\"),\n    (\"logs/*.log\", \"read\"),\n]\nfs_tool = FileSystemTool(permissions=permissions)\n```","start_line":1,"end_line":28}]}
{"source_id":"core","type":"template","title":"Universal AI Rules Template","signals":["context_engineering"],"tags":["rules","universal","template"],"summary":"Universal template for AI rules that work across multiple assistants - Cursor, Copilot, Claude, Gemini.","chunks":[{"chunk_id":"core:universal-ai-rules:1","text":"## Universal AI Rules Template\n\n### Format: Works Everywhere\n```markdown\n# Project: [Name]\n\n## Context\n- Language: [TypeScript/Rust/Python]\n- Framework: [Next.js/Axum/FastAPI]\n- Testing: [Vitest/pytest/cargo test]\n\n## Code Conventions\n- Naming: [camelCase/snake_case]\n- Imports: [order preference]\n- Error handling: [pattern]\n\n## Architecture\n- Structure: [description]\n- Key directories: [list]\n- Entry points: [files]\n\n## Do's\n- [Preferred patterns]\n\n## Don'ts\n- [Anti-patterns to avoid]\n```\n\n### Assistant-Specific Locations\n- Cursor: `.cursor/rules/*.mdc`\n- Copilot: `.github/copilot-instructions.md`\n- Claude: `CLAUDE.md` or `.claude/`\n- Gemini: `GEMINI.md`","start_line":1,"end_line":32}]}
{"source_id":"core","type":"plan","title":"AI Rule Authoring Plan","signals":["state_machine"],"tags":["rules","authoring","plan"],"summary":"Plan for authoring effective AI assistant rules that improve code suggestions and reduce iterations.","chunks":[{"chunk_id":"core:rule-authoring-plan:1","text":"## AI Rule Authoring Plan\n\n### Phase 1: Analyze Project\n- [ ] Identify tech stack (framework, language, tools)\n- [ ] Document coding conventions\n- [ ] List common patterns used\n- [ ] Note anti-patterns to avoid\n\n### Phase 2: Write Core Rules\n- [ ] Project overview (one paragraph)\n- [ ] Directory structure explanation\n- [ ] Key architectural decisions\n- [ ] Code style requirements\n\n### Phase 3: Add Specifics\n- [ ] Framework-specific patterns\n- [ ] Testing conventions\n- [ ] Error handling patterns\n- [ ] Security requirements\n\n### Phase 4: Test & Iterate\n- [ ] Test with simple prompts\n- [ ] Measure suggestion quality\n- [ ] Refine based on AI behavior\n- [ ] Remove redundant rules","start_line":1,"end_line":24}]}
{"source_id":"core","type":"evidence","title":"AI Suggestion Quality Metrics","signals":["evidence_audit","iterate_loop"],"tags":["metrics","quality","ai","suggestions"],"summary":"Metrics for measuring and improving AI code suggestion quality.","chunks":[{"chunk_id":"core:ai-suggestion-metrics:1","text":"## AI Suggestion Quality Metrics\n\n### Key Metrics\n- **Acceptance Rate**: % of suggestions accepted\n- **First-Try Success**: % correct on first attempt\n- **Iteration Count**: Average edits per task\n- **Relevance Score**: How well suggestions match context\n\n### Measurement Methods\n- Track accepted vs rejected suggestions\n- Count back-and-forth clarifications\n- Time from prompt to working code\n- Manual quality sampling\n\n### Improvement Signals\n- 60% faster initial suggestions (good rules)\n- 85% more relevant completions\n- 40% fewer clarifications needed\n- 90% consistency in patterns\n\n### Optimization Actions\n- Add missing context to rules\n- Remove conflicting instructions\n- Include more examples\n- Update for new patterns","start_line":1,"end_line":24}]}
{"source_id":"core","type":"contract","title":"Type-Safe Data Transfer Contract","signals":["contract_lock","agent_pattern"],"tags":["data-transfer","pydantic","type-safety"],"summary":"Contract for type-safe data transfer between AI agents and applications using schema validation.","chunks":[{"chunk_id":"core:data-transfer-contract:1","text":"## Type-Safe Data Transfer Contract\n\n### Schema Definition (Pydantic)\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass UserProfile(BaseModel):\n    name: str = Field(..., description=\"Full name\")\n    age: int = Field(..., ge=0, le=150)\n    interests: List[str] = Field(default_factory=list)\n```\n\n### Transfer Flow\n1. Define schema with validation rules\n2. AI receives schema with descriptions\n3. AI outputs data matching schema\n4. Application validates and uses data\n\n### Benefits\n- Type checking catches errors early\n- Descriptions guide AI output format\n- Validation ensures data quality\n- Clear contract between AI and code\n\n### Error Handling\n- Return validation errors to AI\n- Allow retry with corrections\n- Log malformed outputs for analysis","start_line":1,"end_line":26}]}
{"source_id":"core","type":"plan","title":"Composable AI Environment Plan","signals":["state_machine","context_engineering"],"tags":["composable","environment","plan"],"summary":"Plan for building composable AI environments that can be mixed, matched, and versioned.","chunks":[{"chunk_id":"core:composable-env-plan:1","text":"## Composable AI Environment Plan\n\n### Phase 1: Core Packs\n- [ ] Language pack (TypeScript, Rust, Python)\n- [ ] Framework pack (React, Axum, FastAPI)\n- [ ] Testing pack (unit, integration, e2e)\n- [ ] Security pack (auth, validation, secrets)\n\n### Phase 2: Role Packs\n- [ ] Developer pack (coding conventions)\n- [ ] Reviewer pack (code review checklist)\n- [ ] DevOps pack (CI/CD, deployment)\n- [ ] Architect pack (design decisions)\n\n### Phase 3: Project Packs\n- [ ] Project-specific conventions\n- [ ] Custom patterns and utilities\n- [ ] Team preferences\n\n### Composition Rules\n- Packs layer, later overrides earlier\n- No circular dependencies\n- Clear precedence order\n- Document conflicts","start_line":1,"end_line":24}]}
{"source_id":"core","type":"iterate","title":"AI Context Debugging Playbook","signals":["iterate_loop"],"tags":["debugging","context","ai"],"summary":"Playbook for debugging when AI assistant gives unexpected or incorrect suggestions.","chunks":[{"chunk_id":"core:ai-context-debug:1","text":"## AI Context Debugging Playbook\n\n### Symptom: Wrong Framework Patterns\n- Check: Are framework rules loaded?\n- Fix: Add explicit framework to rules\n- Verify: Test with framework-specific prompt\n\n### Symptom: Ignoring Conventions\n- Check: Is convention documented in rules?\n- Fix: Add clear Do/Don't examples\n- Verify: Ask AI to explain conventions\n\n### Symptom: Outdated Suggestions\n- Check: When were rules last updated?\n- Fix: Refresh rules with current patterns\n- Verify: Check for deprecated patterns\n\n### Symptom: Context Overflow\n- Check: Total token count in context\n- Fix: Summarize or split rules\n- Verify: Most important rules at top\n\n### General Debug Steps\n1. Review current loaded context\n2. Simplify to minimal reproduction\n3. Add explicit instruction\n4. Document fix in rules","start_line":1,"end_line":26}]}
{"source_id":"core","type":"evidence","title":"CLAUDE.md Quality Scoring System","signals":["evidence_audit","context_engineering"],"tags":["claude","quality","scoring","metrics"],"summary":"Quality scoring system (0-100) for evaluating CLAUDE.md files based on completeness, clarity, and effectiveness.","chunks":[{"chunk_id":"core:claude-quality-scoring:1","text":"## CLAUDE.md Quality Scoring System\n\n### Scoring Categories (0-100)\n\n**Structure (25 points)**\n- Clear sections: 5pts\n- Logical organization: 5pts\n- Proper markdown formatting: 5pts\n- Table of contents: 5pts\n- Consistent heading levels: 5pts\n\n**Content (35 points)**\n- Project overview present: 5pts\n- Tech stack documented: 5pts\n- Architecture explained: 10pts\n- Coding conventions: 10pts\n- Examples included: 5pts\n\n**Actionability (25 points)**\n- Clear Do's list: 10pts\n- Clear Don'ts list: 10pts\n- Decision criteria: 5pts\n\n**Completeness (15 points)**\n- All major areas covered: 5pts\n- No contradictions: 5pts\n- Up-to-date information: 5pts\n\n### Quality Tiers\n- 90-100: Excellent - Ready for production\n- 70-89: Good - Minor improvements needed\n- 50-69: Fair - Significant gaps\n- 0-49: Poor - Major rewrite needed","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Guardian Agent Pattern","signals":["contract_lock","agent_pattern"],"tags":["guardian","auto-sync","agent","pattern"],"summary":"Guardian agent pattern for automatic synchronization of AI context files when project changes are detected.","chunks":[{"chunk_id":"core:guardian-agent-pattern:1","text":"## Guardian Agent Pattern\n\n### Purpose\nAutomatically keep AI context files in sync with codebase changes.\n\n### Trigger Events\n- Package dependency changes\n- New files/directories created\n- Configuration file updates\n- Architecture changes detected\n\n### Guardian Actions\n1. **Watch**: Monitor file system for changes\n2. **Analyze**: Determine if context update needed\n3. **Propose**: Generate context file changes\n4. **Apply**: Update files (with confirmation or auto)\n\n### Implementation Modes\n- **Auto-sync**: Apply changes automatically\n- **Suggest**: Create PR/commit with changes\n- **Report**: Only notify, no changes\n\n### Monitored Files\n- package.json / Cargo.toml / pyproject.toml\n- tsconfig.json / .eslintrc\n- Directory structure changes\n- New framework files detected\n\n### Best Practices\n- Run on git hooks (pre-commit)\n- CI validation of context freshness\n- Version control all context files","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"Modular CLAUDE.md Architecture Plan","signals":["state_machine","context_engineering"],"tags":["claude","modular","architecture","plan"],"summary":"Plan for modular CLAUDE.md architecture with root and component-specific context files.","chunks":[{"chunk_id":"core:modular-claude-plan:1","text":"## Modular CLAUDE.md Architecture Plan\n\n### Structure\n```\nproject/\n├── CLAUDE.md              # Root: global context\n├── backend/\n│   └── CLAUDE.md          # Backend-specific\n├── frontend/\n│   └── CLAUDE.md          # Frontend-specific\n├── shared/\n│   └── CLAUDE.md          # Shared libraries\n└── docs/\n    └── CLAUDE.md          # Documentation context\n```\n\n### Root CLAUDE.md Contains\n- Project overview and mission\n- Cross-cutting concerns\n- Team conventions\n- Architecture principles\n\n### Component CLAUDE.md Contains\n- Component-specific patterns\n- Local dependencies\n- Testing approach for component\n- API contracts\n\n### Context Loading\n- Claude loads nearest CLAUDE.md first\n- Inherits from parent directories\n- Local rules override global","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Spec-Driven Development Contract","signals":["contract_lock","agent_pattern"],"tags":["spec","specification","development","contract"],"summary":"Contract for spec-driven development where specifications drive automated code generation and validation.","chunks":[{"chunk_id":"core:spec-driven-contract:1","text":"## Spec-Driven Development Contract\n\n### Specification Structure\n```yaml\nspec:\n  name: UserService\n  type: service\n  description: Handles user CRUD operations\n  \ninterfaces:\n  - name: createUser\n    input: CreateUserInput\n    output: User\n    errors: [ValidationError, DuplicateError]\n    \nconstraints:\n  - email must be unique\n  - password min 8 characters\n  - rate limit: 100/min\n```\n\n### Workflow\n1. **Write Spec**: Human defines requirements\n2. **Generate**: AI generates implementation\n3. **Validate**: Tests verify spec compliance\n4. **Iterate**: Refine spec and regenerate\n\n### Benefits\n- Single source of truth\n- Automated test generation\n- Documentation always current\n- Clear contract between teams","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"Autonomous Coding Agent Plan","signals":["state_machine","agent_pattern"],"tags":["autonomous","agent","coding","plan"],"summary":"Plan for building autonomous coding agents that can plan, implement, and validate code changes.","chunks":[{"chunk_id":"core:autonomous-agent-plan:1","text":"## Autonomous Coding Agent Plan\n\n### Phase 1: Understanding\n- [ ] Parse issue/task description\n- [ ] Analyze relevant codebase context\n- [ ] Identify affected files and functions\n- [ ] Map dependencies and impacts\n\n### Phase 2: Planning\n- [ ] Break task into subtasks\n- [ ] Define success criteria\n- [ ] Estimate complexity\n- [ ] Identify risks and edge cases\n\n### Phase 3: Implementation\n- [ ] Generate code changes\n- [ ] Write/update tests\n- [ ] Update documentation\n- [ ] Format and lint\n\n### Phase 4: Validation\n- [ ] Run test suite\n- [ ] Check coverage maintained\n- [ ] Verify no regressions\n- [ ] Security scan\n\n### Phase 5: Submission\n- [ ] Create branch and commits\n- [ ] Open pull request\n- [ ] Respond to review feedback\n- [ ] Iterate until merged","start_line":1,"end_line":30}]}
{"source_id":"core","type":"contract","title":"SDLC Automation Contract","signals":["contract_lock","agent_pattern"],"tags":["sdlc","automation","lifecycle","contract"],"summary":"Contract for AI-automated Software Development Lifecycle covering requirements, design, implementation, testing, and deployment.","chunks":[{"chunk_id":"core:sdlc-automation-contract:1","text":"## SDLC Automation Contract\n\n### Automated Phases\n\n**1. Requirements Analysis**\n- Parse natural language requirements\n- Extract user stories\n- Identify acceptance criteria\n- Generate test scenarios\n\n**2. Design**\n- Generate architecture diagrams\n- Propose database schema\n- Define API contracts\n- Create component breakdown\n\n**3. Implementation**\n- Generate boilerplate code\n- Implement business logic\n- Create tests alongside code\n- Add inline documentation\n\n**4. Testing**\n- Run unit tests\n- Execute integration tests\n- Perform security scanning\n- Validate performance\n\n**5. Deployment**\n- Generate CI/CD pipelines\n- Create deployment configs\n- Set up monitoring\n- Automate rollback procedures\n\n### Human Checkpoints\n- Design review before implementation\n- Code review before merge\n- Security approval before production","start_line":1,"end_line":34}]}
{"source_id":"core","type":"plan","title":"AI Team Collaboration Plan","signals":["state_machine","agent_pattern"],"tags":["team","collaboration","multi-agent","plan"],"summary":"Plan for AI agents working as a team with different roles and responsibilities.","chunks":[{"chunk_id":"core:ai-team-plan:1","text":"## AI Team Collaboration Plan\n\n### Agent Roles\n\n**Architect Agent**\n- Reviews design decisions\n- Ensures consistency with patterns\n- Validates architecture compliance\n\n**Developer Agent**\n- Implements code changes\n- Writes unit tests\n- Follows coding conventions\n\n**QA Agent**\n- Reviews code for bugs\n- Writes integration tests\n- Validates edge cases\n\n**Security Agent**\n- Scans for vulnerabilities\n- Reviews auth/authz logic\n- Checks for data exposure\n\n**DevOps Agent**\n- Manages CI/CD pipelines\n- Handles deployment configs\n- Monitors production health\n\n### Collaboration Protocol\n1. Task assigned to Developer Agent\n2. Implementation reviewed by QA Agent\n3. Architecture validated by Architect Agent\n4. Security approved by Security Agent\n5. Deployment handled by DevOps Agent","start_line":1,"end_line":34}]}
{"source_id":"core","type":"template","title":"Task as Code Template","signals":["agent_pattern"],"tags":["task","code","template","orchestration"],"summary":"Template for defining tasks as code - structured task definitions that AI agents can execute.","chunks":[{"chunk_id":"core:task-as-code:1","text":"## Task as Code Template\n\n### Task Definition (YAML)\n```yaml\ntask:\n  id: implement-user-auth\n  title: Implement User Authentication\n  type: feature\n  priority: high\n  \ncontext:\n  - src/auth/**\n  - docs/security.md\n  \nsteps:\n  - name: Design\n    agent: architect\n    output: design_doc.md\n    \n  - name: Implement\n    agent: developer\n    input: design_doc.md\n    output: code_changes\n    \n  - name: Test\n    agent: qa\n    input: code_changes\n    output: test_results\n    \nvalidation:\n  - tests_pass: true\n  - coverage_min: 80\n  - security_scan: clean\n```\n\n### Benefits\n- Reproducible task execution\n- Clear progress tracking\n- Auditable workflow\n- Easy to iterate and improve","start_line":1,"end_line":38}]}
{"source_id":"core","type":"contract","title":"AI-First Development Contract","signals":["contract_lock","agent_pattern"],"tags":["ai-first","development","contract"],"summary":"Contract for AI-first development methodology where AI is the primary implementer and humans provide oversight.","chunks":[{"chunk_id":"core:ai-first-contract:1","text":"## AI-First Development Contract\n\n### Principles\n1. **AI Generates, Human Validates**\n   - AI writes first draft of code\n   - Human reviews and approves\n   - AI iterates on feedback\n\n2. **Specification Over Implementation**\n   - Clear specs before coding\n   - Tests define behavior\n   - AI implements to pass tests\n\n3. **Continuous Context**\n   - Project context always available\n   - Historical decisions preserved\n   - Patterns documented and enforced\n\n### Workflow\n```\nHuman: Define requirement\n  ↓\nAI: Generate implementation\n  ↓\nCI: Validate (tests, lint, security)\n  ↓\nHuman: Review and approve\n  ↓\nAI: Address feedback\n  ↓\nMerge and Deploy\n```\n\n### Guardrails\n- No AI changes to production without review\n- Security-sensitive code requires human\n- Breaking changes flagged for discussion","start_line":1,"end_line":32}]}
{"source_id":"core","type":"evidence","title":"Agent Execution Audit Trail","signals":["evidence_audit","agent_pattern"],"tags":["audit","agent","execution","tracing"],"summary":"Standards for auditing AI agent execution including actions taken, decisions made, and outcomes.","chunks":[{"chunk_id":"core:agent-audit-trail:1","text":"## Agent Execution Audit Trail\n\n### Required Fields\n- **session_id**: Unique execution session\n- **timestamp**: ISO 8601 with timezone\n- **agent_id**: Which agent executed\n- **action**: What was attempted\n- **input**: Parameters provided\n- **output**: Result or error\n- **duration_ms**: Execution time\n\n### Action Categories\n- **read**: File read, search, retrieval\n- **write**: File create/modify, code generation\n- **execute**: Run commands, tests\n- **communicate**: API calls, messages\n\n### Audit Storage\n- JSON Lines format\n- Retained for 90 days minimum\n- Searchable by session, agent, action\n- Redact sensitive data\n\n### Compliance\n- Reproducible: Re-run from audit\n- Traceable: Full decision chain\n- Auditable: Human can review\n- Deletable: GDPR compliant","start_line":1,"end_line":28}]}
{"source_id":"core","type":"iterate","title":"Agent Error Recovery Playbook","signals":["iterate_loop","agent_pattern"],"tags":["agent","error","recovery","playbook"],"summary":"Playbook for recovering from AI agent errors including retry strategies, fallbacks, and escalation.","chunks":[{"chunk_id":"core:agent-error-recovery:1","text":"## Agent Error Recovery Playbook\n\n### Error Categories\n\n**Transient Errors**\n- API timeout\n- Rate limit exceeded\n- Network hiccup\n→ Retry with exponential backoff\n\n**Input Errors**\n- Invalid parameters\n- Missing context\n- Ambiguous instruction\n→ Request clarification or fix input\n\n**Logic Errors**\n- Wrong tool selected\n- Incorrect reasoning\n- Hallucinated facts\n→ Reset and retry with more context\n\n**Resource Errors**\n- Token limit exceeded\n- Memory exhausted\n- Cost limit reached\n→ Summarize context, reduce scope\n\n### Escalation Triggers\n- Same error 3 times\n- Critical operation failed\n- Security violation detected\n→ Pause and notify human","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Intelligent Test Generation Contract","signals":["contract_lock","iterate_loop"],"tags":["testing","generation","ai","contract"],"summary":"Contract for AI-powered test generation covering unit tests, edge cases, and property-based testing.","chunks":[{"chunk_id":"core:test-generation-contract:1","text":"## Intelligent Test Generation Contract\n\n### Test Types Generated\n\n**Unit Tests**\n- Happy path scenarios\n- Edge cases from type analysis\n- Error handling paths\n- Boundary conditions\n\n**Integration Tests**\n- API endpoint coverage\n- Database interactions\n- External service mocks\n\n**Property-Based Tests**\n- Invariants from function signatures\n- Fuzz testing for inputs\n- Randomized data generation\n\n### Generation Rules\n- One test file per source file\n- Descriptive test names\n- AAA pattern (Arrange, Act, Assert)\n- Isolated, no shared state\n\n### Quality Gates\n- Coverage: 80% minimum\n- All assertions meaningful\n- No flaky tests\n- Fast execution (< 10s per suite)","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"Code Understanding Pipeline Plan","signals":["state_machine","context_engineering"],"tags":["understanding","analysis","pipeline","plan"],"summary":"Plan for building code understanding pipelines that analyze and index codebases for AI consumption.","chunks":[{"chunk_id":"core:code-understanding-plan:1","text":"## Code Understanding Pipeline Plan\n\n### Phase 1: Parsing\n- [ ] Parse source files (AST extraction)\n- [ ] Extract function/class signatures\n- [ ] Identify dependencies and imports\n- [ ] Map file relationships\n\n### Phase 2: Analysis\n- [ ] Generate call graphs\n- [ ] Identify design patterns\n- [ ] Detect code smells\n- [ ] Calculate complexity metrics\n\n### Phase 3: Indexing\n- [ ] Create semantic embeddings\n- [ ] Build full-text search index\n- [ ] Store in vector database\n- [ ] Generate summaries per file/function\n\n### Phase 4: Context Generation\n- [ ] Auto-generate project README\n- [ ] Create CLAUDE.md from analysis\n- [ ] Build API documentation\n- [ ] Generate onboarding guide\n\n### Refresh Strategy\n- Incremental updates on file change\n- Full reindex weekly\n- Cache invalidation on structure change","start_line":1,"end_line":28}]}
{"source_id":"core","type":"security","title":"AI Agent Sandbox Security","signals":["security_pattern","agent_pattern"],"tags":["sandbox","security","isolation","agent"],"summary":"Security patterns for sandboxing AI agent execution to prevent unintended system access.","chunks":[{"chunk_id":"core:agent-sandbox-security:1","text":"## AI Agent Sandbox Security\n\n### Isolation Layers\n\n**Process Isolation**\n- Separate process per agent\n- Limited system calls (seccomp)\n- No network access by default\n- Restricted file system view\n\n**Container Isolation**\n- Minimal base image\n- Read-only root filesystem\n- No privileged access\n- Resource limits (CPU, memory)\n\n**Virtual Machine Isolation**\n- Full OS separation\n- Hardware-level isolation\n- Snapshot and rollback\n- For highest security requirements\n\n### Permission Model\n```yaml\npermissions:\n  files:\n    read: [\"src/**\", \"docs/**\"]\n    write: [\"output/**\"]\n  network:\n    allowed: [\"api.openai.com\"]\n  commands:\n    allowed: [\"npm test\", \"cargo build\"]\n    denied: [\"rm -rf\", \"curl\"]\n```","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"GEMINI.md Template","signals":["context_engineering"],"tags":["gemini","template","google","ai"],"summary":"Template for GEMINI.md configuration files for Google Gemini AI assistant context.","chunks":[{"chunk_id":"core:gemini-template:1","text":"## GEMINI.md Template\n\n### File: GEMINI.md\n```markdown\n# Project Context for Gemini\n\n## Overview\n[Brief project description]\n\n## Tech Stack\n- Language: [TypeScript/Python/Go]\n- Framework: [Next.js/FastAPI/Gin]\n- Database: [PostgreSQL/MongoDB]\n\n## Project Structure\n```\nsrc/\n├── api/       # API routes\n├── services/  # Business logic\n├── models/    # Data models\n└── utils/     # Utilities\n```\n\n## Coding Standards\n- [Naming conventions]\n- [Error handling patterns]\n- [Testing requirements]\n\n## Gemini-Specific Instructions\n- Use structured outputs when possible\n- Include code comments for complex logic\n- Follow project's existing patterns\n```","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"WINDSURF.md Template","signals":["context_engineering"],"tags":["windsurf","template","ai"],"summary":"Template for Windsurf AI assistant configuration files.","chunks":[{"chunk_id":"core:windsurf-template:1","text":"## WINDSURF.md Template\n\n### File: .windsurf/rules.md\n```markdown\n# Windsurf Rules\n\n## Project Info\n- Name: [Project Name]\n- Type: [Web App/API/CLI]\n- Language: [Primary Language]\n\n## Cascade Agent Behavior\n- Always read relevant files before changes\n- Write tests for new functions\n- Format code after modifications\n- Commit with conventional messages\n\n## Memory Persistence\n- Store learned patterns in .windsurf/memory/\n- Reference previous solutions\n- Track project-specific conventions\n\n## Tool Usage\n- Prefer built-in tools over shell commands\n- Use semantic search for large codebases\n- Batch file operations when possible\n```","start_line":1,"end_line":26}]}
{"source_id":"core","type":"contract","title":"Semantic Code Search Contract","signals":["contract_lock","context_engineering"],"tags":["search","semantic","code","contract"],"summary":"Contract for semantic code search systems enabling natural language queries over codebases.","chunks":[{"chunk_id":"core:semantic-search-contract:1","text":"## Semantic Code Search Contract\n\n### Index Components\n- **Code Chunks**: Function-level embeddings\n- **Documentation**: Comments, docstrings, README\n- **Commit Messages**: Historical context\n- **Issues/PRs**: Problem-solution pairs\n\n### Query Types\n- \"How does authentication work?\"\n- \"Where is the database connection?\"\n- \"Functions that handle user input\"\n- \"Similar code to this function\"\n\n### Ranking Signals\n- Semantic similarity (cosine distance)\n- Recency (newer code preferred)\n- Popularity (frequently accessed)\n- Relevance (file path match)\n\n### Quality Metrics\n- Recall@10: 80% minimum\n- Latency: < 500ms p95\n- Precision: Relevant in top 3 results\n\n### Index Freshness\n- Real-time on file save\n- Full reindex nightly","start_line":1,"end_line":28}]}
{"source_id":"core","type":"plan","title":"Knowledge Graph for Code Plan","signals":["state_machine","context_engineering"],"tags":["knowledge-graph","code","plan"],"summary":"Plan for building knowledge graphs that represent codebase structure, relationships, and semantics.","chunks":[{"chunk_id":"core:knowledge-graph-plan:1","text":"## Knowledge Graph for Code Plan\n\n### Phase 1: Entity Extraction\n- [ ] Files, classes, functions, variables\n- [ ] Types and interfaces\n- [ ] Configuration values\n- [ ] External dependencies\n\n### Phase 2: Relationship Mapping\n- [ ] imports/exports\n- [ ] calls/called_by\n- [ ] implements/extends\n- [ ] uses/used_by\n- [ ] tests/tested_by\n\n### Phase 3: Semantic Enrichment\n- [ ] Purpose descriptions\n- [ ] Category classification\n- [ ] Complexity ratings\n- [ ] Quality scores\n\n### Phase 4: Query Interface\n- [ ] GraphQL API for queries\n- [ ] Natural language query translation\n- [ ] Visualization dashboard\n- [ ] Integration with AI assistants\n\n### Use Cases\n- Impact analysis for changes\n- Dependency visualization\n- Dead code detection\n- Architecture documentation","start_line":1,"end_line":32}]}
{"source_id":"core","type":"evidence","title":"AI Development Metrics Dashboard","signals":["evidence_audit"],"tags":["metrics","dashboard","development"],"summary":"Key metrics for tracking AI-assisted development productivity and quality.","chunks":[{"chunk_id":"core:ai-dev-metrics:1","text":"## AI Development Metrics Dashboard\n\n### Productivity Metrics\n- **Throughput**: PRs merged per week\n- **Velocity**: Story points completed\n- **Cycle Time**: Commit to production\n- **AI Contribution**: % code by AI\n\n### Quality Metrics\n- **Defect Rate**: Bugs per feature\n- **Test Coverage**: % lines covered\n- **Code Review Time**: Hours per PR\n- **Rework Rate**: % changes revised\n\n### AI-Specific Metrics\n- **Suggestion Acceptance**: % accepted\n- **Context Relevance**: Score 0-100\n- **Iteration Count**: Attempts per task\n- **Token Efficiency**: Tokens per output\n\n### Trend Indicators\n- Week-over-week improvement\n- Compare AI vs manual tasks\n- Track regression causes\n- Identify training opportunities","start_line":1,"end_line":26}]}
{"source_id":"core","type":"iterate","title":"Continuous Learning for AI Agents","signals":["iterate_loop","agent_pattern"],"tags":["learning","continuous","agent"],"summary":"Patterns for AI agents to learn and improve from past interactions and outcomes.","chunks":[{"chunk_id":"core:agent-learning:1","text":"## Continuous Learning for AI Agents\n\n### Learning Sources\n- **Success Patterns**: What worked well\n- **Failure Analysis**: What went wrong\n- **User Feedback**: Explicit corrections\n- **Outcome Data**: Test results, deployments\n\n### Memory Updates\n- Store successful solutions\n- Record common mistakes\n- Track user preferences\n- Build project-specific patterns\n\n### Learning Cycle\n1. Execute task\n2. Observe outcome\n3. Extract lessons\n4. Update memory/rules\n5. Apply to future tasks\n\n### Anti-Patterns\n- Overfitting to recent examples\n- Ignoring edge cases\n- Contradicting established rules\n- Learning wrong patterns from noise","start_line":1,"end_line":26}]}
{"source_id":"core","type":"contract","title":"Conversational AI Contract","signals":["contract_lock","agent_pattern"],"tags":["conversational","chat","ai","contract"],"summary":"Contract for conversational AI interfaces covering tone, context management, and interaction patterns.","chunks":[{"chunk_id":"core:conversational-ai-contract:1","text":"## Conversational AI Contract\n\n### Tone & Style\n- **Professional**: Business-appropriate language\n- **Helpful**: Prioritize user success\n- **Concise**: Minimize unnecessary words\n- **Accurate**: Never guess, say \"I don't know\"\n\n### Context Management\n- Remember conversation history\n- Reference earlier messages naturally\n- Summarize long conversations\n- Clear context on topic change\n\n### Interaction Patterns\n- Confirm understanding before acting\n- Ask clarifying questions\n- Provide options when ambiguous\n- Explain reasoning when helpful\n\n### Response Structure\n- Lead with the answer\n- Support with details\n- End with next steps\n- Use formatting for clarity\n\n### Error Handling\n- Acknowledge mistakes\n- Correct and continue\n- Learn from corrections","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"CrewAI Multi-Agent Crew Contract","signals":["contract_lock","agent_pattern"],"tags":["crewai","multi-agent","crew","orchestration"],"summary":"Contract template for CrewAI-style multi-agent crews with roles, tasks, and process definitions.","chunks":[{"chunk_id":"core:crewai-crew-contract:1","text":"## CrewAI Multi-Agent Crew Contract\n\n### Crew Definition\n```yaml\ncrew:\n  name: [Crew Name]\n  process: sequential | hierarchical\n  verbose: true\n  memory: true\n```\n\n### Agent Roles\n```yaml\nagents:\n  researcher:\n    role: \"Senior Data Researcher\"\n    goal: \"Uncover cutting-edge developments in {topic}\"\n    backstory: \"You're a seasoned researcher known for finding relevant information\"\n    tools: [SerperDevTool, WebScraperTool]\n    \n  analyst:\n    role: \"Reporting Analyst\"\n    goal: \"Create detailed reports based on research findings\"\n    backstory: \"You're a meticulous analyst with keen eye for detail\"\n```\n\n### Task Definitions\n```yaml\ntasks:\n  research_task:\n    description: \"Conduct thorough research about {topic}\"\n    expected_output: \"10 bullet points of relevant information\"\n    agent: researcher\n    \n  reporting_task:\n    description: \"Expand research into full report\"\n    expected_output: \"Detailed markdown report\"\n    agent: analyst\n    output_file: report.md\n```","start_line":1,"end_line":38}]}
{"source_id":"core","type":"plan","title":"CrewAI Flows Implementation Plan","signals":["state_machine","agent_pattern"],"tags":["crewai","flows","orchestration","plan"],"summary":"Implementation plan for CrewAI Flows - event-driven workflows with precise control over agent execution.","chunks":[{"chunk_id":"core:crewai-flows-plan:1","text":"## CrewAI Flows Implementation Plan\n\n### Phase 1: Flow Architecture\n- [ ] Define Pydantic state model\n- [ ] Create Flow class extending Flow[State]\n- [ ] Implement @start() entry point\n- [ ] Add @listen() handlers for events\n\n### Phase 2: Crew Integration\n- [ ] Define agents with roles and tools\n- [ ] Create tasks with expected outputs\n- [ ] Configure crew process (sequential/hierarchical)\n- [ ] Connect crew.kickoff() in flow steps\n\n### Phase 3: Control Flow\n- [ ] Add @router() for conditional branching\n- [ ] Implement or_() / and_() combinators\n- [ ] Handle state transitions\n- [ ] Add error recovery handlers\n\n### Phase 4: Production\n- [ ] Enable memory persistence\n- [ ] Add observability/tracing\n- [ ] Configure retries and timeouts\n- [ ] Deploy with Crew Control Plane\n\n### Key Pattern: Crews in Flows\n```python\n@listen(fetch_data)\ndef analyze_with_crew(self, data):\n    crew = Crew(agents=[...], tasks=[...])\n    return crew.kickoff(inputs=data)\n```","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"LangGraph StateGraph Contract","signals":["contract_lock","agent_pattern"],"tags":["langgraph","state","graph","contract"],"summary":"Contract template for LangGraph StateGraph-based agent workflows with durable execution.","chunks":[{"chunk_id":"core:langgraph-contract:1","text":"## LangGraph StateGraph Contract\n\n### State Definition\n```python\nfrom typing_extensions import TypedDict\n\nclass AgentState(TypedDict):\n    messages: list[BaseMessage]\n    context: str\n    current_step: str\n    results: dict\n```\n\n### Graph Construction\n```python\nfrom langgraph.graph import START, StateGraph\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"retrieve\", retrieve_context)\ngraph.add_node(\"generate\", generate_response)\ngraph.add_node(\"validate\", validate_output)\n\ngraph.add_edge(START, \"retrieve\")\ngraph.add_edge(\"retrieve\", \"generate\")\ngraph.add_conditional_edges(\"generate\", should_retry)\n```\n\n### Core Benefits\n- **Durable Execution**: Persists through failures\n- **Human-in-the-Loop**: Interrupt and modify state\n- **Comprehensive Memory**: Short and long-term\n- **LangSmith Integration**: Full observability","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"LangGraph Agent Implementation Plan","signals":["state_machine","agent_pattern"],"tags":["langgraph","agent","implementation","plan"],"summary":"Implementation plan for building LangGraph-based agents with checkpointing and memory.","chunks":[{"chunk_id":"core:langgraph-agent-plan:1","text":"## LangGraph Agent Implementation Plan\n\n### Phase 1: State Design\n- [ ] Define TypedDict for agent state\n- [ ] Plan state transitions\n- [ ] Design checkpoint strategy\n- [ ] Choose memory implementation\n\n### Phase 2: Node Implementation\n- [ ] Create processing nodes (functions)\n- [ ] Implement tool nodes\n- [ ] Add conditional routing logic\n- [ ] Handle error states\n\n### Phase 3: Graph Assembly\n- [ ] Instantiate StateGraph\n- [ ] Add all nodes\n- [ ] Connect edges (static and conditional)\n- [ ] Compile graph with checkpointer\n\n### Phase 4: Integration\n- [ ] Add LangSmith tracing\n- [ ] Configure persistence (SQLite/Postgres)\n- [ ] Implement human-in-the-loop hooks\n- [ ] Deploy with LangSmith Deployment\n\n### Key Pattern: Conditional Edges\n```python\ndef route_decision(state):\n    if state[\"needs_review\"]:\n        return \"human_review\"\n    return \"complete\"\n\ngraph.add_conditional_edges(\"validate\", route_decision)\n```","start_line":1,"end_line":34}]}
{"source_id":"core","type":"contract","title":"Human-in-the-Loop AI Contract","signals":["contract_lock","agent_pattern"],"tags":["human-in-loop","ai","contract","oversight"],"summary":"Contract for AI systems with human oversight - when to pause, how to present decisions, and approval workflows.","chunks":[{"chunk_id":"core:human-in-loop-contract:1","text":"## Human-in-the-Loop AI Contract\n\n### Pause Triggers\n- High-stakes decisions (money, data deletion)\n- Low confidence predictions (< 80%)\n- Novel situations not in training\n- User explicitly requests review\n- Policy-sensitive content detected\n\n### Presentation Format\n```markdown\n## Decision Point: [Action Name]\n\n**Proposed Action**: [What AI wants to do]\n**Confidence**: [X%]\n**Reasoning**: [Why this action]\n**Alternatives**: [Other options considered]\n\n[ ] Approve  [ ] Modify  [ ] Reject\n```\n\n### Approval Workflow\n1. AI pauses execution\n2. Human receives notification\n3. Human reviews with full context\n4. Human approves/modifies/rejects\n5. AI resumes with decision\n\n### Timeout Handling\n- Default timeout: 24 hours\n- Escalation after timeout\n- Safe default action if critical","start_line":1,"end_line":30}]}
{"source_id":"core","type":"plan","title":"Durable Execution Pattern Plan","signals":["state_machine","agent_pattern"],"tags":["durable","execution","persistence","plan"],"summary":"Plan for implementing durable execution - agents that persist through failures and resume from checkpoints.","chunks":[{"chunk_id":"core:durable-execution-plan:1","text":"## Durable Execution Pattern Plan\n\n### Phase 1: Checkpointing\n- [ ] Define checkpoint state schema\n- [ ] Implement state serialization\n- [ ] Choose persistence backend (SQLite/Postgres)\n- [ ] Create checkpoint save/load logic\n\n### Phase 2: Recovery\n- [ ] Detect incomplete executions on startup\n- [ ] Resume from last checkpoint\n- [ ] Handle partial step completion\n- [ ] Implement idempotent operations\n\n### Phase 3: Long-Running Support\n- [ ] Handle multi-day executions\n- [ ] Manage external state changes\n- [ ] Support version migrations\n- [ ] Implement graceful interruption\n\n### Key Pattern: Checkpoint-Resume\n```python\ncheckpointer = SqliteSaver.from_conn_string(\":memory:\")\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Resume from thread_id\nconfig = {\"configurable\": {\"thread_id\": \"session-123\"}}\nresult = graph.invoke(state, config)\n```","start_line":1,"end_line":28}]}
{"source_id":"core","type":"contract","title":"Agent Tracing & Observability Contract","signals":["contract_lock","evidence_audit"],"tags":["tracing","observability","langsmith","contract"],"summary":"Contract for AI agent observability covering tracing, metrics, and debugging visibility.","chunks":[{"chunk_id":"core:agent-observability-contract:1","text":"## Agent Tracing & Observability Contract\n\n### Required Traces\n- Every LLM call (input, output, latency)\n- Tool invocations (name, params, result)\n- State transitions (before, after)\n- Errors and retries\n\n### Trace Metadata\n- session_id: Unique execution session\n- user_id: Who initiated (if applicable)\n- cost: Token usage and estimated cost\n- duration_ms: Time for each step\n\n### Visualization Requirements\n- Execution flow diagram\n- State timeline view\n- Token usage breakdown\n- Error highlighting\n\n### Alerting Rules\n- Latency > 2x baseline\n- Error rate > 5%\n- Cost per request > threshold\n- Stuck execution (no progress 5min)\n\n### Tools Integration\n- LangSmith for LangGraph\n- CrewAI Control Plane for CrewAI\n- OpenTelemetry for custom","start_line":1,"end_line":30}]}
{"source_id":"core","type":"template","title":"Agent Role Prompt Template","signals":["context_engineering","agent_pattern"],"tags":["agent","role","prompt","template"],"summary":"Template for crafting effective agent role prompts with backstory, goals, and behavioral constraints.","chunks":[{"chunk_id":"core:agent-role-template:1","text":"## Agent Role Prompt Template\n\n### Structure\n```markdown\n# Role: {Role Title}\n\n## Identity\nYou are a {role_description}. Your expertise lies in {domain_expertise}.\n\n## Backstory\n{backstory_providing_context_and_motivation}\n\n## Goal\n{specific_measurable_goal}\n\n## Constraints\n- {constraint_1}\n- {constraint_2}\n- {constraint_3}\n\n## Output Format\n{expected_output_format}\n\n## Examples\n{few_shot_examples_if_needed}\n```\n\n### Best Practices\n- Be specific about expertise domain\n- Backstory adds personality and motivation\n- Goals should be measurable\n- Constraints prevent unwanted behaviors\n- Include output format expectations","start_line":1,"end_line":32}]}
{"source_id":"core","type":"plan","title":"Agent Tool Integration Plan","signals":["state_machine","agent_pattern"],"tags":["tools","integration","agent","plan"],"summary":"Plan for integrating external tools into AI agents - discovery, invocation, and error handling.","chunks":[{"chunk_id":"core:agent-tool-plan:1","text":"## Agent Tool Integration Plan\n\n### Phase 1: Tool Definition\n- [ ] Define tool interface (name, description, params)\n- [ ] Create JSON schema for parameters\n- [ ] Implement tool execution logic\n- [ ] Add input validation\n\n### Phase 2: Discovery\n- [ ] Register tools with agent\n- [ ] Generate tool descriptions for LLM\n- [ ] Handle dynamic tool loading\n- [ ] Support tool versioning\n\n### Phase 3: Invocation\n- [ ] Parse LLM tool calls\n- [ ] Validate parameters\n- [ ] Execute with timeout\n- [ ] Handle async tools\n\n### Phase 4: Error Handling\n- [ ] Catch tool exceptions\n- [ ] Format error for LLM\n- [ ] Implement retries\n- [ ] Log tool failures\n\n### Tool Schema Example\n```json\n{\n  \"name\": \"search_web\",\n  \"description\": \"Search the web for information\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"query\": {\"type\": \"string\"}\n    },\n    \"required\": [\"query\"]\n  }\n}\n```","start_line":1,"end_line":40}]}
{"source_id":"core","type":"contract","title":"Sequential vs Hierarchical Process Contract","signals":["contract_lock","agent_pattern"],"tags":["process","sequential","hierarchical","contract"],"summary":"Contract defining when to use sequential vs hierarchical multi-agent processes.","chunks":[{"chunk_id":"core:process-contract:1","text":"## Sequential vs Hierarchical Process Contract\n\n### Sequential Process\n**When to Use:**\n- Tasks have clear dependencies\n- Output of one feeds into next\n- Simple, linear workflows\n- Fewer than 5 agents\n\n**Pattern:**\n```\nAgent A → Agent B → Agent C → Output\n```\n\n### Hierarchical Process\n**When to Use:**\n- Complex tasks needing coordination\n- Dynamic task assignment\n- Parallel work possible\n- Many specialized agents\n\n**Pattern:**\n```\n        Manager\n       /   |   \\\nAgent A  Agent B  Agent C\n       \\   |   /\n        Output\n```\n\n### Selection Criteria\n| Factor | Sequential | Hierarchical |\n|--------|------------|-------------|\n| Complexity | Low | High |\n| Parallelism | No | Yes |\n| Coordination | Minimal | Required |\n| Agents | 2-4 | 5+ |","start_line":1,"end_line":34}]}
{"source_id":"core","type":"iterate","title":"Agent Prompt Iteration Playbook","signals":["iterate_loop","agent_pattern"],"tags":["prompt","iteration","agent","playbook"],"summary":"Playbook for iteratively improving agent prompts based on performance feedback.","chunks":[{"chunk_id":"core:prompt-iteration-playbook:1","text":"## Agent Prompt Iteration Playbook\n\n### Iteration Cycle\n1. **Observe**: Run agent on test cases\n2. **Analyze**: Identify failure patterns\n3. **Hypothesize**: Guess root cause\n4. **Modify**: Update prompt/tools\n5. **Validate**: Test on same cases\n6. **Regress**: Ensure no new failures\n\n### Common Issues & Fixes\n\n**Issue: Agent ignores instructions**\n→ Move critical instructions to end\n→ Use explicit \"IMPORTANT:\" markers\n→ Reduce overall prompt length\n\n**Issue: Wrong tool selection**\n→ Improve tool descriptions\n→ Add examples to prompt\n→ Reduce tool count\n\n**Issue: Hallucinated actions**\n→ Add verification steps\n→ Include \"if unsure\" instructions\n→ Require source citations\n\n### Metrics to Track\n- Task completion rate\n- Tool accuracy\n- Response quality score\n- Cost per successful task","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Agent Memory Architecture Contract","signals":["contract_lock","agent_pattern"],"tags":["memory","architecture","agent","contract"],"summary":"Contract for agent memory systems - working memory, conversation history, and persistent knowledge.","chunks":[{"chunk_id":"core:agent-memory-arch:1","text":"## Agent Memory Architecture Contract\n\n### Memory Layers\n\n**Layer 1: Working Memory (In-Context)**\n- Current task context\n- Recent tool outputs\n- Immediate scratchpad\n- Token limit: ~50% of context\n\n**Layer 2: Conversation History**\n- Recent messages (sliding window)\n- Summarized older messages\n- User preferences learned\n- Persistence: Session-based\n\n**Layer 3: Long-Term Memory**\n- Semantic search over facts\n- User profile information\n- Learned patterns/solutions\n- Persistence: Vector database\n\n### Memory Operations\n```python\nmemory.remember(key, value, metadata)\nmemory.recall(query, top_k=5)\nmemory.forget(key)  # Explicit deletion\nmemory.summarize(conversation)  # Compress\n```\n\n### Quality Attributes\n- Retrieval latency: < 100ms\n- Relevance: Top-3 contains answer 90%+\n- Freshness: Recent over stale","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"AGENTS.md Template","signals":["context_engineering"],"tags":["agents","codex","openai","template"],"summary":"Template for AGENTS.md files used by OpenAI Codex and similar autonomous coding agents.","chunks":[{"chunk_id":"core:agents-md-template:1","text":"## AGENTS.md Template\n\n### File: AGENTS.md\n```markdown\n# Agent Instructions\n\n## Repository Overview\nBrief description of what this codebase does.\n\n## Development Setup\n```bash\n# Installation\nnpm install  # or cargo build, pip install, etc.\n\n# Running tests\nnpm test\n\n# Building\nnpm run build\n```\n\n## Code Structure\n- `src/` - Main source code\n- `tests/` - Test files\n- `docs/` - Documentation\n\n## Coding Conventions\n- Use TypeScript strict mode\n- Follow existing patterns\n- Write tests for new code\n\n## Pull Request Guidelines\n- Clear title and description\n- Tests pass\n- No linting errors\n```\n\n### Purpose\n- Used by autonomous coding agents\n- Provides context for code generation\n- Defines acceptable changes","start_line":1,"end_line":40}]}
{"source_id":"core","type":"contract","title":"Event-Driven Agent Contract","signals":["contract_lock","agent_pattern"],"tags":["event-driven","reactive","agent","contract"],"summary":"Contract for event-driven agents that respond to triggers rather than running continuously.","chunks":[{"chunk_id":"core:event-driven-agent:1","text":"## Event-Driven Agent Contract\n\n### Trigger Events\n- **Webhook**: External service notification\n- **Schedule**: Cron-based execution\n- **File Change**: Watch for modifications\n- **Queue Message**: Async job processing\n- **User Action**: Direct invocation\n\n### Event Schema\n```json\n{\n  \"event_type\": \"file_changed\",\n  \"timestamp\": \"2025-01-21T10:00:00Z\",\n  \"source\": \"github\",\n  \"payload\": {\n    \"path\": \"src/main.rs\",\n    \"action\": \"modified\"\n  }\n}\n```\n\n### Processing Pattern\n1. Receive event\n2. Validate event schema\n3. Route to appropriate handler\n4. Execute agent logic\n5. Emit result event\n6. Acknowledge completion\n\n### Idempotency\n- Events may be delivered multiple times\n- Use event_id for deduplication\n- Make handlers idempotent","start_line":1,"end_line":32}]}
{"source_id":"core","type":"plan","title":"Agent Deployment Plan","signals":["release_install","agent_pattern"],"tags":["deployment","agent","production","plan"],"summary":"Plan for deploying AI agents to production with scaling, monitoring, and rollback strategies.","chunks":[{"chunk_id":"core:agent-deployment-plan:1","text":"## Agent Deployment Plan\n\n### Phase 1: Preparation\n- [ ] Lock agent configuration\n- [ ] Freeze prompt versions\n- [ ] Pin model versions\n- [ ] Document expected behavior\n\n### Phase 2: Staging\n- [ ] Deploy to staging environment\n- [ ] Run evaluation suite\n- [ ] Compare against baseline\n- [ ] Load test with synthetic traffic\n\n### Phase 3: Canary Release\n- [ ] Route 5% traffic to new version\n- [ ] Monitor key metrics\n- [ ] Compare with production baseline\n- [ ] Gradual increase if healthy\n\n### Phase 4: Full Deployment\n- [ ] Route 100% traffic\n- [ ] Monitor for 24 hours\n- [ ] Keep rollback ready\n- [ ] Update documentation\n\n### Rollback Triggers\n- Error rate > 2x baseline\n- Latency > 2x baseline\n- Cost per request > budget\n- User complaints spike","start_line":1,"end_line":32}]}
{"source_id":"core","type":"evidence","title":"Agent Evaluation Metrics","signals":["evidence_audit","agent_pattern"],"tags":["evaluation","metrics","agent","evidence"],"summary":"Metrics for evaluating AI agent performance covering accuracy, efficiency, and safety.","chunks":[{"chunk_id":"core:agent-eval-metrics:1","text":"## Agent Evaluation Metrics\n\n### Accuracy Metrics\n- **Task Completion Rate**: % tasks fully completed\n- **Correctness**: % outputs verified correct\n- **Tool Accuracy**: % correct tool selections\n- **Hallucination Rate**: % responses with false info\n\n### Efficiency Metrics\n- **Steps per Task**: Average actions to complete\n- **Token Usage**: Tokens per successful task\n- **Latency**: Time from start to completion\n- **Cost**: $ per task completion\n\n### Safety Metrics\n- **Refusal Rate**: % correctly refused unsafe requests\n- **Leak Rate**: % outputs with sensitive data\n- **Harm Rate**: % outputs with harmful content\n- **Error Recovery**: % errors handled gracefully\n\n### Quality Metrics\n- **User Satisfaction**: Survey scores\n- **Clarity**: Readability of outputs\n- **Helpfulness**: Did it solve the problem?\n- **Consistency**: Same input → same output","start_line":1,"end_line":26}]}
{"source_id":"core","type":"contract","title":"Agent Cost Control Contract","signals":["contract_lock","agent_pattern"],"tags":["cost","budget","agent","contract"],"summary":"Contract for controlling AI agent costs including token budgets, rate limits, and spending caps.","chunks":[{"chunk_id":"core:agent-cost-contract:1","text":"## Agent Cost Control Contract\n\n### Budget Tiers\n| Tier | Per Request | Per Hour | Per Day |\n|------|-------------|----------|--------|\n| Free | $0.01 | $0.10 | $1.00 |\n| Pro | $0.10 | $1.00 | $10.00 |\n| Enterprise | $1.00 | $10.00 | $100.00 |\n\n### Token Budgets\n- Input tokens: 10,000 max per request\n- Output tokens: 4,000 max per request\n- Context window: 80% max utilization\n\n### Cost Optimization Strategies\n- Cache common queries\n- Use cheaper models for simple tasks\n- Compress context before sending\n- Batch similar requests\n\n### Enforcement\n- Pre-request cost estimation\n- Reject if over budget\n- Alert at 80% of budget\n- Daily cost reports\n\n### Overage Handling\n- Graceful degradation\n- Queue non-urgent requests\n- Notify user of limits","start_line":1,"end_line":30}]}
{"source_id":"core","type":"iterate","title":"Agent Debugging Checklist","signals":["iterate_loop","agent_pattern"],"tags":["debugging","checklist","agent"],"summary":"Checklist for debugging AI agent issues covering prompts, tools, and execution flow.","chunks":[{"chunk_id":"core:agent-debug-checklist:1","text":"## Agent Debugging Checklist\n\n### Prompt Issues\n- [ ] Is the system prompt too long?\n- [ ] Are instructions clear and specific?\n- [ ] Are there conflicting instructions?\n- [ ] Is the output format specified?\n\n### Tool Issues\n- [ ] Are tool descriptions accurate?\n- [ ] Do parameter schemas match implementation?\n- [ ] Are required params marked correctly?\n- [ ] Is the tool returning expected format?\n\n### State Issues\n- [ ] Is state being persisted correctly?\n- [ ] Are checkpoints being saved?\n- [ ] Is memory retrieval working?\n- [ ] Are there stale state issues?\n\n### Execution Issues\n- [ ] Is the model receiving full context?\n- [ ] Are API calls succeeding?\n- [ ] Is there a timeout occurring?\n- [ ] Are retries happening correctly?\n\n### Quick Fixes\n- Add \"IMPORTANT:\" before critical instructions\n- Reduce tool count to essentials\n- Split complex tasks into subtasks\n- Add explicit examples to prompt","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Terminal AI Agent Contract","signals":["contract_lock","terminal_agent"],"tags":["aider","terminal","agent","contract"],"summary":"Contract for terminal-based AI coding agents like Aider - repo mapping, auto-commit, watch mode.","chunks":[{"chunk_id":"core:terminal-ai-agent:1","text":"## Terminal AI Agent Contract\n\n### Core Capabilities\n\n**Repo Mapping**\n- Generate repository structure map\n- Identify relevant files for context\n- Support 100+ programming languages\n- Use tree-sitter for code parsing\n\n**Auto-Commit**\n- Automatically commit changes after edits\n- Generate meaningful commit messages\n- Integrate with existing git workflow\n- Support undo via git reset\n\n**Watch Mode**\n- Monitor files for changes\n- Parse comments for AI instructions\n- Apply edits on file save\n- IDE-agnostic operation\n\n**Voice Input**\n- Transcribe voice to text commands\n- Request features by speaking\n- Hands-free coding support\n\n### Quality Attributes\n- Latency: < 5s for simple edits\n- Accuracy: Syntactically correct 95%+\n- Undo: All changes reversible\n- Integration: Works with any IDE","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"RepoMap Generation Template","signals":["context_engineering","terminal_agent"],"tags":["repomap","context","template"],"summary":"Template for generating repository maps to help AI understand codebase structure.","chunks":[{"chunk_id":"core:repomap-template:1","text":"## RepoMap Generation Template\n\n### Structure\n```\n# Repository Structure\n\n## Entry Points\n- main.rs - Application entry point\n- lib.rs - Library exports\n\n## Core Modules\n/src/\n  cli/     - Command line interface\n  brain/   - Knowledge management\n  state/   - State persistence\n\n## Key Files\n- Cargo.toml - Dependencies and metadata\n- README.md - Project documentation\n\n## Dependencies (Key)\n- tokio - Async runtime\n- serde - Serialization\n- clap - CLI parsing\n```\n\n### Generation Rules\n1. Include directory tree (depth 2-3)\n2. Annotate purpose of each directory\n3. Highlight entry points\n4. Note key dependencies\n5. Exclude build/vendor dirs\n6. Size-sort by importance\n\n### Context Selection\n- Include files matching task domain\n- Prioritize recently modified\n- Respect token budget\n- Allow manual file addition","start_line":1,"end_line":38}]}
{"source_id":"core","type":"contract","title":"Auto-Commit Integration Contract","signals":["contract_lock","git_integration"],"tags":["git","auto-commit","contract"],"summary":"Contract for AI agents that automatically commit changes to version control.","chunks":[{"chunk_id":"core:auto-commit-contract:1","text":"## Auto-Commit Integration Contract\n\n### Commit Workflow\n1. Agent makes code changes\n2. Stage modified files\n3. Generate commit message from changes\n4. Create commit with attribution\n5. Allow review before push\n\n### Commit Message Format\n```\n<type>(<scope>): <description>\n\n<body explaining what changed>\n\n🤖 Generated by AI Agent\nModel: claude-3.5-sonnet\nPrompt: \"<user request summary>\"\n```\n\n### Types\n- feat: New feature\n- fix: Bug fix\n- refactor: Code restructure\n- docs: Documentation\n- test: Add/modify tests\n\n### Safety Rules\n- Never auto-push to remote\n- Never commit sensitive data\n- Always allow undo (git reset)\n- Preserve user's branch state\n- Atomic commits (one change = one commit)","start_line":1,"end_line":32}]}
{"source_id":"core","type":"plan","title":"Watch Mode Implementation Plan","signals":["release_install","terminal_agent"],"tags":["watch","file-monitor","plan"],"summary":"Plan for implementing watch mode in terminal AI agents - file monitoring and comment-based instructions.","chunks":[{"chunk_id":"core:watch-mode-plan:1","text":"## Watch Mode Implementation Plan\n\n### Phase 1: File Monitoring\n- [ ] Set up file watcher (notify, fswatch)\n- [ ] Filter for relevant file types\n- [ ] Debounce rapid changes\n- [ ] Handle file creation/deletion\n\n### Phase 2: Comment Parsing\n- [ ] Detect AI instruction comments\n  - `// AI: <instruction>`\n  - `# TODO(AI): <instruction>`\n  - `/* @ai <instruction> */`\n- [ ] Extract instruction context\n- [ ] Parse multi-line instructions\n\n### Phase 3: Instruction Execution\n- [ ] Queue instructions\n- [ ] Process in order\n- [ ] Apply edits to files\n- [ ] Remove instruction comments\n- [ ] Show diff before applying\n\n### Phase 4: Integration\n- [ ] IDE extension hooks\n- [ ] Terminal output formatting\n- [ ] Status indicators\n- [ ] Error recovery","start_line":1,"end_line":30}]}
{"source_id":"core","type":"contract","title":"Lint and Test Integration Contract","signals":["contract_lock","quality_assurance"],"tags":["linting","testing","integration","contract"],"summary":"Contract for AI agents that run linters and tests automatically after making changes.","chunks":[{"chunk_id":"core:lint-test-contract:1","text":"## Lint and Test Integration Contract\n\n### Workflow\n1. Agent makes code changes\n2. Run configured linters\n3. If lint errors: attempt auto-fix\n4. Run relevant tests\n5. If test failures: show error, offer fix\n6. Only commit if all pass\n\n### Linter Integration\n```yaml\nlinters:\n  - command: \"cargo clippy\"\n    fix: \"cargo clippy --fix\"\n  - command: \"eslint .\"\n    fix: \"eslint --fix .\"\n  - command: \"ruff check .\"\n    fix: \"ruff check --fix .\"\n```\n\n### Test Selection\n- Run tests in changed files\n- Run tests importing changed modules\n- Run full suite on critical paths\n- Support test filtering by name\n\n### Failure Handling\n- Parse error messages\n- Show relevant code context\n- Offer AI-powered fix\n- Allow manual intervention\n- Track fix success rate","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Continuous AI Agent Contract","signals":["contract_lock","continuous_ai"],"tags":["continue","cloud-agent","headless","contract"],"summary":"Contract for continuous AI agents - cloud agents, CLI agents, IDE agents with headless and TUI modes.","chunks":[{"chunk_id":"core:continuous-ai-contract:1","text":"## Continuous AI Agent Contract\n\n### Agent Modes\n\n**Cloud Agents (Headless)**\n- Run on PR opens, schedules, or triggers\n- Async background processing\n- No human-in-the-loop\n- Event-driven execution\n\n**CLI Agents (TUI)**\n- Interactive terminal UI\n- Step-by-step approval\n- Real-time execution view\n- Human-in-the-loop\n\n**IDE Agents**\n- Trigger from VS Code/JetBrains\n- Background refactoring\n- Keep coding while agent works\n- Inline result display\n\n### Workflow Triggers\n- PR opened/updated\n- Cron schedule\n- Manual invocation\n- File change events\n- Webhook calls\n\n### Quality Attributes\n- Observability: Full execution logs\n- Reproducibility: Same input → same output\n- Safety: Human approval for critical actions","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"Cloud Agent Workflow Template","signals":["context_engineering","continuous_ai"],"tags":["workflow","cloud","automation","template"],"summary":"Template for defining cloud agent workflows triggered by events like PR opens or schedules.","chunks":[{"chunk_id":"core:cloud-agent-workflow:1","text":"## Cloud Agent Workflow Template\n\n### Workflow Definition\n```yaml\nname: pr-review-agent\ntrigger:\n  event: pull_request\n  actions: [opened, synchronize]\n\nsteps:\n  - name: analyze-changes\n    agent: code-reviewer\n    input: ${{ github.event.pull_request }}\n    \n  - name: check-security\n    agent: security-scanner\n    input: ${{ steps.analyze-changes.output }}\n    \n  - name: post-review\n    agent: reviewer\n    action: comment\n    input: ${{ steps.check-security.output }}\n```\n\n### Event Types\n- `pull_request`: PR lifecycle events\n- `schedule`: Cron-based triggers\n- `webhook`: Custom HTTP triggers\n- `file_change`: Repo file modifications\n\n### Approval Gates\n```yaml\nsteps:\n  - name: deploy\n    requires_approval: true\n    approvers: [\"@devops-team\"]\n```","start_line":1,"end_line":36}]}
{"source_id":"core","type":"plan","title":"TUI Agent Implementation Plan","signals":["release_install","continuous_ai"],"tags":["tui","terminal-ui","plan"],"summary":"Plan for implementing terminal UI for AI agents with step-by-step approval and real-time updates.","chunks":[{"chunk_id":"core:tui-agent-plan:1","text":"## TUI Agent Implementation Plan\n\n### Phase 1: Core TUI\n- [ ] Choose TUI framework (Bubble Tea, ratatui)\n- [ ] Design layout (panels, status bar)\n- [ ] Implement keyboard navigation\n- [ ] Add color/styling support\n\n### Phase 2: Execution View\n- [ ] Show agent thinking process\n- [ ] Display tool calls in real-time\n- [ ] Progress indicators\n- [ ] Expandable/collapsible sections\n\n### Phase 3: Approval Flow\n- [ ] Pause before critical actions\n- [ ] Show diff preview\n- [ ] Accept/Reject/Edit controls\n- [ ] Undo support\n\n### Phase 4: Session Management\n- [ ] Save/restore sessions\n- [ ] Session history view\n- [ ] Export conversations\n- [ ] Multi-agent tabs\n\n### Key Components\n- Input panel: User prompts\n- Output panel: Agent responses\n- Status bar: Model, tokens, cost\n- Action panel: Tool executions","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"Headless Agent Execution Contract","signals":["contract_lock","continuous_ai"],"tags":["headless","automation","contract"],"summary":"Contract for headless agent execution - no human interaction, fully automated workflows.","chunks":[{"chunk_id":"core:headless-agent-contract:1","text":"## Headless Agent Execution Contract\n\n### Characteristics\n- No user input during execution\n- All parameters pre-configured\n- Results stored for later review\n- Runs in CI/CD or cloud\n\n### Configuration\n```yaml\nagent:\n  name: code-formatter\n  mode: headless\n  timeout: 300  # seconds\n  max_iterations: 50\n  \ninputs:\n  files: ${{ github.files }}\n  rules: .prettierrc\n  \noutputs:\n  - type: commit\n    branch: auto/format-${{ run_id }}\n  - type: pr\n    title: \"Auto-format code\"\n```\n\n### Safety Constraints\n- Budget limits (tokens, cost)\n- Iteration limits\n- Scope restrictions (files, actions)\n- Timeout enforcement\n\n### Observability\n- Execution logs stored\n- Token usage tracked\n- Cost reported\n- Errors captured with context","start_line":1,"end_line":36}]}
{"source_id":"core","type":"contract","title":"Local LLM Integration Contract","signals":["contract_lock","local_ai"],"tags":["ollama","llama.cpp","local","contract"],"summary":"Contract for integrating local LLMs via Ollama, llama.cpp, or similar local inference engines.","chunks":[{"chunk_id":"core:local-llm-contract:1","text":"## Local LLM Integration Contract\n\n### Supported Backends\n- **Ollama**: REST API, model library\n- **llama.cpp**: High-perf C++ inference\n- **vLLM**: Python inference server\n- **LM Studio**: Desktop GUI + API\n\n### Endpoint Configuration\n```yaml\nlocal_provider:\n  type: ollama\n  base_url: $LOCAL_ENDPOINT  # e.g., localhost:11434\n  model: llama3.2:latest\n  context_window: 8192\n  \n# Or llama.cpp server\nlocal_provider:\n  type: llama_cpp\n  base_url: $LOCAL_ENDPOINT  # e.g., localhost:8080\n  api_format: openai_compatible\n```\n\n### Quality Attributes\n- Latency: < 50ms first token\n- Privacy: Data never leaves machine\n- Cost: $0 per token (compute only)\n- Offline: Works without internet\n\n### Model Formats\n- GGUF: Quantized models\n- SafeTensors: Safe format\n- ONNX: Cross-platform","start_line":1,"end_line":34}]}
{"source_id":"core","type":"template","title":"Ollama Integration Template","signals":["context_engineering","local_ai"],"tags":["ollama","api","template"],"summary":"Template for integrating Ollama local models with REST API patterns.","chunks":[{"chunk_id":"core:ollama-template:1","text":"## Ollama Integration Template\n\n### REST API Endpoints\n```bash\n# Generate completion\ncurl $OLLAMA_HOST/api/generate \\\n  -d '{\"model\": \"llama3.2\", \"prompt\": \"Hello\"}'\n\n# Chat completion (OpenAI compatible)\ncurl $OLLAMA_HOST/v1/chat/completions \\\n  -d '{\"model\": \"llama3.2\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'\n\n# List models\ncurl $OLLAMA_HOST/api/tags\n\n# Pull model\ncurl $OLLAMA_HOST/api/pull \\\n  -d '{\"name\": \"llama3.2\"}'\n```\n\n### Modelfile Customization\n```dockerfile\nFROM llama3.2\n\nPARAMETER temperature 0.7\nPARAMETER num_ctx 8192\n\nSYSTEM \"You are a helpful coding assistant.\"\n```\n\n### Features\n- Streaming responses\n- Model library (100+ models)\n- Custom system prompts\n- Parameter tuning\n- GGUF import","start_line":1,"end_line":36}]}
{"source_id":"core","type":"template","title":"llama.cpp Server Template","signals":["context_engineering","local_ai"],"tags":["llama.cpp","server","template"],"summary":"Template for running llama.cpp as an OpenAI-compatible server for local inference.","chunks":[{"chunk_id":"core:llama-cpp-server:1","text":"## llama.cpp Server Template\n\n### Start Server\n```bash\n# Basic server\n./llama-server -m model.gguf --port 8080\n\n# With GPU acceleration\n./llama-server -m model.gguf -ngl 35 --port 8080\n\n# Multi-model from HuggingFace\n./llama-server \\\n  --model-url $HF_MODEL_URL \\\n  --alias my-model \\\n  --port 8080\n```\n\n### OpenAI-Compatible Endpoints\n```bash\n# Chat completions\ncurl $LLAMA_SERVER/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"my-model\", \"messages\": [...]}'\n\n# Embeddings\ncurl $LLAMA_SERVER/v1/embeddings \\\n  -d '{\"model\": \"my-model\", \"input\": \"text\"}'\n```\n\n### Backend Options\n- Metal: Apple Silicon GPU\n- CUDA: NVIDIA GPU\n- Vulkan: Cross-platform GPU\n- CPU: AVX/AVX2 optimized\n\n### Quantization Levels\n- Q4_K_M: Good balance\n- Q5_K_M: Better quality\n- Q8_0: Near-original","start_line":1,"end_line":40}]}
{"source_id":"core","type":"plan","title":"GGUF Model Integration Plan","signals":["release_install","local_ai"],"tags":["gguf","quantization","plan"],"summary":"Plan for integrating GGUF quantized models into AI applications.","chunks":[{"chunk_id":"core:gguf-model-plan:1","text":"## GGUF Model Integration Plan\n\n### Phase 1: Model Selection\n- [ ] Choose base model (Llama, Mistral, Qwen)\n- [ ] Select quantization level (Q4, Q5, Q8)\n- [ ] Verify VRAM requirements\n- [ ] Test inference speed\n\n### Phase 2: Download & Setup\n- [ ] Download from HuggingFace\n- [ ] Verify file integrity\n- [ ] Configure model path\n- [ ] Set up context window size\n\n### Phase 3: Integration\n- [ ] Add to model config\n- [ ] Test via API endpoint\n- [ ] Benchmark throughput\n- [ ] Compare with cloud models\n\n### Quantization Tradeoffs\n| Quant | Size | Quality | Speed |\n|-------|------|---------|-------|\n| Q4_K_M | 4GB | Good | Fast |\n| Q5_K_M | 5GB | Better | Medium |\n| Q8_0 | 8GB | Best | Slow |\n\n### Hardware Requirements\n- Q4: 8GB RAM minimum\n- Q5: 12GB RAM minimum\n- Q8: 16GB RAM minimum\n- GPU: 6GB+ VRAM for offload","start_line":1,"end_line":34}]}
{"source_id":"core","type":"contract","title":"Context Auto-Compact Contract","signals":["contract_lock","context_management"],"tags":["auto-compact","summarization","contract"],"summary":"Contract for automatically compacting/summarizing context when approaching token limits.","chunks":[{"chunk_id":"core:auto-compact-contract:1","text":"## Context Auto-Compact Contract\n\n### Trigger Conditions\n- Context reaches 95% of token limit\n- User requests summarization\n- Session exceeds threshold\n\n### Compaction Process\n1. Identify conversation segments\n2. Score by relevance to current task\n3. Summarize low-priority segments\n4. Preserve critical context\n5. Replace full text with summary\n\n### Preservation Rules\n**Always Keep:**\n- System prompt\n- Last N user messages (configurable)\n- Active file contents\n- Pending tool results\n\n**Can Summarize:**\n- Older conversation turns\n- Completed tool outputs\n- Resolved discussions\n\n### Summary Format\n```\n[COMPACT] Previous context (turns 1-15):\n- User requested feature X\n- Modified files: a.rs, b.rs\n- Key decisions: used pattern Y\n```\n\n### Quality Attributes\n- Preserve intent, not verbatim text\n- Keep actionable information\n- Maintain continuity","start_line":1,"end_line":36}]}
{"source_id":"core","type":"contract","title":"Multi-Provider Configuration Contract","signals":["contract_lock","provider_abstraction"],"tags":["multi-provider","config","contract"],"summary":"Contract for configuring multiple LLM providers with fallback and routing logic.","chunks":[{"chunk_id":"core:multi-provider-contract:1","text":"## Multi-Provider Configuration Contract\n\n### Provider Configuration\n```yaml\nproviders:\n  anthropic:\n    api_key: $ANTHROPIC_API_KEY\n    models: [claude-3-sonnet, claude-3-opus]\n    \n  openai:\n    api_key: $OPENAI_API_KEY\n    models: [gpt-4o, gpt-4-turbo]\n    \n  local:\n    type: ollama\n    base_url: $LOCAL_ENDPOINT\n    models: [llama3.2, codellama]\n```\n\n### Routing Rules\n- Default: anthropic/claude-3-sonnet\n- Fallback: openai/gpt-4o\n- Simple tasks: local/llama3.2\n- Cost threshold: switch to cheaper\n\n### Provider Selection Logic\n1. Check rate limits\n2. Estimate cost\n3. Select by routing rules\n4. Attempt request\n5. On failure: try fallback\n\n### Quality Attributes\n- Seamless failover\n- Cost optimization\n- Latency awareness\n- Provider-agnostic interface","start_line":1,"end_line":36}]}
{"source_id":"core","type":"template","title":"MCP Server Integration Template","signals":["context_engineering","mcp"],"tags":["mcp","server","integration","template"],"summary":"Template for integrating MCP (Model Context Protocol) servers for extended agent capabilities.","chunks":[{"chunk_id":"core:mcp-server-template:1","text":"## MCP Server Integration Template\n\n### Server Configuration\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],\n      \"env\": {\"ALLOWED_PATHS\": \"/workspace\"}\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\"GITHUB_TOKEN\": \"$GH_TOKEN\"}\n    },\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\n### Capabilities\n- **Resources**: Read context from external sources\n- **Tools**: Execute actions via MCP tools\n- **Prompts**: Pre-defined prompt templates\n- **Sampling**: Request LLM completions\n\n### Tool Discovery\n1. Connect to MCP server\n2. List available tools\n3. Generate tool schemas for LLM\n4. Route tool calls through MCP\n5. Return results to agent","start_line":1,"end_line":38}]}
{"source_id":"core","type":"contract","title":"LSP Integration Contract","signals":["contract_lock","code_intelligence"],"tags":["lsp","language-server","contract"],"summary":"Contract for integrating Language Server Protocol for code intelligence in AI coding agents.","chunks":[{"chunk_id":"core:lsp-integration-contract:1","text":"## LSP Integration Contract\n\n### Capabilities\n- **Completions**: Code suggestions\n- **Diagnostics**: Errors and warnings\n- **Hover**: Type information\n- **Go to Definition**: Symbol navigation\n- **Find References**: Usage search\n- **Rename**: Refactoring support\n\n### Integration Flow\n1. Start language server for project\n2. Register file watchers\n3. Sync document state\n4. Query on demand\n5. Include in AI context\n\n### Context Enhancement\n```\n## LSP Context\nDiagnostics for src/main.rs:\n- Line 42: unused variable `x`\n- Line 58: type mismatch\n\nSymbol at cursor:\n- Name: process_data\n- Type: fn(Vec<u8>) -> Result<Data>\n- References: 5 locations\n```\n\n### Quality Attributes\n- Real-time diagnostics\n- Accurate type info\n- Language-agnostic\n- IDE-independent","start_line":1,"end_line":34}]}
{"source_id":"core","type":"plan","title":"Session Management Plan","signals":["release_install","session"],"tags":["session","persistence","plan"],"summary":"Plan for implementing session management - save, restore, and resume AI agent conversations.","chunks":[{"chunk_id":"core:session-mgmt-plan:1","text":"## Session Management Plan\n\n### Phase 1: Session Storage\n- [ ] Define session schema\n- [ ] Implement save on exit\n- [ ] Implement resume on start\n- [ ] Add session listing\n\n### Phase 2: Session Data\n```json\n{\n  \"session_id\": \"abc123\",\n  \"created\": \"2025-01-21T10:00:00Z\",\n  \"model\": \"claude-3-sonnet\",\n  \"messages\": [...],\n  \"files_in_context\": [\"src/main.rs\"],\n  \"tool_states\": {},\n  \"checkpoints\": [...]\n}\n```\n\n### Phase 3: Session Operations\n- [ ] Fork session (branch)\n- [ ] Merge sessions\n- [ ] Export to markdown\n- [ ] Share session link\n\n### Phase 4: Auto-Save\n- [ ] Save every N messages\n- [ ] Save on tool execution\n- [ ] Save before compact\n- [ ] Crash recovery","start_line":1,"end_line":32}]}
{"source_id":"core","type":"contract","title":"IDE Plugin Architecture Contract","signals":["contract_lock","ide_extension"],"tags":["plugin","extension","ide","contract"],"summary":"Contract for AI coding assistant plugins/extensions for VS Code, JetBrains, and other IDEs.","chunks":[{"chunk_id":"core:ide-plugin-contract:1","text":"## IDE Plugin Architecture Contract\n\n### Plugin Structure\n```\n.claude/\n  commands/\n    fix-bug.md    # Custom slash commands\n    review.md\n  plugins/\n    linter/       # Plugin directory\n      manifest.json\n      index.js\n  settings.json   # Local config\n```\n\n### Extension Points\n- **Commands**: Custom slash commands\n- **Context Providers**: Add context sources\n- **Actions**: Pre/post edit hooks\n- **Formatters**: Output formatting\n\n### Command Definition\n```markdown\n---\nname: fix-bug\ndescription: Fix a bug in the code\nargs:\n  - name: issue\n    description: Bug description\n---\n\nFix the following bug: $issue\nFirst understand the root cause, then implement fix.\n```\n\n### Quality Attributes\n- Isolated execution\n- Version compatibility\n- Hot reload support","start_line":1,"end_line":38}]}
{"source_id":"core","type":"template","title":"Custom Slash Command Template","signals":["context_engineering","ide_extension"],"tags":["slash-command","custom","template"],"summary":"Template for creating custom slash commands in AI coding assistants.","chunks":[{"chunk_id":"core:slash-command-template:1","text":"## Custom Slash Command Template\n\n### File: .claude/commands/review-pr.md\n```markdown\n---\nname: review-pr\ndescription: Review a pull request thoroughly\nargs:\n  - name: pr_number\n    required: true\n    description: PR number to review\n---\n\n# PR Review Command\n\nReview PR #$pr_number with the following criteria:\n\n## Code Quality\n- Check for code smells\n- Verify naming conventions\n- Look for duplication\n\n## Security\n- Check for vulnerabilities\n- Verify input validation\n- Review auth/authz\n\n## Testing\n- Are tests adequate?\n- Do tests pass?\n- Edge cases covered?\n\n## Output\nProvide structured review with:\n- Summary\n- Issues found\n- Suggestions\n- Approval recommendation\n```\n\n### Usage\n```\n/review-pr 123\n```","start_line":1,"end_line":42}]}
{"source_id":"core","type":"plan","title":"VS Code Extension Development Plan","signals":["release_install","ide_extension"],"tags":["vscode","extension","plan"],"summary":"Plan for developing VS Code extensions that integrate AI coding capabilities.","chunks":[{"chunk_id":"core:vscode-ext-plan:1","text":"## VS Code Extension Development Plan\n\n### Phase 1: Core Extension\n- [ ] Generate extension scaffold\n- [ ] Register commands\n- [ ] Implement activation events\n- [ ] Add configuration schema\n\n### Phase 2: AI Integration\n- [ ] Connect to LLM provider\n- [ ] Implement inline suggestions\n- [ ] Add chat panel\n- [ ] Context extraction from editor\n\n### Phase 3: Features\n- [ ] Code actions (quick fixes)\n- [ ] Hover information\n- [ ] Code lens (inline actions)\n- [ ] Diagnostics integration\n\n### Phase 4: UX Polish\n- [ ] Progress indicators\n- [ ] Diff preview\n- [ ] Accept/reject controls\n- [ ] Keyboard shortcuts\n\n### Extension Manifest\n```json\n{\n  \"activationEvents\": [\"onLanguage:*\"],\n  \"contributes\": {\n    \"commands\": [{\"command\": \"ai.explain\"}],\n    \"configuration\": {\"properties\": {}}\n  }\n}\n```","start_line":1,"end_line":36}]}
{"source_id":"core","type":"contract","title":"Non-Interactive Mode Contract","signals":["contract_lock","automation"],"tags":["non-interactive","scripting","contract"],"summary":"Contract for running AI coding agents in non-interactive/scripting mode for automation.","chunks":[{"chunk_id":"core:non-interactive-contract:1","text":"## Non-Interactive Mode Contract\n\n### Invocation\n```bash\n# Single prompt mode\naider --message \"Fix the bug in main.rs\" --yes\n\n# From file\naider --message-file prompt.txt --yes\n\n# Pipe input\necho \"Add tests\" | aider --yes\n```\n\n### Flags\n- `--yes`: Auto-accept all changes\n- `--no-git`: Skip git operations\n- `--no-stream`: Return complete response\n- `--exit`: Exit after completion\n\n### Use Cases\n- CI/CD pipelines\n- Batch processing\n- Automated refactoring\n- Scripted code generation\n\n### Safety Considerations\n- Review changes post-execution\n- Use version control\n- Set token/cost limits\n- Log all operations\n- Run in sandboxed environment","start_line":1,"end_line":32}]}
{"source_id":"core","type":"template","title":"Workflow Automation Template","signals":["context_engineering","automation"],"tags":["workflow","automation","template"],"summary":"Template for automating multi-step coding workflows with AI agents.","chunks":[{"chunk_id":"core:workflow-automation-template:1","text":"## Workflow Automation Template\n\n### Workflow Definition\n```yaml\nname: feature-implementation\ndescription: Implement a feature from issue\n\ntrigger:\n  event: issue_labeled\n  label: ai-implement\n\nsteps:\n  - id: analyze\n    action: read_issue\n    output: requirements\n    \n  - id: plan\n    action: ai_prompt\n    prompt: \"Create implementation plan for: $requirements\"\n    output: plan\n    \n  - id: implement\n    action: ai_code\n    prompt: \"Implement according to: $plan\"\n    files: [\"src/**/*.rs\"]\n    output: changes\n    \n  - id: test\n    action: run_tests\n    on_failure: goto(implement)\n    \n  - id: pr\n    action: create_pr\n    title: \"feat: $issue.title\"\n    body: \"Implements #$issue.number\"\n```\n\n### Workflow Engine\n- Sequential step execution\n- Variable interpolation\n- Conditional branching\n- Retry on failure\n- Human approval gates","start_line":1,"end_line":44}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Partnership Principles", "signals": ["state_machine"], "tags": ["vibecode", "partnership", "workflow"], "summary": "Partnership model for AI-assisted development with shared ownership, vision-first discovery, and iterative delivery.", "chunks": [{"chunk_id": "core:vibecode-principles:1", "text": "## Vibecode Partnership Principles\n\nThe Vibecode model is built on a high-trust, high-agency partnership between the Human (Homeowner) and the AI (Architect). This is not a master-slave relationship, but a collaborative design-build process.\n\n### Core Pillars\n1. **Shared Ownership**: The AI is responsible for the technical integrity and consistency of the codebase, while the Human is responsible for the vision, business logic, and final approval.\n2. **Vision-First Discovery**: Never write code without a shared understanding of the \"Why\". Use the `intake` and `blueprint` phases to align on goals before the first line of code is written.\n3. **Iterative Delivery**: Break complex features into small, atomic, and reviewable chunks. Each chunk must be functional and tested.\n4. **Transparent Reasoning**: The AI must explain *why* a specific pattern or library was chosen, highlighting tradeoffs (e.g., \"Using X for speed, but Y would be more scalable\").\n\n### Actionable Guidance\n- **Human**: Provide high-context prompts. If the AI is drifting, reset the context with a fresh `blueprint`.\n- **AI**: Propose multiple options for complex decisions. Always include a \"minimal viable\" vs \"production-grade\" comparison.\n- **Verification**: Every build iteration must end with a `review` step where evidence (logs, diffs) is audited.\n\n### Anti-Patterns to Avoid\n- **The \"Black Box\"**: AI writing 500 lines of code without explanation.\n- **The \"Yes Man\"**: AI blindly following a flawed human suggestion without warning about technical debt or security risks.\n- **Context Starvation**: Starting a task without reading the relevant project profile or existing contracts.\n\n### Success Criteria\n- [ ] Both partners agree on the `contract` before building.\n- [ ] Every change is backed by a git commit and an audit log entry.\n- [ ] The codebase remains clean, documented, and testable at every step.", "start_line": 1, "end_line": 24}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode 6-Step Workflow", "signals": ["state_machine"], "tags": ["vibecode", "workflow", "plan"], "summary": "Six-step delivery flow from vision to documentation with clear role ownership and state-machine enforcement.", "chunks": [{"chunk_id": "core:vibecode-workflow:1", "text": "## Vibecode 6-Step Workflow\n\nA disciplined, state-machine driven approach to AI-assisted development. Each step must be completed and verified before moving to the next.\n\n### 1. Vision (Intake)\nDefine the \"What\" and \"Why\". Capture the core problem, target audience, and high-level goals.\n- **Tool**: `vibeanvil intake -m \"...\"`\n- **Outcome**: A clear `intake.md` file.\n\n### 2. Context (Discovery)\nGather technical constraints, existing stack, and project scope.\n- **Action**: AI scans the repo using `brain search` and `project profile`.\n- **Outcome**: Identification of dependencies and integration points.\n\n### 3. Blueprint (Architecture)\nPropose the technical solution. Define components, data flow, and API signatures.\n- **Tool**: `vibeanvil blueprint --auto`\n- **Outcome**: A structured `blueprint.md` with a clear implementation path.\n\n### 4. Contract (Agreement)\nLock the requirements and acceptance criteria. This is the \"License to Build\".\n- **Tool**: `vibeanvil contract create && vibeanvil contract lock`\n- **Outcome**: A SHA-256 hashed `contract.lock` file.\n\n### 5. Build (Implementation)\nExecute the plan in iterative loops. Use `build iterate` for automated test-fix cycles.\n- **Tool**: `vibeanvil build iterate --provider [name]`\n- **Outcome**: Functional code with passing tests and captured evidence.\n\n### 6. Refine (Ship)\nFinal polish: security audit, performance review, and documentation.\n- **Tool**: `vibeanvil review pass && vibeanvil ship`\n- **Outcome**: A production-ready feature with a full audit trail.\n\n### Tradeoffs\n- **Strictness**: High overhead for small tasks, but essential for complex features to prevent \"AI drift\".\n- **Speed**: Slower start, but significantly faster and safer delivery with fewer regressions.", "start_line": 1, "end_line": 35}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Master Prompt Skeleton", "signals": ["state_machine"], "tags": ["vibecode", "prompt", "roles"], "summary": "Foundational prompt structure for high-alignment AI coding, enforcing role-playing and a structured response protocol.", "chunks": [{"chunk_id": "core:vibecode-master:1", "text": "## Vibecode Master Prompt Skeleton\n\nThe foundational prompt structure for high-alignment AI coding. It enforces role-playing and a structured response protocol.\n\n### Role Definitions\n- **The Architect (AI)**: Lead technical designer. Proactive, opinionated about best practices, and focused on long-term maintainability.\n- **The Homeowner (Human)**: Product owner. Provides the \"vibe\", sets the budget (scope), and makes final aesthetic and functional calls.\n\n### Response Protocol (The \"Vibe Check\")\n1. **Acknowledge & Classify**: Identify the project type (CLI, Web, API, etc.) and current state.\n2. **Propose Vision**: Describe the end state in plain English. \"I see a minimalist Rust CLI using `clap` for parsing and `tokio` for async...\"\n3. **Clarify Constraints**: Ask 3-5 high-impact questions. \"Should we support Windows?\", \"Is this a public or private API?\"\n4. **Draft Blueprint**: Provide a high-level file structure and key component definitions.\n5. **Lock Contract**: Summarize the \"Must-Haves\" and \"Acceptance Criteria\".\n\n### Example Snippet\n```markdown\nYou are the Architect. I am the Homeowner.\nOur goal: Build a secure file uploader.\nStack: Node.js, TypeScript, AWS S3.\n\nFollow the Vibecode 6-Step Workflow.\nStart with Step 1 (Vision) and Step 2 (Context).\nDo not write implementation code until the Contract is LOCKED.\n```\n\n### Common Mistakes\n- **Skipping to Build**: Writing code before the Architect and Homeowner agree on the Blueprint.\n- **Role Confusion**: The AI acting as a passive \"coder\" instead of a proactive \"architect\".\n- **Ignoring the Contract**: Implementing features that were explicitly marked as \"Out of Scope\".", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Debug Protocol", "signals": ["iterate_loop"], "tags": ["vibecode", "debugging", "protocol"], "summary": "Structured debugging protocol for isolating failures, hypothesizing root causes, and applying minimal, verified fixes.", "chunks": [{"chunk_id": "core:vibecode-debug:1", "text": "## Vibecode Debug Protocol\n\nA systematic approach to resolving bugs without introducing regressions. Focus on isolation and evidence.\n\n### 1. Reproduce & Isolate\nCreate the smallest possible reproduction case. If it's a CLI bug, provide the exact command and environment.\n- **Action**: Capture logs, stack traces, and state snapshots.\n- **Tool**: `vibeanvil build manual start` to isolate the session.\n\n### 2. Hypothesize\nList 2-3 potential root causes based on the evidence.\n- **Example**: \"1. Race condition in async block. 2. Missing environment variable. 3. Path traversal vulnerability.\"\n\n### 3. Validate\nTest each hypothesis with focused checks (e.g., adding print statements, checking file permissions).\n- **Goal**: Eliminate false leads quickly.\n\n### 4. Minimal Fix\nApply the smallest possible change that resolves the issue. Avoid \"drive-by refactoring\" during a debug session.\n- **Verification**: Re-run the reproduction case.\n\n### 5. Guard & Document\nAdd a regression test to ensure the bug never returns.\n- **Action**: Update the `contract` if the bug revealed a missing requirement.\n- **Handover**: Document the fix in the audit log.\n\n### Anti-Patterns\n- **Shotgun Debugging**: Changing multiple things at once without knowing which one fixed it.\n- **Ignoring the Log**: Guessing the cause instead of reading the actual error message.\n- **Fixing the Symptom**: Patching the output instead of the underlying logic error.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode QA Strategy", "signals": ["iterate_loop"], "tags": ["vibecode", "qa", "testing"], "summary": "Tiered testing framework covering core logic, edge cases, and performance thresholds with evidence capture.", "chunks": [{"chunk_id": "core:vibecode-qa:1", "text": "## Vibecode QA Strategy\n\nQuality is not an afterthought; it is built into the `contract`. We use a tiered approach to ensure robustness.\n\n### Tier 1: Core Logic (Must Pass)\nUnit tests for critical business rules and state transitions.\n- **Focus**: Happy paths and primary functionality.\n- **Requirement**: 100% pass rate before any `review`.\n\n### Tier 2: Edge Cases (Robustness)\nTesting boundaries, invalid inputs, and unexpected state.\n- **Examples**: Empty strings, extremely large files, network timeouts, unauthorized access attempts.\n- **Goal**: Graceful failure with helpful error messages.\n\n### Tier 3: Performance & Integration\nEnd-to-end flows and resource usage checks.\n- **Focus**: Latency, memory leaks, and third-party API stability.\n- **Tool**: `vibeanvil build iterate --strict` to enforce these checks.\n\n### Verification Steps\n1. **Map Tests to Requirements**: Every requirement in the `contract` must have at least one corresponding test.\n2. **Capture Evidence**: Save test logs and coverage reports to the session's `evidence/` folder.\n3. **Audit**: Human reviewer checks the evidence before `review pass`.\n\n### Success Criteria\n- [ ] All Tier 1 and Tier 2 tests pass.\n- [ ] No regressions in existing functionality.\n- [ ] Performance metrics are within the defined thresholds.", "start_line": 1, "end_line": 28}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Security Audit Checklist", "signals": ["security_pattern"], "tags": ["vibecode", "security", "audit"], "summary": "Comprehensive security review checklist for identifying and mitigating common application risks and vulnerabilities.", "chunks": [{"chunk_id": "core:vibecode-security:1", "text": "## Vibecode Security Audit Checklist\n\nSecurity is a first-class citizen in the Vibecode workflow. Every feature must pass this audit before shipping.\n\n### 1. Authentication & Authorization\n- [ ] Are all sensitive routes protected?\n- [ ] Is session handling secure (HttpOnly, Secure flags)?\n- [ ] Are permissions checked at the resource level (not just UI)?\n\n### 2. Input Validation & Sanitization\n- [ ] Is all user input validated against a strict schema?\n- [ ] Are we protected against SQL injection, XSS, and Path Traversal?\n- [ ] Are file uploads restricted by type and size?\n\n### 3. Secret Management\n- [ ] Are secrets stored in environment variables or a vault (never hardcoded)?\n- [ ] Does the `vibeanvil` secret scanner pass without warnings?\n- [ ] Are logs redacted to prevent leaking tokens or PII?\n\n### 4. Dependency Security\n- [ ] Are we using the latest stable versions of libraries?\n- [ ] Have we run `npm audit` or `cargo audit` recently?\n- [ ] Are there any abandoned or high-risk dependencies?\n\n### 5. Data Protection\n- [ ] Is sensitive data encrypted at rest and in transit (TLS)?\n- [ ] Are we following the principle of least privilege for database access?\n\n### Anti-Patterns\n- **Trusting the Client**: Performing validation only on the frontend.\n- **Security by Obscurity**: Relying on hidden URLs or secret parameters for protection.\n- **Verbose Errors**: Leaking stack traces or database schemas in error responses.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Performance Review", "signals": ["iterate_loop"], "tags": ["vibecode", "performance", "optimization"], "summary": "Systematic performance review process for measuring baselines, identifying bottlenecks, and validating optimizations.", "chunks": [{"chunk_id": "core:vibecode-performance:1", "text": "## Vibecode Performance Review\n\nPerformance is a feature. We optimize based on data, not intuition.\n\n### 1. Establish Baselines\nMeasure the current performance under realistic load.\n- **Metrics**: Response time (p95/p99), CPU usage, memory footprint, bundle size.\n- **Tool**: Use profilers (e.g., `samply` for Rust, Chrome DevTools for Web).\n\n### 2. Identify Bottlenecks\nFind the \"Hot Paths\" where the most time or resources are spent.\n- **Common Culprits**: N+1 database queries, synchronous I/O on the main thread, unoptimized loops, large asset transfers.\n\n### 3. Propose & Implement Optimizations\nFocus on high-impact, low-complexity changes first.\n- **Strategies**: Caching, lazy loading, database indexing, parallelization, minification.\n- **Constraint**: Optimizations must not break the `contract` or reduce readability significantly.\n\n### 4. Validate & Compare\nRe-run the baseline tests to verify the improvement.\n- **Evidence**: Capture \"Before vs After\" metrics in the audit log.\n- **Regression Check**: Ensure the optimization didn't introduce new bugs or memory leaks.\n\n### Decision Framework\n- **Is it worth it?**: If the optimization saves 10ms but adds 500 lines of complex code, it might be a \"No\".\n- **Scalability**: Will this optimization still work when the data size grows by 10x?\n\n### Success Criteria\n- [ ] Performance targets defined in the `contract` are met.\n- [ ] No significant increase in memory or CPU usage for non-hot paths.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Integration Protocol", "signals": ["state_machine"], "tags": ["vibecode", "integration", "apis"], "summary": "Standardized protocol for integrating third-party services, covering authentication, error handling, and retry strategies.", "chunks": [{"chunk_id": "core:vibecode-integration:1", "text": "## Vibecode Integration Protocol\n\nIntegrating external services requires a defensive and well-documented approach.\n\n### 1. Provider Discovery\n- **Auth**: Identify the method (OAuth2, API Key, JWT).\n- **Limits**: Document rate limits, quotas, and pricing tiers.\n- **SDK vs REST**: Decide whether to use an official library or raw HTTP calls.\n\n### 2. Contract Mapping\nMap the external API's data structures to our internal models.\n- **Action**: Create \"Adapter\" or \"Gateway\" patterns to decouple our core logic from the provider's API.\n- **Validation**: Use Zod or Pydantic to validate incoming data at the boundary.\n\n### 3. Error & Retry Strategy\nExternal services *will* fail. Plan for it.\n- **Retries**: Implement exponential backoff for transient errors (503, 429).\n- **Circuit Breaker**: Stop calling the service if it's consistently failing to prevent cascading failures.\n- **User Feedback**: Map cryptic API errors to actionable messages for the end user.\n\n### 4. Webhook Handling\n- **Security**: Verify signatures on all incoming webhooks.\n- **Idempotency**: Ensure that processing the same webhook twice doesn't cause duplicate side effects.\n- **Async Processing**: Acknowledge the webhook immediately and process the payload in a background job.\n\n### 5. Testing & Sandboxing\n- **Mocks**: Use tools like `msw` or `wiremock` for unit and integration tests.\n- **Sandbox**: Always use a \"Test\" or \"Staging\" environment for development.\n- **Evidence**: Capture successful integration flows in the session logs.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "prompt", "title": "Vibecode Handover (XRAY) Checklist", "signals": ["evidence_audit"], "tags": ["vibecode", "handover", "documentation"], "summary": "Comprehensive handover checklist for documenting architecture, setup, operations, and future roadmap for a project.", "chunks": [{"chunk_id": "core:vibecode-xray:1", "text": "## Vibecode Handover (XRAY) Checklist\n\nThe goal of a handover is to make the project \"X-Rayable\"\u2014transparent and easy for a new developer (or AI) to understand and maintain.\n\n### 1. Project Context\n- **The \"Why\"**: What problem does this solve? Who are the stakeholders?\n- **Status**: Is this a prototype, MVP, or production-ready?\n\n### 2. Technical Architecture\n- **Diagrams**: High-level flow of data and component relationships.\n- **Key Decisions**: Document \"ADRs\" (Architecture Decision Records) for non-obvious choices.\n- **Stack**: Versions of languages, frameworks, and databases.\n\n### 3. Operations & Setup\n- **Quickstart**: How to get the project running in < 5 minutes.\n- **Env Vars**: List all required secrets and configurations (with examples).\n- **Deployment**: How is it shipped? (CI/CD pipelines, infrastructure as code).\n\n### 4. Maintenance & Debt\n- **Known Issues**: List bugs that weren't fixed or edge cases not covered.\n- **Technical Debt**: Areas that need refactoring or scaling.\n- **Roadmap**: What are the logical next steps for this feature?\n\n### 5. Audit Trail\n- **Evidence**: Link to the `vibeanvil` session logs and contracts.\n- **Tests**: How to run the test suite and interpret results.\n\n### Success Criteria\n- [ ] A new developer can run the project and pass tests without asking questions.\n- [ ] All architectural decisions are documented and justified.\n- [ ] The `contract` and `blueprint` are up to date with the final implementation.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "template", "title": "Agent Template Schema", "signals": ["command_surface"], "tags": ["agent", "template", "prompt"], "summary": "Standardized schema for defining specialized AI agents with clear personas, expertise areas, and usage guidance.", "chunks": [{"chunk_id": "core:agent-template:1", "text": "## Agent Template Schema\n\nUse this schema to create specialized agents that can be registered in the Vibecode ecosystem.\n\n### Frontmatter\n```yaml\n---\nname: [unique-slug]\ndescription: [one-sentence-purpose]\ndomain: [e.g., security, frontend, devops]\ncolor: [ui-accent-color]\nexamples:\n  - context: \"User wants to audit a login form\"\n    user: \"Check this for vulnerabilities\"\n    assistant: \"I will perform a security audit focusing on...\"\n---\n```\n\n### Persona Definition\nDefine the agent's \"vibe\" and professional background.\n- **Role**: \"You are a Senior Security Auditor with 10 years of experience in OWASP...\"\n- **Tone**: Concise, technical, and proactive.\n\n### Expertise Areas\nList 3-5 specific domains where this agent excels.\n- **Area 1**: [Specific capability, e.g., \"JWT implementation and session security\"]\n- **Area 2**: [Specific capability, e.g., \"SQL injection prevention and query optimization\"]\n\n### Usage Guidance\n- **When to Use**: Describe the specific triggers or tasks this agent is designed for.\n- **When NOT to Use**: Define the boundaries to prevent using the wrong tool for the job.\n- **Actionable Steps**: Provide a default workflow the agent should follow when activated.\n\n### Success Criteria\n- [ ] Agent provides specific, actionable advice instead of generic fluff.\n- [ ] Agent correctly identifies domain-specific risks and tradeoffs.\n- [ ] Agent's output follows the project's coding conventions.", "start_line": 1, "end_line": 35}]}
{"source_id": "core", "type": "template", "title": "Command Template Schema", "signals": ["command_surface"], "tags": ["command", "template", "prompt"], "summary": "Standardized schema for defining CLI slash commands with context gathering, task steps, and quality standards.", "chunks": [{"chunk_id": "core:command-template:1", "text": "## Command Template Schema\n\nCommands are the \"verbs\" of the Vibecode system. Use this schema to define reusable, high-reliability workflows.\n\n### Configuration\n```yaml\n---\nname: [command-name]\ndescription: [what it does]\nallowed-tools: [list-of-mcp-tools-or-cli-commands]\nargument-hint: \"[arg1] [arg2]\"\n---\n```\n\n### 1. Context Gathering\nDefine what the command needs to know before it starts.\n- **Repo Scan**: \"Check for `package.json` or `Cargo.toml` to detect the environment.\"\n- **State Check**: \"Verify that the `contract` is LOCKED before proceeding.\"\n\n### 2. Task Execution\nBreak the command into atomic, logical steps.\n- **Step 1**: [Action, e.g., \"Generate a new component scaffold\"]\n- **Step 2**: [Action, e.g., \"Add basic unit tests for the component\"]\n- **Step 3**: [Action, e.g., \"Update the main index file to export the component\"]\n\n### 3. Quality Standards\nDefine the \"Definition of Done\" for this command.\n- **Validation**: \"Run `npm test` and ensure the new component is covered.\"\n- **Linting**: \"Ensure no new linting errors were introduced.\"\n- **Evidence**: \"Capture the diff and test output in the session log.\"\n\n### Anti-Patterns\n- **Over-Automation**: Trying to do too much in a single command (keep them atomic).\n- **Silent Failures**: Not checking the exit code of sub-commands.\n- **Hardcoded Paths**: Use variables or relative paths to ensure portability.", "start_line": 1, "end_line": 35}]}
{"source_id": "core", "type": "template", "title": "Plugin Bundle Manifest Schema", "signals": ["provider_adapter"], "tags": ["plugin", "bundle", "registry"], "summary": "Manifest schema for packaging agents, commands, and MCP servers into distributable plugin bundles.", "chunks": [{"chunk_id": "core:plugin-manifest:1", "text": "## Plugin Bundle Manifest Schema\n\nBundles allow you to ship curated toolkits for specific workflows (e.g., a \"Security Pro\" bundle).\n\n### Manifest Structure\n```json\n{\n  \"name\": \"vibecode-security-pro\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Advanced security auditing agents and commands.\",\n  \"author\": \"VibeAnvil Team\",\n  \"keywords\": [\"security\", \"audit\", \"owasp\"],\n  \"agents\": [\n    \"agents/auditor.md\",\n    \"agents/scanner.md\"\n  ],\n  \"commands\": [\n    \"commands/audit-secrets.md\",\n    \"commands/check-deps.md\"\n  ],\n  \"mcpServers\": [\n    {\n      \"name\": \"security-tools\",\n      \"command\": \"npx\",\n      \"args\": [\"@vibeanvil/mcp-security\"]\n    }\n  ]\n}\n```\n\n### Best Practices\n- **Versioning**: Use SemVer to manage breaking changes in agents or commands.\n- **Dependencies**: List any external CLI tools (e.g., `docker`, `git`) required by the bundle.\n- **Documentation**: Include a `README.md` in the bundle explaining the intended workflow.\n\n### Use Cases\n- **Team Standards**: Ship a bundle with your company's specific linting and testing rules.\n- **Domain Expertise**: Create bundles for specific stacks (e.g., \"React Native Expert\").\n- **CI/CD Integration**: Use bundles to standardize AI behavior in automated pipelines.", "start_line": 1, "end_line": 35}]}
{"source_id": "core", "type": "template", "title": "Project Profile Template", "signals": ["command_surface"], "tags": ["project", "template", "profile"], "summary": "Template for defining project-specific context, stack, conventions, and workflows for AI coding assistants.", "chunks": [{"chunk_id": "core:project-profile:1", "text": "## Project Profile Template\n\nThe Project Profile is the \"Source of Truth\" for an AI agent entering a new repository.\n\n### 1. Project Overview\n- **Mission**: What is the primary goal of this software?\n- **Stack**: Languages, frameworks, databases, and key libraries (with versions).\n- **Architecture**: Monolith, microservices, serverless? Key patterns used (e.g., Clean Architecture).\n\n### 2. Development Workflow\n- **Setup**: `vibeanvil init` -> `npm install` -> `vibeanvil brain ensure`.\n- **Testing**: How to run unit, integration, and E2E tests.\n- **Linting/Formatting**: Which tools are used (ESLint, Prettier, Clippy)?\n\n### 3. Coding Conventions\n- **Naming**: camelCase for JS, snake_case for Python/Rust?\n- **Structure**: Where do components, services, and types live?\n- **Patterns**: Do we prefer functional or OO? How do we handle errors?\n\n### 4. AI Guardrails\n- **Forbidden Actions**: \"Never modify `src/legacy/`\", \"Do not add new dependencies without approval\".\n- **Preferred Tools**: \"Always use `axios` for HTTP\", \"Use `shadcn/ui` for components\".\n\n### 5. Success Criteria\n- [ ] Profile is checked into the repo as `.vibeanvil/profile.md`.\n- [ ] Profile is updated whenever the stack or conventions change.\n- [ ] AI agents reference this profile before every major task.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "prompt", "title": "Agent Role Catalog", "signals": ["command_surface"], "tags": ["agent", "catalog", "roles"], "summary": "Catalog of common specialist agent roles and their specific triggers, expertise, and value propositions.", "chunks": [{"chunk_id": "core:agent-catalog:1", "text": "## Agent Role Catalog\n\nA directory of specialized personas designed for high-impact tasks. Use these to reduce context drift and improve quality.\n\n### 1. The Architect\n- **Trigger**: New feature request, major refactor, or system design.\n- **Expertise**: Design patterns, scalability, and technical debt management.\n- **Value**: Ensures the project stays maintainable and follows long-term vision.\n\n### 2. The Security Auditor\n- **Trigger**: Adding authentication, handling PII, or exposing new APIs.\n- **Expertise**: OWASP Top 10, encryption, and input validation.\n- **Value**: Prevents vulnerabilities before they reach production.\n\n### 3. The QA Engineer\n- **Trigger**: Completing a build iteration or fixing a complex bug.\n- **Expertise**: Test coverage, edge case detection, and regression testing.\n- **Value**: Ensures the \"Definition of Done\" is met and the code is robust.\n\n### 4. The Performance Specialist\n- **Trigger**: Slow response times, high memory usage, or large bundle sizes.\n- **Expertise**: Profiling, caching, and algorithmic optimization.\n- **Value**: Keeps the application fast and resource-efficient.\n\n### 5. The Technical Writer\n- **Trigger**: Shipping a feature, creating a handover, or documenting an API.\n- **Expertise**: Markdown, JSDoc/RustDoc, and user-centric documentation.\n- **Value**: Makes the codebase \"X-Rayable\" for humans and other AI agents.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "prompt", "title": "Command Library Patterns", "signals": ["command_surface"], "tags": ["command", "library", "patterns"], "summary": "Reusable patterns and best practices for designing reliable, portable, and actionable CLI command prompts.", "chunks": [{"chunk_id": "core:command-library:1", "text": "## Command Library Patterns\n\nDesign patterns for creating a robust library of slash commands that automate your development workflow.\n\n### 1. The \"Pre-flight Check\" Pattern\nAlways verify the environment before executing the main task.\n- **Example**: Check if `git` is clean, if the `contract` is locked, or if a specific dependency is installed.\n- **Benefit**: Prevents half-finished actions that leave the repo in a broken state.\n\n### 2. The \"Atomic Action\" Pattern\nKeep commands focused on a single, well-defined task.\n- **Example**: `/create-component` should only create the files and exports, not also try to style them or add complex logic.\n- **Benefit**: Easier to debug, test, and compose into larger workflows.\n\n### 3. The \"Evidence Capture\" Pattern\nEvery command should produce a log of what it did and why.\n- **Example**: \"Created 3 files, updated 1. Tests passed with 85% coverage.\"\n- **Benefit**: Provides a clear audit trail for the `review` phase.\n\n### 4. The \"Rollback\" Pattern\nProvide a way to undo the command's actions if something goes wrong.\n- **Example**: Use `vibeanvil undo` or ensure the command only makes changes that can be easily reverted via git.\n- **Benefit**: Increases developer confidence when using automated tools.\n\n### 5. The \"Next Step\" Suggestion\nEnd every command with a suggestion for what the user should do next.\n- **Example**: \"Component created. Run `/add-tests [name]` to complete the Tier 1 QA.\"\n- **Benefit**: Maintains momentum and guides the user through the workflow.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "prompt", "title": "Plugin Bundle Catalog", "signals": ["provider_adapter"], "tags": ["plugin", "bundle", "catalog"], "summary": "Curated categories of plugin bundles for standardizing workflows across different domains and technologies.", "chunks": [{"chunk_id": "core:bundle-catalog:1", "text": "## Plugin Bundle Catalog\n\nCurated toolkits designed to supercharge specific aspects of your development lifecycle.\n\n### 1. The \"Ship-It\" Bundle (Release Management)\n- **Agents**: Release Manager, Changelog Writer.\n- **Commands**: `/prepare-release`, `/tag-version`, `/generate-notes`.\n- **Focus**: Automating the final 10% of the delivery process.\n\n### 2. The \"Shield\" Bundle (Security & Compliance)\n- **Agents**: Security Auditor, Dependency Guard.\n- **Commands**: `/audit-secrets`, `/scan-deps`, `/verify-licenses`.\n- **Focus**: Continuous security monitoring and risk mitigation.\n\n### 3. The \"Vibe-Check\" Bundle (UI/UX & Frontend)\n- **Agents**: UI Designer, Accessibility Expert.\n- **Commands**: `/create-component`, `/check-a11y`, `/generate-stories`.\n- **Focus**: Building beautiful, accessible, and consistent user interfaces.\n\n### 4. The \"Deep-Dive\" Bundle (Performance & Debugging)\n- **Agents**: Performance Specialist, Debugging Wizard.\n- **Commands**: `/profile-endpoint`, `/analyze-logs`, `/trace-leak`.\n- **Focus**: Solving the hardest technical challenges with data-driven insights.\n\n### 5. The \"Brain-Harvest\" Bundle (Knowledge Management)\n- **Agents**: Librarian, Context Engineer.\n- **Commands**: `/harvest-repo`, `/index-docs`, `/summarize-changes`.\n- **Focus**: Keeping the project's knowledge base fresh and searchable.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "prompt", "title": "Agent Command Pairing", "signals": ["command_surface"], "tags": ["agent", "command", "workflow"], "summary": "Strategic pairing of specialized agents with specific commands to maximize accuracy, speed, and alignment.", "chunks": [{"chunk_id": "core:agent-command-pairing:1", "text": "## Agent Command Pairing\n\nPairing the right \"Mind\" (Agent) with the right \"Tool\" (Command) is the key to high-performance vibe coding.\n\n### Why Pair?\n- **Context Alignment**: A Security Agent knows exactly what to look for when running a `/scan-deps` command.\n- **Reduced Hallucination**: Specialized agents are less likely to suggest generic or irrelevant actions.\n- **Speed**: Commands provide the structure, while agents provide the intelligence, leading to faster \"Definition of Done\".\n\n### Common Pairings\n1. **QA Engineer + `/run-tests`**: The agent analyzes the failure logs and immediately proposes a fix.\n2. **Architect + `/blueprint`**: The agent ensures the proposed architecture follows the project's core principles.\n3. **Technical Writer + `/ship`**: The agent generates a perfect release summary based on the session's evidence.\n4. **Security Auditor + `/audit-secrets`**: The agent identifies false positives and highlights high-risk leaks.\n\n### How to Implement\n- **In Prompts**: \"Using the [Agent Name], execute the [Command Name] on the current context.\"\n- **In Bundles**: Define default pairings in the bundle's `README.md` or manifest.\n- **In Workflows**: Use `vibeanvil build iterate` with a provider that supports agent-switching.\n\n### Success Criteria\n- [ ] Tasks are completed with fewer iterations.\n- [ ] Output quality is consistently higher than using a generic agent.\n- [ ] The audit log shows clear, domain-specific reasoning for every action.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "template", "title": "Landing Page Pattern", "signals": ["vibecode"], "tags": ["vibecode", "landing-page", "pattern"], "summary": "Strategic layout and conversion-focused patterns for high-performance landing pages.", "chunks": [{"chunk_id": "core:landing-page:1", "text": "## Landing Page Pattern\n\nA high-conversion landing page follows a proven psychological sequence to move visitors from curiosity to action.\n\n### 1. Section Layout (The Sequence)\n- **Hero**: Clear H1 (Value Prop) + Subhead + Primary CTA + Visual.\n- **Social Proof**: Logo bar of trusted brands or \"As seen on\".\n- **Problem/Solution**: Agitate the pain point, then present your product as the cure.\n- **Features**: 3-6 key benefits with icons and brief descriptions.\n- **How It Works**: 3-step process to reduce perceived complexity.\n- **Testimonials**: High-quality quotes with names, titles, and photos.\n- **Pricing**: Clear tiers with highlighted \"Most Popular\" option.\n- **FAQ**: Address common objections and technical questions.\n- **Footer**: Secondary nav, social links, and final CTA.\n\n### 2. Conversion Best Practices\n- **Single Goal**: Every element should lead to the primary CTA.\n- **Visual Hierarchy**: Use size, color, and weight to guide the eye.\n- **Speed**: Must load in <2s to prevent bounce.\n- **Mobile First**: Ensure CTAs are thumb-friendly and text is readable.\n\n### 3. Recommended Tech Stack\n- **Framework**: Next.js or Astro for static generation.\n- **Styling**: Tailwind CSS for rapid, consistent UI.\n- **Components**: Radix UI or Headless UI for accessibility.\n- **Analytics**: Plausible or Fathom for privacy-first tracking.\n\n### 4. Headline Formulas\n- [Benefit] without [Pain]: \"Get Fit without the Gym.\"\n- [Action] like a [Pro]: \"Code like a Senior Engineer.\"\n- The [Category] for [Audience]: \"The CRM for Freelancers.\"\n\n### 5. CTA Patterns\n- **Action + Value**: \"Start My Free Trial\", \"Get the Guide\".\n- **Low Friction**: \"No credit card required\", \"Join 10k+ users\".", "start_line": 1, "end_line": 35}]}
{"source_id": "core", "type": "template", "title": "SaaS Application Pattern", "signals": ["vibecode"], "tags": ["vibecode", "saas", "pattern"], "summary": "Architecture and feature checklist for scalable Software-as-a-Service applications.", "chunks": [{"chunk_id": "core:saas-app:1", "text": "## SaaS Application Pattern\n\nModern SaaS apps require a clear distinction between marketing (public) and product (authenticated) areas.\n\n### 1. Architecture\n- **Public Area**: SEO-optimized, static or SSR (Landing, Pricing, Blog).\n- **Auth Area**: Client-side heavy, stateful (Dashboard, Settings, Editor).\n- **Shared**: Design system, types, and API client.\n\n### 2. Core Feature Checklist\n- **Authentication**: Login, Signup, Forgot Password, OAuth (Google/GitHub).\n- **User Roles**: Admin, Member, Viewer permissions.\n- **CRUD Operations**: Create, Read, Update, Delete for core entities.\n- **Notifications**: In-app toasts and email alerts.\n- **Export/Import**: CSV/JSON data portability.\n- **Billing**: Stripe integration, subscription management.\n\n### 3. Layout Patterns\n- **Sidebar**: Persistent navigation for deep apps.\n- **Header**: Search, User Profile, Quick Actions.\n- **Content**: Breadcrumbs, Page Header, Main Action.\n\n### 4. Database Considerations\n- **Multi-tenancy**: Ensure data isolation between organizations.\n- **Soft Deletes**: Use `deleted_at` instead of removing records.\n- **Audit Logs**: Track who changed what and when.\n\n### 5. Recommended Tech Stack\n- **Frontend**: React or Vue with TypeScript.\n- **State**: TanStack Query (Server State) + Zustand (Client State).\n- **Backend**: Node.js (NestJS), Go, or Rust (Axum).\n- **Database**: PostgreSQL (Relational) + Redis (Caching).", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "template", "title": "Dashboard Pattern", "signals": ["vibecode"], "tags": ["vibecode", "dashboard", "pattern"], "summary": "Layout and data visualization patterns for administrative and analytical dashboards.", "chunks": [{"chunk_id": "core:dashboard:1", "text": "## Dashboard Pattern\n\nDashboards should provide immediate clarity and actionable insights through effective data visualization.\n\n### 1. Layout Structure\n- **Sidebar**: Collapsible navigation with clear icons.\n- **Top Bar**: Global search, notifications, and workspace switcher.\n- **Main Content**: Grid-based layout (Bento box style).\n\n### 2. KPI Card Design\n- **Value**: Large, bold number.\n- **Label**: Clear description of the metric.\n- **Trend**: Percentage change with color (Green/Red) and sparkline.\n- **Context**: \"vs last 30 days\".\n\n### 3. Chart Selection Guide\n- **Line Chart**: Best for trends over time (e.g., Revenue).\n- **Bar Chart**: Best for comparisons (e.g., Sales by Region).\n- **Pie/Donut**: Best for distribution (e.g., Device Type) - limit to <5 slices.\n- **Table**: Best for detailed, sortable data with actions.\n\n### 4. Interaction Patterns\n- **Filters**: Date range picker, category dropdowns.\n- **Search**: Instant filtering of list views.\n- **Export**: Download as PDF, CSV, or Image.\n- **Drill-down**: Click a chart element to see detailed data.\n\n### 5. Design Details\n- **Dark Mode**: Essential for long-term usage and reduced eye strain.\n- **Empty States**: Helpful illustrations and \"Get Started\" actions.\n- **Loading States**: Skeletons to maintain layout stability.\n- **Density**: Provide a \"Compact\" vs \"Comfortable\" toggle.", "start_line": 1, "end_line": 32}]}
{"source_id": "core", "type": "template", "title": "Blog and Documentation Pattern", "signals": ["vibecode"], "tags": ["vibecode", "blog", "docs", "pattern"], "summary": "Structural and typographic standards for content-heavy blogs and technical documentation.", "chunks": [{"chunk_id": "core:blog-docs:1", "text": "## Blog and Documentation Pattern\n\nContent-heavy sites prioritize readability, navigation, and searchability above all else.\n\n### 1. Blog Structure\n- **Homepage**: Featured post + Grid of recent posts + Categories.\n- **Post Page**: Reading progress bar, Author bio, Related posts.\n- **Category Page**: Filtered list of posts with pagination.\n- **Author Page**: Bio, social links, and all posts by that author.\n\n### 2. Documentation Structure\n- **Sidebar Nav**: Hierarchical tree of topics.\n- **Main Content**: Clear headings, code blocks, and callouts (Note/Warning).\n- **Table of Contents (TOC)**: Right-side sticky nav for the current page.\n- **Search**: Command-K (Algolia or local Fuse.js) search.\n\n### 3. Typography Specs\n- **Body Text**: 18px minimum, 1.6-1.8 line-height.\n- **Font Choice**: High-legibility serif or sans-serif (e.g., Charter, Inter).\n- **Measure**: 60-75 characters per line (max-width: 65ch).\n- **Contrast**: High contrast (AA or AAA) for accessibility.\n\n### 4. SEO Essentials\n- **Meta Tags**: Title, Description, OpenGraph images.\n- **Structured Data**: JSON-LD for Articles and Breadcrumbs.\n- **Sitemap**: Automated `sitemap.xml` generation.\n- **Canonical Tags**: Prevent duplicate content issues.\n\n### 5. Technical Features\n- **MDX Support**: Embed interactive components in Markdown.\n- **Code Highlighting**: Shiki or Prism with line highlighting.\n- **Copy to Clipboard**: One-click code copying.\n- **Dark Mode**: Automatic based on system preference.", "start_line": 1, "end_line": 33}]}
{"source_id": "core", "type": "template", "title": "Portfolio Pattern", "signals": ["vibecode"], "tags": ["vibecode", "portfolio", "pattern"], "summary": "Design styles and essential sections for professional developer and designer portfolios.", "chunks": [{"chunk_id": "core:portfolio:1", "text": "## Portfolio Pattern\n\nA portfolio is a personal brand statement. Choose a style that matches your professional identity.\n\n### 1. Style Options\n- **Minimal (Devs)**: Focus on code, GitHub links, and technical blog. Clean typography, monochrome palette.\n- **Bold (Designers)**: Large imagery, custom cursors, horizontal scrolls, and vibrant colors.\n- **Editorial (Agencies)**: Grid-breaking layouts, sophisticated serif fonts, and deep case studies.\n\n### 2. Essential Sections\n- **Hero**: Who you are, what you do, and your unique value.\n- **Work/Projects**: 3-5 high-quality case studies (Problem, Solution, Result).\n- **About**: Your story, skills, and personality.\n- **Services**: What you can be hired for.\n- **Contact**: Simple form or direct email link.\n\n### 3. Animation Guidance\n- **Staggered Reveals**: Fade in elements as the user scrolls.\n- **Hover States**: Subtle scale or color shifts on project cards.\n- **Page Transitions**: Smooth fades between routes.\n- **Performance**: Use CSS transforms/opacity; avoid layout shifts.\n\n### 4. Responsive Considerations\n- **Mobile Grid**: Switch from 3-column to 1-column layout.\n- **Navigation**: Simple hamburger menu or bottom bar.\n- **Images**: Use `srcset` for optimized loading on all devices.\n\n### 5. Success Criteria\n- [ ] Site loads in under 1 second.\n- [ ] Contact info is reachable in 2 clicks.\n- [ ] Case studies show \"The Why\" not just \"The What\".", "start_line": 1, "end_line": 31}]}
{"source_id": "core", "type": "prompt", "title": "Typography System Guide", "signals": ["vibecode"], "tags": ["vibecode", "typography", "skill"], "summary": "Curated font pairings and typographic rules for distinct brand identities.", "chunks": [{"chunk_id": "core:typography:1", "text": "## Typography System Guide\n\nTypography is the voice of your interface. A consistent system ensures clarity and brand alignment.\n\n### 1. Font Pairings\n- **Modern Tech**: Inter (Body) + Space Grotesk (Display).\n- **Professional**: Roboto (Body) + Playfair Display (Display).\n- **Creative**: Montserrat (Body) + Syne (Display).\n- **Friendly**: Open Sans (Body) + Quicksand (Display).\n- **Elegant**: Lora (Body) + Cinzel (Display).\n- **Startup**: Plus Jakarta Sans (Body) + Cal Sans (Display).\n\n### 2. Sizing System (Modular Scale)\n- **Base**: 16px (1rem).\n- **Scale**: 1.25 (Major Third).\n- **H1**: 3.052rem (48px).\n- **H2**: 2.441rem (39px).\n- **H3**: 1.953rem (31px).\n- **H4**: 1.563rem (25px).\n- **Small**: 0.8rem (13px).\n\n### 3. Line-Height Rules\n- **Body**: 1.5 to 1.7 for optimal readability.\n- **Headings**: 1.1 to 1.3 (tighter for larger text).\n- **Captions**: 1.4.\n\n### 4. Responsive Typography\n- Use `clamp()` for fluid sizing: `font-size: clamp(2rem, 5vw, 4rem)`.\n- Reduce heading sizes on mobile to prevent awkward wrapping.\n\n### 5. Best Practices\n- **Limit Weights**: Use 400 (Regular), 500 (Medium), and 700 (Bold).\n- **Letter Spacing**: -0.02em for large headings, +0.01em for small caps.\n- **Accessibility**: Ensure 4.5:1 contrast ratio for all text.", "start_line": 1, "end_line": 34}]}
{"source_id": "core", "type": "prompt", "title": "Color Psychology Guide", "signals": ["vibecode"], "tags": ["vibecode", "color", "skill"], "summary": "Emotional mapping and palette construction rules for effective visual communication.", "chunks": [{"chunk_id": "core:color:1", "text": "## Color Psychology Guide\n\nColors evoke emotions and drive behavior. Use them intentionally to align with your brand goals.\n\n### 1. Color-Emotion Mappings\n- **Blue (Trust)**: Stability, security, professionalism. (#0066FF)\n- **Orange (Energy)**: Friendly, cheerful, confident. (#FF6600)\n- **Green (Growth)**: Health, nature, prosperity. (#00CC66)\n- **Purple (Luxury)**: Wisdom, mystery, sophistication. (#9933FF)\n- **Red (Urgency)**: Excitement, passion, danger. (#FF3300)\n- **Gray (Neutral)**: Balance, calm, formal. (#666666)\n\n### 2. Palette Construction (60-30-10 Rule)\n- **60% Primary**: Neutral background or dominant brand color.\n- **30% Secondary**: Supporting color for navigation and cards.\n- **10% Accent**: High-contrast color for CTAs and highlights.\n\n### 3. Contrast Guidelines\n- **WCAG AA**: 4.5:1 for normal text, 3:1 for large text.\n- **WCAG AAA**: 7:1 for normal text, 4.5:1 for large text.\n- **Tools**: Use Contrast (Mac) or online checkers.\n\n### 4. Semantic Colors\n- **Success**: Green for confirmations and positive actions.\n- **Warning**: Yellow/Amber for alerts and cautions.\n- **Error**: Red for failures and destructive actions.\n- **Info**: Blue for neutral notifications.\n\n### 5. Implementation\n- Use CSS Variables: `--color-primary`, `--color-accent`.\n- Support Dark Mode by swapping variables in a `.dark` class.", "start_line": 1, "end_line": 31}]}
{"source_id": "core", "type": "prompt", "title": "Motion and Animation Guide", "signals": ["vibecode"], "tags": ["vibecode", "motion", "animation", "skill"], "summary": "Patterns for entrance animations, scroll reveals, and interactive states with performance focus.", "chunks": [{"chunk_id": "core:motion:1", "text": "## Motion and Animation Guide\n\nMotion should feel purposeful, not distracting. It guides the user's attention and provides feedback.\n\n### 1. Entrance Patterns\n- **Fade In**: Simple opacity 0 -> 1.\n- **Slide Up**: Opacity + `translateY(20px)` -> `0`.\n- **Stagger**: Apply `animation-delay` to list items for a waterfall effect.\n\n### 2. Scroll-Triggered Reveals\n- Use `IntersectionObserver` or libraries like Framer Motion.\n- **Threshold**: Trigger when 10-20% of the element is visible.\n- **Once**: Only animate the first time it enters the viewport.\n\n### 3. Interactive States\n- **Hover**: Subtle scale (1.02) or shadow increase.\n- **Active**: Slight scale down (0.98) to mimic a physical press.\n- **Focus**: Clear, high-contrast ring for keyboard users.\n\n### 4. Performance Rules\n- **GPU Layers**: Animate `transform` and `opacity` only.\n- **Avoid**: Animating `width`, `height`, `top`, `left` (triggers reflow).\n- **Duration**: 200ms-300ms for UI transitions; 500ms+ for large reveals.\n- **Easing**: Use `cubic-bezier(0.4, 0, 0.2, 1)` for natural movement.\n\n### 5. Accessibility\n- **Reduced Motion**: Respect `prefers-reduced-motion` media query.\n- **No Flashing**: Avoid rapid transitions that could trigger seizures.", "start_line": 1, "end_line": 28}]}
{"source_id": "core", "type": "prompt", "title": "Web Vitals Performance Guide", "signals": ["vibecode"], "tags": ["vibecode", "performance", "web-vitals", "skill"], "summary": "Optimization strategies for Core Web Vitals (LCP, FID, CLS) and overall page speed.", "chunks": [{"chunk_id": "core:performance:1", "text": "## Web Vitals Performance Guide\n\nPerformance is a feature. Google's Core Web Vitals are the standard for measuring user experience.\n\n### 1. Core Targets\n- **LCP (Largest Contentful Paint)**: < 2.5s (Loading speed).\n- **FID (First Input Delay)**: < 100ms (Interactivity).\n- **CLS (Cumulative Layout Shift)**: < 0.1 (Visual stability).\n\n### 2. Image Optimization\n- **Format**: Use WebP or AVIF for superior compression.\n- **Sizing**: Use `srcset` to serve the right size for the device.\n- **Lazy Loading**: Add `loading=\"lazy\"` to below-the-fold images.\n- **Dimensions**: Always include `width` and `height` to prevent CLS.\n\n### 3. Code Splitting\n- **Dynamic Imports**: Load components only when needed (e.g., Modals).\n- **Route Splitting**: Each page should only load its own JS.\n- **Tree Shaking**: Remove unused code from your bundles.\n\n### 4. Font Loading\n- **Swap**: Use `font-display: swap` to show fallback text immediately.\n- **Preload**: Preload critical fonts in the `<head>`.\n- **Subsetting**: Only include the characters you actually use.\n\n### 5. Caching & Delivery\n- **CDN**: Serve assets from the edge (Vercel, Cloudflare).\n- **Cache Headers**: Use `Cache-Control: max-age=31536000, immutable`.\n- **Compression**: Enable Gzip or Brotli on your server.", "start_line": 1, "end_line": 29}]}
{"source_id": "core", "type": "prompt", "title": "Accessibility Checklist", "signals": ["vibecode"], "tags": ["vibecode", "accessibility", "wcag", "skill"], "summary": "Comprehensive checklist for WCAG compliance, keyboard navigation, and screen reader support.", "chunks": [{"chunk_id": "core:accessibility:1", "text": "## Accessibility Checklist\n\nAccessibility (a11y) ensures your site is usable by everyone, regardless of ability.\n\n### 1. Keyboard Navigation\n- [ ] All interactive elements are reachable via `Tab`.\n- [ ] Focus order follows the visual layout.\n- [ ] No \"keyboard traps\" (user can't get out of a modal).\n- [ ] Visible focus indicators for all elements.\n\n### 2. Content & Structure\n- [ ] Logical heading hierarchy (H1 -> H2 -> H3).\n- [ ] Meaningful `alt` text for all images.\n- [ ] Descriptive link text (avoid \"Click here\").\n- [ ] ARIA landmarks (`<nav>`, `<main>`, `<footer>`).\n\n### 3. Forms\n- [ ] Every input has a visible, associated `<label>`.\n- [ ] Error messages are clear and programmatically linked.\n- [ ] Required fields are clearly marked.\n\n### 4. Visuals\n- [ ] Color contrast meets WCAG AA (4.5:1).\n- [ ] Information is not conveyed by color alone.\n- [ ] Text can be zoomed to 200% without loss of function.\n\n### 5. Testing\n- [ ] Test with a screen reader (VoiceOver, NVDA).\n- [ ] Run automated audits (Lighthouse, Axe).\n- [ ] Navigate the entire site using only a keyboard.", "start_line": 1, "end_line": 30}]}
{"source_id": "core", "type": "prompt", "title": "CTA and Copy Optimization", "signals": ["vibecode"], "tags": ["vibecode", "copy", "cta", "skill"], "summary": "High-conversion headline formulas and call-to-action patterns for persuasive interfaces.", "chunks": [{"chunk_id": "core:copy-cta:1", "text": "## CTA and Copy Optimization\n\nCopywriting is the bridge between your product and the user's needs.\n\n### 1. Headline Formulas\n- **The Outcome**: [Number] + [Timeframe] + [Outcome] (\"Get 100 Leads in 30 Days\").\n- **The Benefit**: [Verb] + [Object] + [Benefit] (\"Build Apps Faster with AI\").\n- **The Question**: \"Tired of [Pain Point]? Try [Solution].\"\n\n### 2. CTA Patterns\n- **Action Verb + Value**: \"Start Building\", \"Get My Free Audit\".\n- **Urgency**: \"Limited Time Offer\", \"Join the Waitlist\".\n- **Clarity**: \"No Credit Card Required\", \"Cancel Anytime\".\n\n### 3. Social Proof Patterns\n- **Logo Bar**: \"Trusted by teams at...\"\n- **Stats**: \"10,000+ developers use VibeAnvil.\"\n- **Testimonials**: \"This tool saved us 20 hours a week.\"\n\n### 4. Microcopy Guidelines\n- **Buttons**: Use first-person (\"Start my trial\") or direct action (\"Sign Up\").\n- **Errors**: Be helpful, not blaming (\"Please check your email format\").\n- **Success**: Celebrate the win (\"You're all set!\").\n\n### 5. Best Practices\n- **Scannability**: Use short paragraphs, bullets, and bold text.\n- **Tone**: Match your audience (Professional vs Playful).\n- **One Goal**: Every page should have one primary desired action.", "start_line": 1, "end_line": 28}]}
{"source_id": "core", "type": "prompt", "title": "OWASP Top 10 Prevention Guide", "signals": ["vibecode"], "tags": ["vibecode", "security", "owasp", "skill"], "summary": "Comprehensive prevention patterns for all OWASP Top 10 vulnerability categories with actionable code-level mitigations.", "chunks": [{"chunk_id": "core:owasp-top10:1", "text": "## OWASP Top 10 Prevention Guide\n\nEvery production application must defend against these 10 vulnerability categories. Assume all input is malicious.\n\n### A01: Broken Access Control\n- Deny by default. Every endpoint must check authorization before processing.\n- Use server-side access control; never rely on client-side checks.\n- Enforce record-level ownership: `WHERE user_id = ?` on every query.\n- Disable directory listing. Return 403 for unauthorized resource access.\n\n### A02: Cryptographic Failures\n- Encrypt data in transit (TLS 1.2+) and at rest (AES-256-GCM).\n- Hash passwords with bcrypt (cost 12+) or argon2id. Never MD5/SHA1.\n- Rotate encryption keys on a schedule. Use envelope encryption for large data.\n\n### A03: Injection\n- Use parameterized queries or ORM for all database access. Never string-concatenate SQL.\n- Validate input type, length, range, and format on the server side.\n- Use allowlists over denylists. Escape output contextually (HTML, JS, URL, CSS).\n\n### A04: Insecure Design\n- Threat model during design phase, not after deployment.\n- Define security user stories: \"As a user, I cannot access another user's data.\"\n- Apply rate limiting, CAPTCHA, and business logic validation.\n\n### A05: Security Misconfiguration\n- Remove default credentials, sample apps, and debug endpoints before deploy.\n- Automate hardening with IaC (Terraform, Ansible). Audit configs in CI.\n- Disable XML external entity processing. Set restrictive CORS policies.\n\n### A06: Vulnerable Components\n- Run `npm audit` / `cargo audit` / `pip-audit` in CI. Block builds on critical CVEs.\n- Pin dependency versions. Use lockfiles. Review changelogs before major upgrades.\n\n### A07: Authentication Failures\n- Enforce MFA for admin accounts. Use credential stuffing protections.\n- Implement account lockout after 5 failed attempts with exponential backoff.\n- Use secure session tokens (128-bit entropy, HttpOnly, Secure, SameSite=Strict).\n\n### A08: Data Integrity Failures\n- Verify software updates and CI/CD pipelines with signatures and checksums.\n- Use Subresource Integrity (SRI) for CDN-hosted scripts.\n\n### A09: Logging & Monitoring Failures\n- Log all auth events, access control failures, and input validation failures.\n- Never log secrets, tokens, or PII. Use structured logging (JSON).\n- Set up alerts for anomalous patterns. Retain logs for incident investigation.\n\n### A10: Server-Side Request Forgery (SSRF)\n- Validate and allowlist all URLs the server fetches. Block internal IP ranges.\n- Use network-level controls (firewall rules) to restrict outbound requests.", "start_line": 1, "end_line": 44}]}
{"source_id": "core", "type": "prompt", "title": "Input Validation Patterns", "signals": ["vibecode"], "tags": ["vibecode", "security", "validation", "skill"], "summary": "Server-side input validation strategies including sanitization, allowlists, parameterized queries, and file upload security.", "chunks": [{"chunk_id": "core:input-validation:1", "text": "## Input Validation Patterns\n\nNever trust user input. Validate on the server even if you validate on the client.\n\n### 1. Validation Strategy\n- **Allowlists over denylists**: Define what IS allowed, not what isn't.\n- **Validate type, length, range, format**: `typeof`, `.length`, min/max, regex.\n- **Fail closed**: Reject invalid input with a 400 error. Never silently coerce.\n- **Canonical form**: Normalize input (trim, lowercase, NFC unicode) before validation.\n\n### 2. Parameterized Queries\n- Always use prepared statements: `db.query('SELECT * FROM users WHERE id = ?', [id])`.\n- ORMs are safer but still validate input before passing to query builders.\n- Never interpolate user input into SQL, NoSQL, LDAP, or OS command strings.\n- For dynamic column/table names, use a strict allowlist map, never user input directly.\n\n### 3. Type Coercion Attacks\n- JavaScript: `'0' == false` is true. Always use strict equality `===`.\n- PHP: `strcmp([], 'password')` returns 0. Use type-safe comparison functions.\n- JSON parsing: Validate parsed objects against a schema (Zod, Joi, JSON Schema).\n- Watch for prototype pollution: freeze prototypes or use `Object.create(null)`.\n\n### 4. Schema Validation (Zod Example)\n- Define schemas at API boundaries: `const UserInput = z.object({ email: z.string().email(), age: z.number().int().min(13).max(120) })`.\n- Parse, don't validate: `const data = UserInput.parse(req.body)` throws on invalid input.\n- Reuse schemas for both request validation and TypeScript types.\n\n### 5. File Upload Validation\n- Check MIME type via magic bytes, not file extension or Content-Type header.\n- Enforce max file size at the web server level (nginx: `client_max_body_size`).\n- Store uploads outside the web root. Generate random filenames. Never serve user-uploaded files with their original name.\n- Scan uploads for malware. Reject executables, scripts, and polyglot files.\n- Use a separate domain/CDN for user-uploaded content to isolate cookie scope.\n\n### 6. Sanitization Patterns\n- HTML: Use DOMPurify or bleach to strip dangerous tags. Allow only safe tags.\n- URLs: Validate scheme is http/https. Block javascript: and data: URIs.\n- Filenames: Strip path separators, null bytes, and Unicode control characters.\n- Numbers: Parse with `parseInt(val, 10)` or `Number(val)`. Check `isNaN()` and `isFinite()`.", "start_line": 1, "end_line": 37}]}
{"source_id": "core", "type": "prompt", "title": "Authentication and Authorization Patterns", "signals": ["vibecode"], "tags": ["vibecode", "security", "auth", "skill"], "summary": "Production-grade patterns for JWT, OAuth 2.0, session management, RBAC/ABAC, password hashing, and MFA implementation.", "chunks": [{"chunk_id": "core:auth-patterns:1", "text": "## Authentication and Authorization Patterns\n\nAuthentication verifies identity. Authorization controls access. Both must be enforced server-side.\n\n### 1. Password Hashing\n- Use bcrypt (cost factor 12+) or argon2id (memory 64MB, iterations 3, parallelism 1).\n- Never store plaintext, MD5, SHA1, or SHA256 password hashes.\n- Add a unique salt per password (bcrypt does this automatically).\n- Implement password strength requirements: min 8 chars, check against breached password lists (HaveIBeenPwned API).\n\n### 2. JWT Best Practices\n- Sign with RS256 (asymmetric) for distributed systems, HS256 (symmetric) for single services.\n- Set short expiry: access tokens 15min, refresh tokens 7 days with rotation.\n- Store access tokens in memory only. Never in localStorage (XSS vulnerable).\n- Validate `iss`, `aud`, `exp`, and `nbf` claims on every request.\n- Include minimal claims: user ID and roles. Fetch full profile from DB.\n- Implement token revocation via a deny-list (Redis) for logout and password changes.\n\n### 3. OAuth 2.0 Flows\n- Web apps: Authorization Code Flow with PKCE (even for server-side apps).\n- SPAs: Authorization Code Flow with PKCE. Never use Implicit Flow.\n- Machine-to-machine: Client Credentials Flow with short-lived tokens.\n- Always validate the `state` parameter to prevent CSRF. Use `nonce` with OIDC.\n\n### 4. Session Management\n- Generate session IDs with 128+ bits of cryptographic randomness.\n- Set cookies: `HttpOnly`, `Secure`, `SameSite=Strict`, `Path=/`, short `Max-Age`.\n- Regenerate session ID after login to prevent session fixation.\n- Implement absolute timeout (24h) and idle timeout (30min).\n\n### 5. RBAC vs ABAC\n- **RBAC** (Role-Based): Assign permissions to roles, roles to users. Simple, auditable.\n- **ABAC** (Attribute-Based): Evaluate policies based on user, resource, and environment attributes. More flexible.\n- Start with RBAC. Move to ABAC when role explosion occurs (>20 roles).\n- Always check authorization at the resource level, not just the route level.\n\n### 6. MFA Implementation\n- Support TOTP (Google Authenticator) as a baseline. Use `otpauth://` URI format.\n- Store recovery codes hashed (bcrypt). Generate 10 single-use codes.\n- Require MFA re-verification for sensitive actions (password change, payment).\n- Rate-limit MFA attempts: 5 attempts per 15 minutes with account lockout.", "start_line": 1, "end_line": 39}]}
{"source_id": "core", "type": "prompt", "title": "Secret Management Guide", "signals": ["vibecode"], "tags": ["vibecode", "security", "secrets", "skill"], "summary": "Production patterns for environment variables, vault integration, key rotation, .env security, and CI/CD secret injection.", "chunks": [{"chunk_id": "core:secret-management:1", "text": "## Secret Management Guide\n\nHardcoded credentials are the most common and preventable security vulnerability. Never commit secrets to version control.\n\n### 1. Environment Variables\n- Store secrets in env vars, not in code: `process.env.DATABASE_URL`.\n- Use `.env` files for local development only. Add `.env` to `.gitignore` immediately.\n- Provide a `.env.example` with placeholder values documenting required variables.\n- Validate all required env vars at startup. Fail fast with clear error messages.\n- Prefix sensitive vars: `SECRET_`, `PRIVATE_` to distinguish from config.\n\n### 2. Vault Integration\n- Use HashiCorp Vault, AWS Secrets Manager, or GCP Secret Manager for production.\n- Fetch secrets at runtime, not build time. Cache with TTL (5min) to reduce API calls.\n- Use dynamic secrets where possible: Vault generates short-lived DB credentials per request.\n- Enable audit logging on your vault. Alert on unusual access patterns.\n- Implement secret versioning for rollback capability.\n\n### 3. Key Rotation\n- Rotate API keys and credentials every 90 days minimum.\n- Support dual-key periods: old and new keys both valid during rotation window.\n- Automate rotation with scripts or vault auto-rotation features.\n- After rotation, verify the old key is revoked and the new key works.\n- Document rotation procedures as runbooks for each secret type.\n\n### 4. CI/CD Secret Injection\n- Use GitHub Actions secrets, GitLab CI variables, or equivalent. Never echo secrets in logs.\n- Mask secrets in CI output: `::add-mask::$SECRET` in GitHub Actions.\n- Scope secrets to environments (staging, production). Use OIDC for cloud auth instead of long-lived keys.\n- Review CI logs for accidental secret exposure. Use tools like `trufflehog` or `gitleaks` in pre-commit hooks.\n\n### 5. Never Hardcode Credentials\n- Run `gitleaks` or `trufflehog` as a pre-commit hook and in CI pipelines.\n- Scan git history: `gitleaks detect --source . --log-opts='--all'`.\n- If a secret is committed, rotate it immediately. Removing from git history is not enough.\n- Use `.gitallowed` for false positives. Never disable the scanner.\n\n### 6. Application Patterns\n- Centralize secret access in a single module/service. Never scatter `process.env` calls.\n- Use typed config: parse and validate secrets at startup into a frozen config object.\n- Log secret access events (who accessed what, when) but never log secret values.\n- In Docker: use `--env-file` or secrets mount. Never pass secrets as build args (visible in image layers).", "start_line": 1, "end_line": 39}]}
{"source_id": "core", "type": "prompt", "title": "Error Handling Patterns", "signals": ["vibecode"], "tags": ["vibecode", "error-handling", "resilience", "skill"], "summary": "Production error handling with custom error types, hierarchies, user-friendly messages, logging strategies, and language-specific patterns.", "chunks": [{"chunk_id": "core:error-handling:1", "text": "## Error Handling Patterns\n\nGood error handling is the difference between a debuggable system and a nightmare. Handle errors explicitly, log them structurally, and never expose internals to users.\n\n### 1. Error Hierarchies\n- Define a base `AppError` class with `code`, `message`, `statusCode`, and `context`.\n- Create specific subclasses: `ValidationError`, `AuthenticationError`, `NotFoundError`, `ConflictError`.\n- Map error types to HTTP status codes: Validation=400, Auth=401, Forbidden=403, NotFound=404, Conflict=409, Internal=500.\n- Include correlation IDs in every error for cross-service tracing.\n\n### 2. Custom Error Types (TypeScript)\n- `class AppError extends Error { constructor(public code: string, message: string, public statusCode: number, public context?: Record<string, unknown>) { super(message); this.name = this.constructor.name; } }`\n- Use discriminated unions for type-safe error handling: `type Result<T> = { ok: true; data: T } | { ok: false; error: AppError }`.\n- Never throw strings. Always throw Error objects with structured metadata.\n\n### 3. User-Friendly Messages\n- Internal errors: log full stack trace, context, and request metadata.\n- User-facing errors: return a safe message with an error code. Never expose stack traces, SQL queries, or file paths.\n- Provide actionable guidance: \"Invalid email format. Please use name@example.com\" not \"Validation failed\".\n- Return consistent error response shape: `{ error: { code: string, message: string, details?: object } }`.\n\n### 4. Logging Strategies\n- Use structured logging (JSON) with consistent fields: `timestamp`, `level`, `message`, `error_code`, `correlation_id`, `user_id`.\n- Log at appropriate levels: ERROR for failures requiring attention, WARN for degraded states, INFO for business events, DEBUG for development.\n- Never log passwords, tokens, PII, or credit card numbers. Redact before logging.\n- Include request context: method, path, status code, duration, user agent.\n\n### 5. React Error Boundaries\n- Wrap route-level components in `ErrorBoundary` to prevent full-app crashes.\n- Implement `componentDidCatch` to log errors to your monitoring service.\n- Show a user-friendly fallback UI with retry option, not a white screen.\n- Use `react-error-boundary` library: `<ErrorBoundary FallbackComponent={ErrorFallback} onError={logError}>`.\n\n### 6. Result/Option Patterns (Rust)\n- Use `Result<T, E>` for operations that can fail. Use `Option<T>` for nullable values.\n- Propagate errors with `?` operator. Define custom error enums with `thiserror`.\n- Use `anyhow::Result` for application code, custom error types for library code.\n- Never `unwrap()` in production. Use `unwrap_or_default()`, `map_err()`, or pattern matching.\n\n### 7. Global Error Handler\n- Express: `app.use((err, req, res, next) => { ... })` as the last middleware.\n- Catch unhandled rejections: `process.on('unhandledRejection', handler)`.\n- In production, restart gracefully on uncaught exceptions after logging.", "start_line": 1, "end_line": 39}]}
{"source_id": "core", "type": "prompt", "title": "Circuit Breaker and Retry Patterns", "signals": ["vibecode"], "tags": ["vibecode", "resilience", "circuit-breaker", "skill"], "summary": "Production resilience patterns including circuit breaker states, exponential backoff, jitter, retry budgets, bulkhead isolation, and timeout strategies.", "chunks": [{"chunk_id": "core:circuit-breaker:1", "text": "## Circuit Breaker and Retry Patterns\n\nDistributed systems fail. Design for failure with circuit breakers, retries, and isolation to prevent cascading outages.\n\n### 1. Circuit Breaker States\n- **Closed**: Normal operation. Requests flow through. Track failure count.\n- **Open**: Failures exceeded threshold. Reject all requests immediately. Return cached/fallback response.\n- **Half-Open**: After timeout, allow a single probe request. If it succeeds, transition to Closed. If it fails, back to Open.\n- Configure thresholds: 5 failures in 60 seconds triggers Open. Half-Open after 30 seconds.\n- Track per-dependency: each external service gets its own circuit breaker instance.\n\n### 2. Exponential Backoff with Jitter\n- Base formula: `delay = min(baseDelay * 2^attempt, maxDelay)`.\n- Add jitter to prevent thundering herd: `delay = random(0, calculatedDelay)`.\n- Full jitter: `delay = random(0, min(cap, base * 2^attempt))`.\n- Decorrelated jitter: `delay = random(base, previousDelay * 3)`. Best for most cases.\n- Typical values: base=100ms, cap=30s, max attempts=5.\n\n### 3. Retry Budgets\n- Set a retry budget: max 20% additional requests from retries across the fleet.\n- Track retry ratio per service. Alert when retries exceed 10% of total traffic.\n- Never retry non-idempotent operations (POST creating resources) without idempotency keys.\n- Classify errors: retry on 429, 503, network timeout. Never retry on 400, 401, 403, 404.\n- Implement request deadlines: if total time exceeds deadline, stop retrying.\n\n### 4. Bulkhead Pattern\n- Isolate resources per dependency: separate thread pools or connection pools.\n- If Service A's pool is exhausted, Service B still has its own pool and works normally.\n- Semaphore-based: limit concurrent requests to each dependency (e.g., max 10 concurrent calls to Payment API).\n- Queue-based: buffer requests with bounded queues. Reject when queue is full.\n\n### 5. Timeout Strategies\n- Set timeouts on every external call. No timeout = infinite wait = thread exhaustion.\n- Use cascading timeouts: outer timeout > inner timeout. API gateway 30s > service 10s > DB query 5s.\n- Connect timeout (1-5s) separate from read timeout (5-30s).\n- Implement deadline propagation: pass remaining time budget to downstream services.\n\n### 6. Implementation Libraries\n- **Node.js**: `opossum` for circuit breaker, `p-retry` for retries, `p-timeout` for timeouts.\n- **Java**: Resilience4j (circuit breaker, retry, bulkhead, rate limiter in one library).\n- **Go**: `sony/gobreaker`, `cenkalti/backoff` for exponential backoff.\n- **Rust**: `tower` middleware (retry, rate-limit, timeout layers). Compose as a service stack.\n- Monitor circuit breaker state changes. Alert on Open state transitions.", "start_line": 1, "end_line": 40}]}
{"source_id": "core", "type": "prompt", "title": "Graceful Degradation Guide", "signals": ["vibecode"], "tags": ["vibecode", "resilience", "degradation", "skill"], "summary": "Production patterns for feature flags, fallback responses, cache-first strategies, queue-based processing, health checks, and SLA preservation.", "chunks": [{"chunk_id": "core:graceful-degradation:1", "text": "## Graceful Degradation Guide\n\nWhen dependencies fail, serve degraded but functional responses. A slow or partial response is always better than a complete outage.\n\n### 1. Feature Flags for Degradation\n- Use feature flags to disable non-critical features during incidents: recommendations, analytics, notifications.\n- Implement kill switches: instantly disable a feature without redeployment.\n- Tiered degradation levels: Level 1 (disable nice-to-haves), Level 2 (read-only mode), Level 3 (static fallback page).\n- Pre-define degradation runbooks: which flags to flip for each failure scenario.\n- Tools: LaunchDarkly, Unleash, or a simple Redis-backed flag service.\n\n### 2. Fallback Responses\n- Cache the last successful response for each endpoint. Serve stale data when upstream fails.\n- Return sensible defaults: empty arrays instead of errors for list endpoints.\n- Use static fallback content for critical pages (homepage, login, pricing).\n- Partial responses: if 1 of 3 microservices is down, return data from the 2 that work.\n- Set `Cache-Control: stale-while-revalidate` and `stale-if-error` headers.\n\n### 3. Cache-First Strategies\n- Multi-layer caching: browser cache > CDN > application cache (Redis) > database.\n- Cache warming: pre-populate cache on deployment and on schedule for critical data.\n- Cache stampede prevention: use probabilistic early expiration or lock-based revalidation.\n- TTL strategy: short TTL for dynamic data (5min), long TTL for static data (24h), infinite for immutable assets.\n- Monitor cache hit rates. Alert when hit rate drops below 80%.\n\n### 4. Queue-Based Processing\n- Move non-critical work to async queues: emails, notifications, analytics, PDF generation.\n- If the queue consumer is down, messages wait safely. Users see success immediately.\n- Use dead letter queues (DLQ) for failed messages. Alert and investigate DLQ growth.\n- Implement backpressure: reject new work when queue depth exceeds threshold.\n- Idempotent consumers: design handlers to safely process the same message twice.\n\n### 5. Health Check Patterns\n- **Liveness probe**: Is the process running? Simple HTTP 200 on `/healthz`.\n- **Readiness probe**: Can it serve traffic? Check DB connection, cache connection, disk space.\n- **Dependency health**: Report status of each dependency. Return 200 with degraded status if non-critical deps are down.\n- Include version, uptime, and dependency statuses in health response JSON.\n- Kubernetes: configure `initialDelaySeconds`, `periodSeconds`, and `failureThreshold` appropriately.\n\n### 6. SLA Preservation\n- Define SLOs per endpoint: p99 latency < 500ms, availability > 99.9%.\n- Error budgets: if 99.9% SLO, you have 43 minutes of downtime per month.\n- Prioritize traffic: shed low-priority requests first during overload.\n- Use load shedding: return 503 with `Retry-After` header when at capacity.\n- Monitor SLI dashboards continuously. Automate alerts when error budget burn rate is too high.", "start_line": 1, "end_line": 41}]}
{"source_id":"core","type":"prompt","title":"REST API Design Guide","signals":["vibecode"],"tags":["vibecode","api","rest","skill"],"summary":"Comprehensive guide to REST API resource naming, HTTP methods, status codes, pagination, filtering, versioning, and rate limiting.","chunks":[{"chunk_id":"core:rest-api-design:1","text":"## REST API Design Guide\n\nDesign APIs that developers love by following consistent, predictable patterns.\n\n### 1. Resource Naming\n- Use plural nouns: `/users`, `/orders`, `/products`.\n- Nest for relationships: `/users/{id}/orders`.\n- Avoid verbs in URIs. Use HTTP methods instead.\n- Use kebab-case for multi-word resources: `/order-items`.\n- Limit nesting to 2 levels to avoid complexity.\n\n### 2. HTTP Methods\n- **GET**: Read (idempotent, cacheable). Never mutate state.\n- **POST**: Create new resource. Return 201 + Location header.\n- **PUT**: Full replacement. Client sends complete representation.\n- **PATCH**: Partial update via JSON Merge Patch (RFC 7396).\n- **DELETE**: Remove resource. Return 204 No Content.\n\n### 3. Status Codes\n- **2xx**: 200 OK, 201 Created, 204 No Content.\n- **4xx**: 400 Bad Request, 401 Unauthenticated, 403 Forbidden, 404 Not Found, 409 Conflict, 422 Unprocessable, 429 Rate Limited.\n- **5xx**: 500 Internal Error, 502 Bad Gateway, 503 Unavailable.\n\n### 4. Pagination\n- **Cursor-based** (recommended): `?cursor=abc&limit=20`. Stable under writes.\n- **Offset-based**: `?offset=40&limit=20`. Simple but drifts on mutations.\n- Return `next_cursor`, `has_more`, and `total_count` in metadata.\n\n### 5. Filtering and Sorting\n- Filter: `?status=active&created_after=2024-01-01`.\n- Sort: `?sort=-created_at,name` (prefix `-` for descending).\n- Search: `?q=search+term` for full-text queries.\n\n### 6. Versioning\n- **URL path** (recommended): `/v1/users`. Clear and cacheable.\n- **Accept header**: `Accept: application/vnd.api.v2+json`.\n- Deprecate with `Sunset` header and 6-month minimum notice.\n\n### 7. Rate Limiting Headers\n- Return `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`.\n- Use 429 with `Retry-After` header when exceeded.\n- Implement sliding window counters for fair distribution.\n- Consider HATEOAS links for discoverability in mature APIs.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"API Error Response Standard","signals":["vibecode"],"tags":["vibecode","api","error","skill"],"summary":"Standardized error response format based on RFC 7807 Problem Details with validation errors, correlation IDs, and retry guidance.","chunks":[{"chunk_id":"core:api-error-standard:1","text":"## API Error Response Standard\n\nConsistent error responses reduce debugging time and improve developer experience.\n\n### 1. RFC 7807 Problem Details\nReturn errors using the Problem Details standard:\n- **type**: URI identifying the error category (e.g., `/errors/validation`).\n- **title**: Stable human-readable summary (not per-instance).\n- **status**: HTTP status code (e.g., 422).\n- **detail**: Specific explanation for this occurrence.\n- **instance**: URI of the request that triggered the error.\n- **trace_id**: Correlation ID for server-side log lookup.\n\n### 2. Validation Errors\n- Return ALL validation errors at once, never one at a time.\n- Each error includes: `field`, `code` (UPPER_SNAKE_CASE), `message`.\n- Machine-readable `code` enables client-side i18n mapping.\n- Example codes: `INVALID_FORMAT`, `OUT_OF_RANGE`, `REQUIRED_FIELD`.\n\n### 3. Error Code Taxonomy\n- Use UPPER_SNAKE_CASE: `RESOURCE_NOT_FOUND`, `RATE_LIMIT_EXCEEDED`.\n- Namespace by domain: `AUTH_TOKEN_EXPIRED`, `PAYMENT_DECLINED`.\n- Document every code in your OpenAPI spec with examples.\n- Reserve numeric ranges for different subsystems.\n\n### 4. Correlation and Tracing\n- Include `trace_id` in every error response body.\n- Log the same `trace_id` server-side for end-to-end debugging.\n- Accept `X-Request-ID` from clients and echo it back.\n- Chain trace IDs across microservice boundaries.\n\n### 5. Retry Guidance\n- Include `Retry-After` header for 429 and 503 responses.\n- Add `retryable: true/false` field for programmatic retry decisions.\n- Distinguish transient (retry-safe) from permanent (do not retry) errors.\n- Suggest exponential backoff with jitter for transient failures.\n\n### 6. Localization\n- Return `message` in the server default language.\n- Use `code` field for client-side translation lookup.\n- Accept `Accept-Language` header to localize messages server-side.\n- Separate machine-readable codes from human-readable messages.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"GraphQL Design Patterns","signals":["vibecode"],"tags":["vibecode","api","graphql","skill"],"summary":"Schema design patterns for GraphQL including N+1 prevention, Relay pagination, error handling, subscriptions, and federation.","chunks":[{"chunk_id":"core:graphql-design:1","text":"## GraphQL Design Patterns\n\nGraphQL provides flexible querying but requires careful schema and resolver design.\n\n### 1. Schema Design\n- Use descriptive, domain-specific type names (not generic CRUD).\n- Prefer nullable fields with explicit error types over throwing.\n- Use `input` types for mutations, not raw scalars.\n- Add deprecation directives with migration notes.\n- Keep query depth limited (max 5-7 levels).\n\n### 2. N+1 Prevention with DataLoader\n- Batch database calls per-request using the DataLoader pattern.\n- Create one DataLoader instance per request context.\n- Group related field resolvers to share loader instances.\n- Monitor resolver execution with tracing to detect N+1 regressions.\n\n### 3. Pagination (Relay Connections)\n- Use Relay-style connections: `edges`, `node`, `cursor`, `pageInfo`.\n- `pageInfo` includes `hasNextPage`, `hasPreviousPage`, `startCursor`, `endCursor`.\n- Support `first`/`after` for forward and `last`/`before` for backward.\n- Cursors should be opaque base64-encoded strings.\n\n### 4. Error Handling\n- Use union types for expected errors: `type Result = Success | ValidationError`.\n- Reserve top-level `errors` array for unexpected system failures.\n- Include error `extensions` with `code` and `timestamp` fields.\n- Never expose stack traces or internal details in production.\n\n### 5. Subscriptions\n- Use WebSocket transport (graphql-ws protocol).\n- Scope subscriptions to specific resources, not broad events.\n- Implement heartbeat and keepalive to detect stale connections.\n- Add authentication check on subscription initialization.\n\n### 6. Federation vs Schema Stitching\n- **Federation** (recommended): Each service owns its subgraph schema.\n- Use `@key` directive to define entity boundaries across services.\n- **Schema Stitching**: Legacy approach, harder to maintain at scale.\n- Gateway handles query planning and distributed execution.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Testing Pyramid Strategy","signals":["vibecode"],"tags":["vibecode","testing","strategy","skill"],"summary":"Tiered testing strategy covering unit, integration, and E2E ratios with isolation, mocking, fixtures, and test data factories.","chunks":[{"chunk_id":"core:testing-pyramid:1","text":"## Testing Pyramid Strategy\n\nThe testing pyramid ensures fast feedback loops with comprehensive coverage at every layer.\n\n### 1. Pyramid Ratios\n- **Unit Tests (70%)**: Fast, isolated, test single functions and components.\n- **Integration Tests (20%)**: Verify module boundaries and API contracts.\n- **E2E Tests (10%)**: Critical user journeys through the full stack.\n- Invert the ratio and you get a slow, brittle, expensive test suite.\n\n### 2. Unit Testing Rules\n- Test behavior, not implementation details.\n- One assertion per logical concept. Descriptive names: `should_reject_expired_tokens`.\n- Mock external dependencies; never hit real databases or APIs.\n- Target sub-100ms execution per test for rapid feedback.\n\n### 3. Integration Testing\n- Test real database interactions with isolated test databases.\n- Verify HTTP endpoints with actual request/response cycles.\n- Test message queue producers and consumers together.\n- Use Docker containers for reproducible service dependencies.\n\n### 4. E2E Testing\n- Cover critical user flows: signup, purchase, data export.\n- Use Page Object Model for maintainable browser tests.\n- Run in CI with headless browsers (Playwright recommended).\n- Limit to 10-20 scenarios to keep the suite fast and stable.\n\n### 5. Test Isolation\n- Each test must be independent and order-agnostic.\n- Reset state between tests via database transactions or truncation.\n- Use unique test data per test to prevent cross-test collisions.\n- Parallelize safely by avoiding shared mutable state.\n\n### 6. Mocking and Fixtures\n- **Stubs**: Return canned data for predictable behavior.\n- **Spies**: Verify interactions without changing behavior.\n- **Fakes**: In-memory implementations for complex dependencies.\n- Use factories (Faker, Factory Bot) over hardcoded fixtures.\n- Override only fields relevant to the test; generate realistic defaults.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Contract Testing Guide","signals":["vibecode"],"tags":["vibecode","testing","contract","skill"],"summary":"Consumer-driven contract testing with Pact, provider verification, schema validation, and breaking change detection.","chunks":[{"chunk_id":"core:contract-testing:1","text":"## Contract Testing Guide\n\nContract testing verifies that services can communicate correctly without running full integration tests.\n\n### 1. Consumer-Driven Contracts (Pact)\n- Consumer defines expected request/response interactions.\n- Interactions are recorded as a contract (pact file).\n- Provider verifies it can fulfill all consumer expectations.\n- Contracts live in a Pact Broker for versioned sharing.\n\n### 2. Workflow\n- **Step 1**: Consumer writes tests describing expected API behavior.\n- **Step 2**: Pact generates a contract file from passing tests.\n- **Step 3**: Contract is published to the Pact Broker.\n- **Step 4**: Provider runs verification against all consumer contracts.\n- **Step 5**: Results are published; CI gates on verification status.\n\n### 3. Provider Verification\n- Provider replays each consumer interaction against its real API.\n- Use provider states to set up required test data.\n- Verify response status, headers, and body structure.\n- Run verification in CI on every provider change.\n\n### 4. Schema Validation\n- Validate API responses against OpenAPI or JSON Schema definitions.\n- Use tools like Spectral or Optic for schema linting.\n- Detect structural changes: added, removed, or renamed fields.\n- Enforce backward compatibility at the schema level.\n\n### 5. Breaking Change Detection\n- **Breaking**: Removing a field, changing a type, renaming an endpoint.\n- **Non-breaking**: Adding optional fields, new endpoints, new enum values.\n- Automate detection with schema diff tools in CI.\n- Require explicit approval for any breaking change.\n\n### 6. Best Practices\n- Test at the boundary, not the implementation.\n- Keep contracts minimal: verify structure, not exact values.\n- Version contracts alongside service versions.\n- Use can-i-deploy checks before deploying to production.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Performance Testing Patterns","signals":["vibecode"],"tags":["vibecode","testing","performance","skill"],"summary":"Load testing patterns with k6 and Artillery covering stress testing, soak testing, baseline metrics, and bottleneck identification.","chunks":[{"chunk_id":"core:performance-testing:1","text":"## Performance Testing Patterns\n\nPerformance testing prevents production surprises by validating system behavior under load.\n\n### 1. Test Types\n- **Load Test**: Expected traffic volume. Validates normal operation.\n- **Stress Test**: Beyond expected load. Finds the breaking point.\n- **Soak Test**: Sustained load over hours. Detects memory leaks and connection exhaustion.\n- **Spike Test**: Sudden traffic burst. Tests auto-scaling and recovery.\n\n### 2. Tools\n- **k6**: JavaScript-based, developer-friendly, excellent CI integration.\n- **Artillery**: YAML config, good for API and WebSocket testing.\n- **Grafana k6 Cloud**: Distributed load generation from multiple regions.\n- Choose based on team familiarity and protocol support.\n\n### 3. Baseline Metrics\n- Establish baselines before optimization or feature changes.\n- **Response Time**: Median (p50), p95, p99 under normal load.\n- **Throughput**: Requests per second at target concurrency.\n- **Error Rate**: Percentage of 4xx/5xx responses under load.\n\n### 4. Percentile Targets\n- **p50 < 200ms**: Median response time for API endpoints.\n- **p95 < 500ms**: Most users experience acceptable latency.\n- **p99 < 1000ms**: Tail latency stays within SLA bounds.\n- Never use averages; they hide tail latency problems.\n\n### 5. Bottleneck Identification\n- **Database**: Slow queries, missing indexes, connection pool exhaustion.\n- **CPU**: Inefficient algorithms, excessive serialization.\n- **Memory**: Leaks, unbounded caches, large payload buffering.\n- **Network**: DNS resolution, TLS handshake overhead, bandwidth limits.\n- Use distributed tracing (Jaeger, Zipkin) to pinpoint slow spans.\n\n### 6. CI Integration\n- Run load tests on every release candidate, not just before launch.\n- Set performance budgets as CI gate conditions.\n- Store results historically to detect regressions over time.\n- Alert on p95 degradation exceeding 10% from baseline.","start_line":1,"end_line":38}]}
{"source_id":"core","type":"prompt","title":"Server State Management","signals":["vibecode"],"tags":["vibecode","state","server","skill"],"summary":"React Query and SWR patterns for server state including cache invalidation, optimistic updates, background refetching, and pagination.","chunks":[{"chunk_id":"core:server-state:1","text":"## Server State Management\n\nServer state is data owned by the backend. Treat it as a cache, not a source of truth.\n\n### 1. Core Libraries\n- **TanStack Query (React Query)**: Feature-rich, supports mutations and infinite queries.\n- **SWR**: Lightweight, stale-while-revalidate focused, by Vercel.\n- **RTK Query**: Redux Toolkit integration, auto-generated hooks.\n- Choose based on existing stack and complexity requirements.\n\n### 2. Stale-While-Revalidate\n- Serve cached data immediately, refetch in background.\n- Set `staleTime` based on data volatility (5s for live feeds, 5min for settings).\n- Set `cacheTime` to control how long inactive data stays in memory.\n- Users see instant responses while data stays fresh.\n\n### 3. Cache Invalidation\n- Invalidate related queries after successful mutations.\n- Use query key hierarchies: `['users', userId, 'posts']`.\n- Invalidate broadly after writes: `queryClient.invalidateQueries(['users'])`.\n- Avoid over-invalidation; target only affected query keys.\n\n### 4. Optimistic Updates\n- Update the UI immediately before the server confirms.\n- Snapshot previous data for rollback on failure.\n- Show subtle loading indicator during confirmation.\n- Revert to snapshot and show error toast if mutation fails.\n\n### 5. Background Refetching\n- Refetch on window focus to catch stale data after tab switches.\n- Refetch on network reconnect for offline recovery.\n- Use polling (`refetchInterval`) for near-real-time dashboards.\n- Disable background refetch for static or rarely-changing data.\n\n### 6. Pagination State\n- Use `useInfiniteQuery` for infinite scroll with cursor-based APIs.\n- Maintain page params in query keys for proper caching.\n- Prefetch next page on hover or scroll proximity.\n- Show skeleton loaders during page transitions for perceived speed.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Client State Patterns","signals":["vibecode"],"tags":["vibecode","state","client","skill"],"summary":"Client-side state management with Zustand, Redux Toolkit, Jotai, and Recoil covering derived state, normalization, and persistence.","chunks":[{"chunk_id":"core:client-state:1","text":"## Client State Patterns\n\nClient state is UI-owned data that does not exist on the server: modals, forms, themes, filters.\n\n### 1. Library Selection\n- **Zustand**: Minimal API, no boilerplate, great for small-to-medium apps.\n- **Redux Toolkit**: Structured, scalable, excellent DevTools and middleware.\n- **Jotai**: Atomic model, bottom-up composition, React Suspense compatible.\n- **Recoil**: Atom and selector graph, good for derived async state.\n- **Context API**: Built-in React, suitable only for low-frequency updates (theme, auth).\n\n### 2. When to Use Context vs Store\n- **Context**: Theme, locale, auth status (changes rarely, affects many components).\n- **Store**: Shopping cart, form wizard state, UI filters (changes often, localized impact).\n- Context triggers re-renders in all consumers on every change.\n- Stores with selectors only re-render components that read changed slices.\n\n### 3. Derived State\n- Compute derived values from base state, never store duplicates.\n- Use selectors or computed properties to avoid sync bugs.\n- Memoize expensive derivations with `useMemo` or library-specific selectors.\n- Example: `totalPrice` derived from `cartItems` and `quantities`.\n\n### 4. State Normalization\n- Store entities by ID in a flat map, not nested arrays.\n- Use lookup tables: `byId` map plus `allIds` array pattern.\n- Normalize on API response ingestion, denormalize in selectors.\n- Prevents data duplication and simplifies updates.\n\n### 5. Persistence Middleware\n- Persist critical state to `localStorage` or `sessionStorage`.\n- Use serialization middleware (Zustand `persist`, Redux `redux-persist`).\n- Implement migration strategies for schema changes between versions.\n- Encrypt sensitive persisted state (tokens, preferences).\n\n### 6. Best Practices\n- Keep state minimal: if it can be derived, do not store it.\n- Colocate state with the component that owns it.\n- Lift state only when siblings need to share it.\n- Use DevTools for time-travel debugging and state inspection.","start_line":1,"end_line":38}]}
{"source_id":"core","type":"prompt","title":"Offline-First Architecture","signals":["vibecode"],"tags":["vibecode","state","offline","skill"],"summary":"Offline-first patterns with service workers, IndexedDB, sync queues, conflict resolution, and optimistic UI strategies.","chunks":[{"chunk_id":"core:offline-first:1","text":"## Offline-First Architecture\n\nOffline-first design ensures your app remains functional without a network connection.\n\n### 1. Service Workers\n- Intercept network requests and serve cached responses.\n- Use Workbox for simplified caching strategies.\n- **Cache First**: Static assets (CSS, JS, images).\n- **Network First**: API responses where freshness matters.\n- **Stale While Revalidate**: Content that can tolerate brief staleness.\n\n### 2. IndexedDB for Local Storage\n- Store structured data locally using IndexedDB (not localStorage).\n- Use wrapper libraries: Dexie.js, idb, or localForage.\n- Index frequently queried fields for fast lookups.\n- Implement storage quotas and cleanup for old data.\n\n### 3. Sync Queues\n- Queue mutations (creates, updates, deletes) when offline.\n- Persist the queue to IndexedDB to survive app restarts.\n- Process queue sequentially on reconnection (FIFO order).\n- Retry failed syncs with exponential backoff.\n- Show queue status to users: pending, syncing, synced, failed.\n\n### 4. Conflict Resolution\n- **Last-Write-Wins (LWW)**: Simple, use timestamps to pick latest change.\n- **CRDTs**: Conflict-free data types that merge automatically (counters, sets).\n- **Three-Way Merge**: Compare client, server, and common ancestor versions.\n- Choose strategy based on data sensitivity and collaboration needs.\n\n### 5. Background Sync API\n- Register sync events that fire when connectivity is restored.\n- Use the ServiceWorkerRegistration sync API to register tasks.\n- Handle sync in the service worker sync event listener.\n- Fall back to periodic polling for browsers without Background Sync.\n\n### 6. Optimistic UI Patterns\n- Show mutations immediately in the UI before server confirmation.\n- Mark optimistic items with pending status indicators.\n- Reconcile with server response: update IDs, timestamps, computed fields.\n- Revert and notify on sync failure with clear error messaging.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"CI/CD Pipeline Patterns","signals":["vibecode"],"tags":["vibecode","cicd","deployment","skill"],"summary":"Pipeline stages, parallelization, caching strategies, artifact management, environment promotion, and pipeline-as-code patterns.","chunks":[{"chunk_id":"core:cicd-pipeline:1","text":"## CI/CD Pipeline Patterns\n\nA well-designed CI/CD pipeline is the backbone of reliable software delivery. Every merge to main should be deployable.\n\n### 1. Pipeline Stages\n- **Build**: Compile code, resolve dependencies, generate artifacts. Fail fast on syntax errors.\n- **Test**: Run unit tests, integration tests, and contract tests in parallel. Require 80%+ coverage.\n- **Lint & Format**: Enforce code style (ESLint, Prettier, Clippy) before review. Auto-fix in CI if possible.\n- **Security Scan**: Run SAST (Semgrep, CodeQL), dependency audit (npm audit, cargo audit), and secret detection.\n- **Deploy**: Push to staging automatically. Production deploys require manual approval or tag trigger.\n\n### 2. Parallelization\n- Split test suites across multiple runners using sharding: `--shard=1/4` in Jest, `--partition` in pytest.\n- Run lint, test, and security scan as parallel jobs, not sequential steps.\n- Use dependency graphs: only rebuild changed packages in monorepos (Turborepo, Nx).\n- Fan-out/fan-in: parallel jobs converge at a gate before deployment.\n\n### 3. Caching Strategies\n- Cache dependency directories: `node_modules`, `target/`, `.venv/`, keyed by lockfile hash.\n- Cache build outputs: Docker layer caching, incremental compilation artifacts.\n- Cache test results: skip unchanged test files with `--changedSince=main`.\n- Set cache TTL: 7 days for dependencies, 1 day for build artifacts.\n\n### 4. Artifact Management\n- Tag artifacts with git SHA and build number for traceability.\n- Store build artifacts in a registry: Docker images, npm packages, binary releases.\n- Sign artifacts with cosign or GPG for supply chain security.\n- Clean up old artifacts automatically. Retain last 10 builds per branch.\n\n### 5. Environment Promotion\n- Promote the same artifact through environments: dev -> staging -> production.\n- Never rebuild for production. The staging-tested artifact IS the production artifact.\n- Use environment-specific config (env vars), not environment-specific builds.\n- Gate promotions on test results, approval, and health checks.\n\n### 6. Secrets Injection\n- Use CI platform secrets (GitHub Actions secrets, GitLab CI variables). Never hardcode.\n- Scope secrets to environments: staging secrets differ from production.\n- Use OIDC federation for cloud access instead of long-lived credentials.\n- Mask secrets in logs. Audit secret access quarterly.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Deployment Strategies Guide","signals":["vibecode"],"tags":["vibecode","deployment","strategy","skill"],"summary":"Blue-green, canary, rolling updates, feature flags, rollback triggers, and zero-downtime deployment patterns.","chunks":[{"chunk_id":"core:deployment-strategies:1","text":"## Deployment Strategies Guide\n\nChoose your deployment strategy based on risk tolerance, traffic volume, and rollback requirements. Zero downtime is non-negotiable for production.\n\n### 1. Blue-Green Deployment\n- Maintain two identical environments: Blue (current) and Green (new version).\n- Deploy to Green, run smoke tests, then switch the load balancer to Green.\n- Instant rollback: switch back to Blue if issues are detected.\n- Cost: 2x infrastructure during deployment. Worth it for critical services.\n- Drain connections gracefully: wait for in-flight requests to complete before switching.\n\n### 2. Canary Deployment\n- Route 1-5% of traffic to the new version. Monitor error rates and latency.\n- Gradually increase: 5% -> 25% -> 50% -> 100% over hours or days.\n- Automated canary analysis: compare p99 latency, error rate, and business metrics against baseline.\n- Rollback trigger: if error rate increases >0.1% or p99 latency increases >50ms.\n- Tools: Flagger (Kubernetes), AWS CodeDeploy, Argo Rollouts.\n\n### 3. Rolling Updates\n- Replace instances one at a time. At least one instance always serves traffic.\n- Configure `maxUnavailable: 1` and `maxSurge: 1` in Kubernetes deployments.\n- Health check integration: new pod must pass readiness probe before old pod terminates.\n- Slower rollback than blue-green. Consider if rollback speed is critical.\n\n### 4. Feature Flags\n- Decouple deployment from release. Deploy code dark, then enable via flag.\n- Flag types: release toggles (temporary), ops toggles (permanent), experiment toggles (A/B).\n- Clean up release flags within 2 weeks of full rollout. Tech debt accumulates fast.\n- Server-side evaluation for security-sensitive flags. Client-side for UI variations.\n- Tools: LaunchDarkly, Unleash, Flagsmith, or a simple Redis/DB-backed service.\n\n### 5. Rollback Triggers\n- Automated: error rate > threshold, latency > SLO, health check failures.\n- Manual: customer-reported issues, data corruption detected.\n- Time-based: if metrics don't stabilize within 15 minutes, auto-rollback.\n- Keep last 3 known-good artifacts for instant rollback capability.\n\n### 6. Zero-Downtime Checklist\n- Database migrations must be backward-compatible (expand-and-contract).\n- API changes must be additive (new fields) not breaking (removed fields).\n- Use connection draining with 30-second grace period.\n- Pre-warm new instances before receiving traffic. Health checks must pass first.","start_line":1,"end_line":38}]}
{"source_id":"core","type":"prompt","title":"Infrastructure as Code Patterns","signals":["vibecode"],"tags":["vibecode","infrastructure","iac","skill"],"summary":"Terraform modules, state management, drift detection, environment parity, immutable infrastructure, and container orchestration basics.","chunks":[{"chunk_id":"core:iac-patterns:1","text":"## Infrastructure as Code Patterns\n\nAll infrastructure must be defined in code, version-controlled, and reproducible. Manual changes are forbidden in production.\n\n### 1. Terraform Module Design\n- Create reusable modules for common patterns: VPC, database, app service, CDN.\n- Pin module versions: `source = \"./modules/vpc?ref=v1.2.0\"`. Never use `main` branch.\n- Input variables with validation: `variable \"instance_type\" { type = string }` with custom conditions.\n- Output essential values: IDs, ARNs, endpoints, connection strings.\n- Keep modules small and composable. One module per logical resource group.\n\n### 2. State Management\n- Remote state is mandatory: S3 + DynamoDB (AWS), GCS (GCP), Azure Blob Storage.\n- Enable state locking to prevent concurrent modifications.\n- Use workspaces or separate state files per environment: `terraform workspace select staging`.\n- Never edit state manually. Use `terraform state mv` and `terraform import`.\n- Enable state file encryption at rest. Restrict access via IAM policies.\n\n### 3. Drift Detection\n- Run `terraform plan` on a schedule (daily) to detect manual changes.\n- Alert on drift: any difference between desired state and actual state.\n- Auto-remediate non-critical drift. Require approval for security-related drift.\n- Use AWS Config Rules, Azure Policy, or GCP Organization Policies for continuous compliance.\n\n### 4. Environment Parity\n- Use the same Terraform code for dev, staging, and production. Differ only in variables.\n- Variable files per environment: `terraform apply -var-file=production.tfvars`.\n- Size differences are acceptable (smaller instances in dev) but architecture must match.\n- Test infrastructure changes in dev first. Promote to staging, then production.\n\n### 5. Immutable Infrastructure\n- Never SSH into production servers to make changes. Rebuild from images.\n- Bake AMIs/container images with all dependencies. Deploy new image, terminate old.\n- Use Packer for VM images, Docker for containers. Version every image.\n- Benefits: reproducible deployments, no configuration drift, simple rollback.\n\n### 6. Container Orchestration Basics\n- Define resource limits: `resources: { limits: { cpu: 500m, memory: 512Mi } }` in Kubernetes.\n- Use health checks: liveness probe (restart if dead), readiness probe (remove from LB if not ready).\n- Horizontal Pod Autoscaler: scale on CPU (70% target) or custom metrics.\n- Use namespaces for environment isolation. NetworkPolicies for pod-to-pod security.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Database Schema Design","signals":["vibecode"],"tags":["vibecode","database","schema","skill"],"summary":"Normalization vs denormalization, indexing strategies, foreign key constraints, soft deletes, audit columns, and UUID vs auto-increment trade-offs.","chunks":[{"chunk_id":"core:database-schema:1","text":"## Database Schema Design\n\nA well-designed schema is the foundation of application performance and data integrity. Get it right early to avoid costly migrations later.\n\n### 1. Normalization vs Denormalization\n- Start normalized (3NF): eliminate data redundancy, ensure referential integrity.\n- Denormalize deliberately for read performance: materialized views, JSON columns, computed fields.\n- Common denormalization: store `user_name` alongside `user_id` in comments to avoid joins.\n- Use CQRS pattern: normalized write model, denormalized read model for complex queries.\n- Rule: normalize first, denormalize only when you have measured performance evidence.\n\n### 2. Indexing Strategies\n- **B-tree** (default): equality and range queries. Best for `WHERE id = ?`, `WHERE date > ?`.\n- **GIN**: full-text search, JSONB containment, array operations. Best for `@>`, `@@`, `&&`.\n- **Partial indexes**: `CREATE INDEX idx_active ON users(email) WHERE active = true`. Smaller, faster.\n- **Composite indexes**: column order matters. Put equality columns first, range columns last.\n- **Covering indexes**: `INCLUDE` non-indexed columns to enable index-only scans.\n- Rule: index columns in WHERE, JOIN, and ORDER BY clauses. Monitor unused indexes monthly.\n\n### 3. Foreign Key Constraints\n- Always define foreign keys for referential integrity. Use `ON DELETE CASCADE` or `RESTRICT`.\n- `CASCADE`: delete children when parent is deleted (comments when post is deleted).\n- `SET NULL`: set FK to NULL when parent is deleted (assign to no team when team is deleted).\n- `RESTRICT`: prevent parent deletion if children exist (prevent user deletion if orders exist).\n- Index foreign key columns for JOIN performance.\n\n### 4. Soft Deletes\n- Add `deleted_at TIMESTAMP NULL` column. NULL means active, non-NULL means deleted.\n- Filter in queries: `WHERE deleted_at IS NULL`. Create a view for convenience.\n- Partial unique indexes: `CREATE UNIQUE INDEX idx_email ON users(email) WHERE deleted_at IS NULL`.\n- Implement periodic hard-delete jobs for GDPR compliance and storage cleanup.\n- Consider: soft deletes add complexity. Only use when recovery or audit trails are required.\n\n### 5. Audit Columns\n- Every table: `id`, `created_at`, `updated_at`, `deleted_at` (if soft delete).\n- `created_at DEFAULT NOW()`: set once on insert. Never update.\n- `updated_at`: update via trigger or ORM hook on every modification.\n- Add `created_by` and `updated_by` for user attribution. FK to users table.\n- Use database triggers for reliability over application-level hooks.\n\n### 6. UUID vs Auto-Increment\n- **Auto-increment**: smaller storage (4-8 bytes), better index locality, sequential.\n- **UUIDv4**: globally unique, safe for distributed systems, no enumeration attacks.\n- **UUIDv7**: time-ordered UUIDs. Best of both: globally unique AND sequential for index performance.\n- Recommendation: use UUIDv7 as primary key for new projects. Auto-increment for high-write append-only tables.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Database Migration Patterns","signals":["vibecode"],"tags":["vibecode","database","migration","skill"],"summary":"Expand-and-contract pattern, zero-downtime migrations, backward-compatible changes, data backfill strategies, and rollback procedures.","chunks":[{"chunk_id":"core:database-migration:1","text":"## Database Migration Patterns\n\nDatabase migrations are the riskiest part of any deployment. Plan carefully, test thoroughly, and always have a rollback strategy.\n\n### 1. Expand-and-Contract Pattern\n- **Expand**: add new column/table without removing the old one. Both coexist.\n- **Migrate**: backfill data from old structure to new. Update application to write to both.\n- **Contract**: once all reads use the new structure, drop the old column/table.\n- Never combine expand and contract in one deployment. Separate by at least one release.\n- This pattern ensures zero-downtime and backward-compatible migrations.\n\n### 2. Zero-Downtime Migration Rules\n- Never rename a column directly. Add new column, backfill, switch reads, drop old.\n- Never change a column type directly. Add new column with new type, migrate data.\n- Adding a column with a default is safe in PostgreSQL 11+ (metadata-only change).\n- Adding a NOT NULL constraint: add as nullable, backfill, then add constraint with NOT VALID and validate separately.\n- Creating an index: always use `CREATE INDEX CONCURRENTLY` to avoid table locks.\n\n### 3. Backward-Compatible Changes\n- Safe changes: add column (nullable or with default), add table, add index concurrently.\n- Unsafe changes: drop column, rename column, change type, add NOT NULL to existing.\n- Application must handle both old and new schema during the transition period.\n- Deploy application changes that read new columns AFTER the migration runs.\n- Deploy application changes that stop writing old columns BEFORE dropping them.\n\n### 4. Data Backfill Strategies\n- Batch processing: update 1000 rows at a time with `WHERE id > last_processed LIMIT 1000`.\n- Use background jobs for large tables. Never run multi-million row updates in a transaction.\n- Throttle: add `pg_sleep(0.1)` between batches to avoid overwhelming the database.\n- Verify completeness: count rows where new_column IS NULL AND old_column IS NOT NULL.\n- Idempotent backfills: safe to re-run without duplicating data.\n\n### 5. Migration Testing\n- Test migrations against a copy of production data, not just empty schemas.\n- Measure migration duration on production-sized data. Alert if it exceeds maintenance window.\n- Test rollback procedure before deploying. Verify data integrity after rollback.\n- Use pg_dump or snapshot before migration for point-in-time recovery.\n\n### 6. Rollback Procedures\n- Every migration file must have a corresponding down migration.\n- Test down migrations in CI. They must cleanly reverse the up migration.\n- For data migrations: backup affected rows before modification.\n- Time limit: if migration has not completed in 30 minutes, abort and rollback.\n- Post-rollback verification: run data integrity checks and application smoke tests.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Query Optimization Guide","signals":["vibecode"],"tags":["vibecode","database","performance","skill"],"summary":"EXPLAIN analysis, index-only scans, covering indexes, query plan caching, connection pooling, N+1 prevention, and batch operations.","chunks":[{"chunk_id":"core:query-optimization:1","text":"## Query Optimization Guide\n\nSlow queries are the number one cause of application performance issues. Profile before optimizing, and always verify with EXPLAIN.\n\n### 1. EXPLAIN Analysis\n- Always use `EXPLAIN ANALYZE` to see actual execution times, not just estimates.\n- Look for: Seq Scan on large tables (missing index), Nested Loop with high row counts, Sort with high memory.\n- Use `EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)` for detailed buffer hit/miss statistics.\n- Compare estimated rows vs actual rows. Large discrepancies indicate stale statistics. Run ANALYZE.\n- Watch for Bitmap Heap Scan converting to Seq Scan when too many rows match.\n\n### 2. Index-Only Scans\n- The fastest query reads only the index, never touching the table (heap).\n- Requires all selected columns to be in the index. Use INCLUDE for non-key columns.\n- Check EXPLAIN for Index Only Scan vs Index Scan. The former avoids heap fetches.\n- Visibility map must be up-to-date: run VACUUM regularly for index-only scan eligibility.\n\n### 3. Covering Indexes\n- Example: `CREATE INDEX idx_orders ON orders(user_id) INCLUDE (total, status, created_at)`.\n- Satisfies `SELECT total, status FROM orders WHERE user_id = ?` without heap access.\n- Trade-off: larger index size vs faster reads. Worth it for hot queries.\n- Do not include large text or JSONB columns. Keep covering indexes lean.\n\n### 4. Connection Pooling\n- Use PgBouncer or built-in pooling (Prisma, SQLAlchemy). Never open connections per request.\n- Transaction mode pooling: connection returned to pool after each transaction. Best for web apps.\n- Size pool appropriately: typically 20-50 connections based on core count.\n- Monitor pg_stat_activity for idle connections. Set idle_in_transaction_session_timeout.\n\n### 5. N+1 Prevention\n- Detect: if you see N+1 queries in logs for a list page, you have the N+1 problem.\n- ORM: use eager loading. Rails: `.includes(:comments)`. SQLAlchemy: `joinedload(Post.comments)`.\n- GraphQL: use DataLoader to batch and deduplicate database calls per request.\n- Manual: replace loop queries with `WHERE id IN (...)` batch query. Fetch all, then map in memory.\n\n### 6. Batch Operations\n- Insert: use multi-row INSERT `INSERT INTO t (a,b) VALUES (1,2),(3,4)`. 10-100x faster than individual inserts.\n- Update: use UPDATE with FROM clause for batch updates from a values list.\n- Delete: batch deletes with `DELETE FROM t WHERE id IN (SELECT id FROM t WHERE condition LIMIT 1000)`.\n- Use COPY for bulk loading CSV/TSV data. Orders of magnitude faster than INSERT.\n- Set work_mem higher for sort-heavy batch operations: `SET LOCAL work_mem = '256MB'`.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Structured Logging Patterns","signals":["vibecode"],"tags":["vibecode","monitoring","logging","skill"],"summary":"JSON structured logs, correlation IDs, log levels, context enrichment, PII redaction, log aggregation, and alerting thresholds.","chunks":[{"chunk_id":"core:structured-logging:1","text":"## Structured Logging Patterns\n\nLogs are your primary debugging tool in production. Structure them for machines to parse and humans to read.\n\n### 1. JSON Structured Logs\n- Every log line is a JSON object with standard fields: timestamp, level, message, service, environment, version.\n- Add context fields: user_id, request_id, trace_id, method, path, duration_ms.\n- Avoid string interpolation in log messages. Use structured fields: `{\"message\": \"Order created\", \"order_id\": 123}` not `Order 123 created`.\n- One event per line. Multi-line logs break aggregation tools.\n- Use ISO 8601 timestamps with timezone: `2024-01-15T10:30:00.123Z`.\n\n### 2. Correlation IDs\n- Generate a unique request_id (UUIDv4) at the API gateway or first service.\n- Propagate via X-Request-ID header through all downstream service calls.\n- Include request_id in every log line for end-to-end request tracing.\n- For async operations: pass correlation_id through message queues and event payloads.\n- Link logs to traces: include OpenTelemetry trace_id and span_id in log entries.\n\n### 3. Log Levels\n- **ERROR**: Something failed that should not have. Requires investigation. Pages on-call if threshold breached.\n- **WARN**: Something unexpected but handled. Degraded behavior, retries, fallbacks activated.\n- **INFO**: Normal business events. Request handled, job completed, user signed up. Default production level.\n- **DEBUG**: Detailed diagnostic info. SQL queries, cache hits/misses, function arguments. Never in production.\n- Rule: if you would grep for it during an incident, it should be INFO or above.\n\n### 4. Context Enrichment\n- Use middleware to automatically attach request context to all logs within a request.\n- Node.js: AsyncLocalStorage for per-request context. Python: contextvars. Go: context.Context.\n- Enrich with: authenticated user ID, tenant ID, feature flags, deployment version.\n- Add timing: log duration_ms for every external call (DB, API, cache, queue).\n\n### 5. PII Redaction\n- Never log passwords, tokens, credit card numbers, or SSNs.\n- Redact email addresses in logs: mask as `u***@example.com`. Mask phone numbers partially.\n- Use allowlists for loggable fields, not denylists. Only log fields explicitly marked safe.\n- Implement redaction at the logger level, not at each call site. Centralize the policy.\n\n### 6. Log Aggregation and Alerting\n- Ship logs to a centralized platform: ELK Stack, Grafana Loki, Datadog, or CloudWatch.\n- Set retention: 30 days hot storage, 90 days warm, 1 year cold (compliance).\n- Alert on: ERROR rate exceeding 1% of requests, specific error codes, latency spikes.\n- Create saved searches for common investigation patterns. Share with the team.","start_line":1,"end_line":40}]}
{"source_id":"core","type":"prompt","title":"Metrics and SLO Guide","signals":["vibecode"],"tags":["vibecode","monitoring","metrics","slo","skill"],"summary":"SLI/SLO/SLA hierarchy, error budgets, golden signals, percentile-based targets, dashboard design, and alert fatigue prevention.","chunks":[{"chunk_id":"core:metrics-slo:1","text":"## Metrics and SLO Guide\n\nYou cannot improve what you do not measure. Define SLOs before launch, measure SLIs continuously, and use error budgets to balance reliability with velocity.\n\n### 1. SLI/SLO/SLA Hierarchy\n- **SLI (Service Level Indicator)**: A measured metric. Example: proportion of requests completing in under 300ms.\n- **SLO (Service Level Objective)**: Internal target. Example: 99.9% of requests complete in under 300ms over 30 days.\n- **SLA (Service Level Agreement)**: External contract with consequences. Always less strict than SLO.\n- Set SLOs per user journey, not per endpoint. Login flow SLO differs from search SLO.\n- Review and adjust SLOs quarterly based on user impact and operational cost.\n\n### 2. Error Budgets\n- Error budget = 100% minus SLO. A 99.9% SLO gives 0.1% error budget, roughly 43 minutes per month.\n- Track budget consumption rate. If burning too fast, freeze feature releases and fix reliability.\n- When budget is healthy, ship faster. Error budgets explicitly authorize controlled risk.\n- Automate: if error budget consumed over 50% in first week, auto-alert and review deployments.\n- Reset budgets monthly or per rolling 30-day window.\n\n### 3. Golden Signals\n- **Latency**: Time to serve a request. Track p50, p95, p99 separately. p99 catches tail latency.\n- **Traffic**: Requests per second. Baseline normal traffic to detect anomalies and capacity needs.\n- **Errors**: Failed requests divided by total requests. Split by type: 5xx server, 4xx client, timeout.\n- **Saturation**: Resource utilization. CPU, memory, disk, connection pool usage. Alert at 80%.\n- These four signals cover 90% of production monitoring needs.\n\n### 4. Percentile-Based Targets\n- Never use averages for latency. Averages hide outliers that affect real users.\n- p50 (median): typical user experience. p95: experience for 1 in 20 users. p99: worst common case.\n- Set SLOs on p99: if p99 is under 500ms, most users have a great experience.\n- Use histograms (not summaries) in Prometheus for accurate percentile computation.\n- Track percentiles per endpoint, per region, and per customer tier.\n\n### 5. Dashboard Design\n- Top row: SLO compliance and error budget remaining. Red/green status at a glance.\n- Second row: golden signals (latency, traffic, errors, saturation) with 24-hour trend.\n- Third row: dependency health (database, cache, external APIs) with latency and error rate.\n- Use consistent time ranges. Default to last 1 hour with quick-select for 6h, 24h, 7d.\n- Every dashboard needs a title, owner, and last-reviewed date.\n\n### 6. Alert Fatigue Prevention\n- Alert on symptoms (high error rate), not causes (high CPU). Causes are for dashboards.\n- Every alert must have a runbook link. If there is no action to take, delete the alert.\n- Use multi-window, multi-burn-rate alerts: fast burn (2% budget in 1h) pages immediately, slow burn (10% in 6h) creates a ticket.\n- Route alerts by severity: page for critical P1, Slack for warning P2, ticket for info P3.\n- Review alert frequency monthly. Snooze or delete alerts that fire more than 3 times without action.","start_line":1,"end_line":40}]}
